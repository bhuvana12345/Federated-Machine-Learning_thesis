{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSwpmKT5bVHf",
        "outputId": "2af2b03a-9705-439c-9fc7-124cd0ac1dac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q flwr[simulation] torch torchvision matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJMx0CpBbhqZ"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "from collections import OrderedDict\n",
        "from typing import List, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "\n",
        "import flwr as fl\n",
        "from flwr.common import Metrics\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYgHteKq0C-e",
        "outputId": "070b1b77-40fc-4c92-afef-b71f1f043efc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "CLASSES = (\n",
        "    \"0\",\n",
        "    \"1\",\n",
        "    \"2\",\n",
        "    \"3\",\n",
        "    \"4\",\n",
        "    \"5\",\n",
        "    \"6\",\n",
        "    \"7\",\n",
        "    \"8\",\n",
        "    \"9\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16hwFVFWcBZo"
      },
      "outputs": [],
      "source": [
        "NUM_CLIENTS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P3ThftBHki8"
      },
      "source": [
        "# IID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNeAe0O4y90w"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def determine_pca_components(public_dataset, n_components=64):\n",
        "    # Reshape the public dataset to 2D (num_samples, num_features)\n",
        "    public_data = public_dataset.reshape(public_dataset.shape[0], -1)\n",
        "\n",
        "    # Fit PCA on the public dataset\n",
        "    pca = PCA(n_components=n_components)\n",
        "    pca.fit(public_data)\n",
        "\n",
        "    return pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-LsG5axzB_h"
      },
      "outputs": [],
      "source": [
        "def apply_pca_locally(data, pca, n_components):\n",
        "    # Reshape the data to 2D (num_samples, num_features)\n",
        "    data_2d = data.reshape(data.shape[0], -1)\n",
        "\n",
        "    # Apply PCA transformation\n",
        "    data_transformed = pca.transform(data_2d)[:, :n_components]\n",
        "\n",
        "    # Calculate the percentage of data reduced or removed\n",
        "    original_size = data_2d.size\n",
        "    transformed_size = data_transformed.size\n",
        "    data_removed_percentage = (1 - transformed_size / original_size) * 100\n",
        "\n",
        "    # Reshape the transformed data to (num_samples, 1, n_components)\n",
        "    data_transformed = data_transformed.reshape(-1, 1, n_components)\n",
        "\n",
        "    return data_transformed, data_removed_percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmLNeLIMBBcd",
        "outputId": "3c3cf9eb-2a34-42a3-9bfa-278f76adbccd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./dataset/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 90418541.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./dataset/MNIST/raw/train-images-idx3-ubyte.gz to ./dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 20331603.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./dataset/MNIST/raw/train-labels-idx1-ubyte.gz to ./dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 33266110.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to ./dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 18300219.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./dataset/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 87616721.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 84415117.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 33001805.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 12871978.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import MNIST\n",
        "\n",
        "# Define the transform to preprocess the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "mnist_dataset = MNIST(\"./dataset\", train=True, download=True, transform=transform)\n",
        "\n",
        "# Load the MNIST dataset\n",
        "trainset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = MNIST(root='./data', train=False, download=True, transform=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55_B90pgTzTW"
      },
      "outputs": [],
      "source": [
        "pca_components_range = [8, 16, 32, 64, 128]\n",
        "#pca_components_range = [16, 32]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYJ1AIuyg6jL"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "\n",
        "def load_datasets(pca, n_components):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    trainset = MNIST(\"./dataset\", train=True, download=True, transform=transform)\n",
        "    testset = MNIST(\"./dataset\", train=False, download=True, transform=transform)\n",
        "\n",
        "    trainset_transformed, train_removed_percentage = apply_pca_locally(trainset.data.numpy(), pca, n_components)\n",
        "    testset_transformed, test_removed_percentage = apply_pca_locally(testset.data.numpy(), pca, n_components)\n",
        "\n",
        "    partition_size = len(trainset) // NUM_CLIENTS\n",
        "    lengths = [partition_size] * NUM_CLIENTS\n",
        "    datasets = random_split(trainset, lengths, torch.Generator().manual_seed(42))\n",
        "\n",
        "    trainloaders = []\n",
        "    valloaders = []\n",
        "    for ds in datasets:\n",
        "        len_val = len(ds) // 10\n",
        "        len_train = len(ds) - len_val\n",
        "        indices = list(range(len(ds)))\n",
        "        train_indices = indices[:len_train]\n",
        "        val_indices = indices[len_train:]\n",
        "        ds_train = Subset(ds, train_indices)\n",
        "        ds_val = Subset(ds, val_indices)\n",
        "        trainloaders.append(DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True))\n",
        "        valloaders.append(DataLoader(ds_val, batch_size=BATCH_SIZE))\n",
        "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
        "    return trainloaders, valloaders, testloader, datasets, testset, train_removed_percentage, test_removed_percentage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mbev__z_30S"
      },
      "source": [
        "we can can two major insights from this graph:\n",
        "\n",
        "a. Based on the bar graph image you've provided, the distribution appears to be homogeneous. Each client has data across all the different classes (digits 0-9), as indicated by the presence of different colors (representing different clients) within each digit's bar.\n",
        "\n",
        "b. The graph and the code imply an IID distribution. In the context of the MNIST dataset, each client's data is a random subset of the entire dataset, maintaining the overall distribution of labels. The IID distribution is supported by the fact that each digit is represented fairly evenly across all clients, as shown by the similar heights of different colors within each bar on the graph. Non-IID would show a significant imbalance, with some clients potentially having more of certain digits and less of others.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-nt28wZ0fFW"
      },
      "source": [
        "## Centralized Training with pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd07KFPmzAD4"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 7 * 7)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUfLhhn10nhu"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.optim import Adam\n",
        "from torch.nn.functional import relu, softmax\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def train(net, trainloader, validloader, testloader, epochs, device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "\n",
        "    best_valid_loss = float('inf')\n",
        "    best_model = None\n",
        "    best_test_loss = float('inf')\n",
        "    best_test_accuracy = 0.0\n",
        "    patience_counter = 0\n",
        "    early_stopping_epoch = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in trainloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(trainloader)\n",
        "        train_acc = correct / total\n",
        "\n",
        "        net.eval()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in validloader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = net(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        valid_loss = running_loss / len(validloader)\n",
        "        valid_acc = correct / total\n",
        "\n",
        "        # Calculate test loss and accuracy\n",
        "        with torch.no_grad():\n",
        "            test_loss, test_accuracy = test(net, testloader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.4f}\")\n",
        "\n",
        "        scheduler.step(valid_loss)\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            best_model = net.state_dict()\n",
        "            best_test_loss = test_loss\n",
        "            best_test_accuracy = test_accuracy\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= 10:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            early_stopping_epoch = epoch + 1\n",
        "            break\n",
        "\n",
        "    net.load_state_dict(best_model)\n",
        "    return net, early_stopping_epoch, best_test_loss, best_test_accuracy\n",
        "\n",
        "def test(net, testloader):\n",
        "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    correct, total, loss = 0, 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    loss /= len(testloader.dataset)\n",
        "    accuracy = correct / total\n",
        "    return loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBjS0UIw17RR",
        "outputId": "4df11fa0-efb3-469f-a4a7-01807903f778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with 8 PCA components...\n",
            "Epoch 1/100 - Train Loss: 0.8130, Train Acc: 0.7415, Valid Loss: 0.2556, Valid Acc: 0.9133, Test Loss: 0.0080, Test Acc: 0.9272\n",
            "Epoch 2/100 - Train Loss: 0.2272, Train Acc: 0.9291, Valid Loss: 0.1621, Valid Acc: 0.9517, Test Loss: 0.0043, Test Acc: 0.9613\n",
            "Epoch 3/100 - Train Loss: 0.1529, Train Acc: 0.9531, Valid Loss: 0.1085, Valid Acc: 0.9733, Test Loss: 0.0033, Test Acc: 0.9647\n",
            "Epoch 4/100 - Train Loss: 0.1033, Train Acc: 0.9654, Valid Loss: 0.1089, Valid Acc: 0.9733, Test Loss: 0.0030, Test Acc: 0.9694\n",
            "Epoch 5/100 - Train Loss: 0.0770, Train Acc: 0.9750, Valid Loss: 0.0960, Valid Acc: 0.9733, Test Loss: 0.0024, Test Acc: 0.9759\n",
            "Epoch 6/100 - Train Loss: 0.0547, Train Acc: 0.9830, Valid Loss: 0.1007, Valid Acc: 0.9783, Test Loss: 0.0023, Test Acc: 0.9772\n",
            "Epoch 7/100 - Train Loss: 0.0439, Train Acc: 0.9856, Valid Loss: 0.1127, Valid Acc: 0.9683, Test Loss: 0.0028, Test Acc: 0.9714\n",
            "Epoch 8/100 - Train Loss: 0.0362, Train Acc: 0.9881, Valid Loss: 0.1093, Valid Acc: 0.9783, Test Loss: 0.0023, Test Acc: 0.9780\n",
            "Epoch 9/100 - Train Loss: 0.0382, Train Acc: 0.9874, Valid Loss: 0.0967, Valid Acc: 0.9783, Test Loss: 0.0025, Test Acc: 0.9774\n",
            "Epoch 10/100 - Train Loss: 0.0285, Train Acc: 0.9915, Valid Loss: 0.0987, Valid Acc: 0.9833, Test Loss: 0.0022, Test Acc: 0.9800\n",
            "Epoch 11/100 - Train Loss: 0.0206, Train Acc: 0.9944, Valid Loss: 0.0962, Valid Acc: 0.9850, Test Loss: 0.0023, Test Acc: 0.9802\n",
            "Epoch 12/100 - Train Loss: 0.0098, Train Acc: 0.9974, Valid Loss: 0.0914, Valid Acc: 0.9833, Test Loss: 0.0022, Test Acc: 0.9813\n",
            "Epoch 13/100 - Train Loss: 0.0088, Train Acc: 0.9980, Valid Loss: 0.0921, Valid Acc: 0.9833, Test Loss: 0.0022, Test Acc: 0.9815\n",
            "Epoch 14/100 - Train Loss: 0.0089, Train Acc: 0.9974, Valid Loss: 0.0927, Valid Acc: 0.9833, Test Loss: 0.0022, Test Acc: 0.9816\n",
            "Epoch 15/100 - Train Loss: 0.0080, Train Acc: 0.9987, Valid Loss: 0.0976, Valid Acc: 0.9833, Test Loss: 0.0022, Test Acc: 0.9814\n",
            "Epoch 16/100 - Train Loss: 0.0071, Train Acc: 0.9989, Valid Loss: 0.0967, Valid Acc: 0.9850, Test Loss: 0.0023, Test Acc: 0.9815\n",
            "Epoch 17/100 - Train Loss: 0.0067, Train Acc: 0.9985, Valid Loss: 0.0947, Valid Acc: 0.9850, Test Loss: 0.0022, Test Acc: 0.9818\n",
            "Epoch 18/100 - Train Loss: 0.0067, Train Acc: 0.9985, Valid Loss: 0.0977, Valid Acc: 0.9867, Test Loss: 0.0022, Test Acc: 0.9823\n",
            "Epoch 19/100 - Train Loss: 0.0057, Train Acc: 0.9991, Valid Loss: 0.0976, Valid Acc: 0.9867, Test Loss: 0.0022, Test Acc: 0.9821\n",
            "Epoch 20/100 - Train Loss: 0.0054, Train Acc: 0.9991, Valid Loss: 0.0974, Valid Acc: 0.9867, Test Loss: 0.0022, Test Acc: 0.9822\n",
            "Epoch 21/100 - Train Loss: 0.0047, Train Acc: 0.9993, Valid Loss: 0.0968, Valid Acc: 0.9867, Test Loss: 0.0022, Test Acc: 0.9822\n",
            "Epoch 22/100 - Train Loss: 0.0052, Train Acc: 0.9993, Valid Loss: 0.0973, Valid Acc: 0.9867, Test Loss: 0.0022, Test Acc: 0.9819\n",
            "Early stopping at epoch 22\n",
            "Final test set performance for 8 PCA components: Loss 0.0022, Accuracy 0.9813\n",
            "Early stopping at epoch: 22\n",
            "\n",
            "Training with 16 PCA components...\n",
            "Epoch 1/100 - Train Loss: 0.8000, Train Acc: 0.7439, Valid Loss: 0.2190, Valid Acc: 0.9317, Test Loss: 0.0067, Test Acc: 0.9367\n",
            "Epoch 2/100 - Train Loss: 0.2251, Train Acc: 0.9333, Valid Loss: 0.1591, Valid Acc: 0.9500, Test Loss: 0.0044, Test Acc: 0.9595\n",
            "Epoch 3/100 - Train Loss: 0.1412, Train Acc: 0.9570, Valid Loss: 0.1329, Valid Acc: 0.9717, Test Loss: 0.0039, Test Acc: 0.9588\n",
            "Epoch 4/100 - Train Loss: 0.1071, Train Acc: 0.9707, Valid Loss: 0.1003, Valid Acc: 0.9700, Test Loss: 0.0027, Test Acc: 0.9717\n",
            "Epoch 5/100 - Train Loss: 0.0796, Train Acc: 0.9767, Valid Loss: 0.0980, Valid Acc: 0.9683, Test Loss: 0.0027, Test Acc: 0.9704\n",
            "Epoch 6/100 - Train Loss: 0.0654, Train Acc: 0.9785, Valid Loss: 0.1120, Valid Acc: 0.9667, Test Loss: 0.0028, Test Acc: 0.9694\n",
            "Epoch 7/100 - Train Loss: 0.0491, Train Acc: 0.9854, Valid Loss: 0.1007, Valid Acc: 0.9700, Test Loss: 0.0025, Test Acc: 0.9743\n",
            "Epoch 8/100 - Train Loss: 0.0369, Train Acc: 0.9870, Valid Loss: 0.0935, Valid Acc: 0.9800, Test Loss: 0.0026, Test Acc: 0.9736\n",
            "Epoch 9/100 - Train Loss: 0.0324, Train Acc: 0.9893, Valid Loss: 0.0933, Valid Acc: 0.9767, Test Loss: 0.0023, Test Acc: 0.9780\n",
            "Epoch 10/100 - Train Loss: 0.0280, Train Acc: 0.9907, Valid Loss: 0.1086, Valid Acc: 0.9750, Test Loss: 0.0031, Test Acc: 0.9686\n",
            "Epoch 11/100 - Train Loss: 0.0289, Train Acc: 0.9907, Valid Loss: 0.1052, Valid Acc: 0.9783, Test Loss: 0.0026, Test Acc: 0.9760\n",
            "Epoch 12/100 - Train Loss: 0.0205, Train Acc: 0.9944, Valid Loss: 0.0951, Valid Acc: 0.9833, Test Loss: 0.0024, Test Acc: 0.9789\n",
            "Epoch 13/100 - Train Loss: 0.0313, Train Acc: 0.9885, Valid Loss: 0.0999, Valid Acc: 0.9733, Test Loss: 0.0024, Test Acc: 0.9786\n",
            "Epoch 14/100 - Train Loss: 0.0180, Train Acc: 0.9941, Valid Loss: 0.0974, Valid Acc: 0.9767, Test Loss: 0.0022, Test Acc: 0.9804\n",
            "Epoch 15/100 - Train Loss: 0.0150, Train Acc: 0.9950, Valid Loss: 0.0817, Valid Acc: 0.9833, Test Loss: 0.0023, Test Acc: 0.9806\n",
            "Epoch 16/100 - Train Loss: 0.0158, Train Acc: 0.9941, Valid Loss: 0.0926, Valid Acc: 0.9800, Test Loss: 0.0022, Test Acc: 0.9811\n",
            "Epoch 17/100 - Train Loss: 0.0149, Train Acc: 0.9944, Valid Loss: 0.0981, Valid Acc: 0.9783, Test Loss: 0.0022, Test Acc: 0.9808\n",
            "Epoch 18/100 - Train Loss: 0.0200, Train Acc: 0.9933, Valid Loss: 0.1233, Valid Acc: 0.9717, Test Loss: 0.0028, Test Acc: 0.9769\n",
            "Epoch 19/100 - Train Loss: 0.0145, Train Acc: 0.9939, Valid Loss: 0.0989, Valid Acc: 0.9783, Test Loss: 0.0026, Test Acc: 0.9768\n",
            "Epoch 20/100 - Train Loss: 0.0162, Train Acc: 0.9946, Valid Loss: 0.1096, Valid Acc: 0.9750, Test Loss: 0.0027, Test Acc: 0.9779\n",
            "Epoch 21/100 - Train Loss: 0.0111, Train Acc: 0.9963, Valid Loss: 0.0870, Valid Acc: 0.9800, Test Loss: 0.0026, Test Acc: 0.9790\n",
            "Epoch 22/100 - Train Loss: 0.0067, Train Acc: 0.9981, Valid Loss: 0.0940, Valid Acc: 0.9750, Test Loss: 0.0024, Test Acc: 0.9808\n",
            "Epoch 23/100 - Train Loss: 0.0047, Train Acc: 0.9991, Valid Loss: 0.0944, Valid Acc: 0.9767, Test Loss: 0.0024, Test Acc: 0.9807\n",
            "Epoch 24/100 - Train Loss: 0.0044, Train Acc: 0.9991, Valid Loss: 0.0955, Valid Acc: 0.9767, Test Loss: 0.0024, Test Acc: 0.9814\n",
            "Epoch 25/100 - Train Loss: 0.0042, Train Acc: 0.9991, Valid Loss: 0.1049, Valid Acc: 0.9767, Test Loss: 0.0024, Test Acc: 0.9814\n",
            "Early stopping at epoch 25\n",
            "Final test set performance for 16 PCA components: Loss 0.0023, Accuracy 0.9806\n",
            "Early stopping at epoch: 25\n",
            "\n",
            "Training with 32 PCA components...\n",
            "Epoch 1/100 - Train Loss: 0.8513, Train Acc: 0.7267, Valid Loss: 0.2264, Valid Acc: 0.9333, Test Loss: 0.0071, Test Acc: 0.9375\n",
            "Epoch 2/100 - Train Loss: 0.2168, Train Acc: 0.9315, Valid Loss: 0.1598, Valid Acc: 0.9600, Test Loss: 0.0045, Test Acc: 0.9578\n",
            "Epoch 3/100 - Train Loss: 0.1439, Train Acc: 0.9550, Valid Loss: 0.1244, Valid Acc: 0.9683, Test Loss: 0.0035, Test Acc: 0.9662\n",
            "Epoch 4/100 - Train Loss: 0.1080, Train Acc: 0.9665, Valid Loss: 0.1062, Valid Acc: 0.9700, Test Loss: 0.0030, Test Acc: 0.9704\n",
            "Epoch 5/100 - Train Loss: 0.0844, Train Acc: 0.9750, Valid Loss: 0.1113, Valid Acc: 0.9750, Test Loss: 0.0026, Test Acc: 0.9742\n",
            "Epoch 6/100 - Train Loss: 0.0761, Train Acc: 0.9754, Valid Loss: 0.1170, Valid Acc: 0.9783, Test Loss: 0.0027, Test Acc: 0.9714\n",
            "Epoch 7/100 - Train Loss: 0.0537, Train Acc: 0.9794, Valid Loss: 0.1024, Valid Acc: 0.9717, Test Loss: 0.0023, Test Acc: 0.9781\n",
            "Epoch 8/100 - Train Loss: 0.0472, Train Acc: 0.9846, Valid Loss: 0.1035, Valid Acc: 0.9767, Test Loss: 0.0024, Test Acc: 0.9764\n",
            "Epoch 9/100 - Train Loss: 0.0427, Train Acc: 0.9856, Valid Loss: 0.1063, Valid Acc: 0.9800, Test Loss: 0.0020, Test Acc: 0.9795\n",
            "Epoch 10/100 - Train Loss: 0.0341, Train Acc: 0.9887, Valid Loss: 0.0876, Valid Acc: 0.9817, Test Loss: 0.0025, Test Acc: 0.9778\n",
            "Epoch 11/100 - Train Loss: 0.0317, Train Acc: 0.9898, Valid Loss: 0.1012, Valid Acc: 0.9817, Test Loss: 0.0023, Test Acc: 0.9771\n",
            "Epoch 12/100 - Train Loss: 0.0220, Train Acc: 0.9941, Valid Loss: 0.1278, Valid Acc: 0.9767, Test Loss: 0.0023, Test Acc: 0.9780\n",
            "Epoch 13/100 - Train Loss: 0.0218, Train Acc: 0.9944, Valid Loss: 0.1268, Valid Acc: 0.9783, Test Loss: 0.0022, Test Acc: 0.9799\n",
            "Epoch 14/100 - Train Loss: 0.0194, Train Acc: 0.9939, Valid Loss: 0.1153, Valid Acc: 0.9833, Test Loss: 0.0022, Test Acc: 0.9800\n",
            "Epoch 15/100 - Train Loss: 0.0226, Train Acc: 0.9922, Valid Loss: 0.1004, Valid Acc: 0.9817, Test Loss: 0.0025, Test Acc: 0.9775\n",
            "Epoch 16/100 - Train Loss: 0.0207, Train Acc: 0.9928, Valid Loss: 0.0979, Valid Acc: 0.9800, Test Loss: 0.0023, Test Acc: 0.9787\n",
            "Epoch 17/100 - Train Loss: 0.0136, Train Acc: 0.9957, Valid Loss: 0.1061, Valid Acc: 0.9817, Test Loss: 0.0021, Test Acc: 0.9819\n",
            "Epoch 18/100 - Train Loss: 0.0091, Train Acc: 0.9974, Valid Loss: 0.1072, Valid Acc: 0.9817, Test Loss: 0.0021, Test Acc: 0.9817\n",
            "Epoch 19/100 - Train Loss: 0.0084, Train Acc: 0.9976, Valid Loss: 0.1093, Valid Acc: 0.9833, Test Loss: 0.0021, Test Acc: 0.9815\n",
            "Epoch 20/100 - Train Loss: 0.0064, Train Acc: 0.9985, Valid Loss: 0.1122, Valid Acc: 0.9833, Test Loss: 0.0021, Test Acc: 0.9822\n",
            "Early stopping at epoch 20\n",
            "Final test set performance for 32 PCA components: Loss 0.0025, Accuracy 0.9778\n",
            "Early stopping at epoch: 20\n",
            "\n",
            "Training with 64 PCA components...\n",
            "Epoch 1/100 - Train Loss: 0.8502, Train Acc: 0.7320, Valid Loss: 0.2485, Valid Acc: 0.9367, Test Loss: 0.0077, Test Acc: 0.9328\n",
            "Epoch 2/100 - Train Loss: 0.2329, Train Acc: 0.9320, Valid Loss: 0.1515, Valid Acc: 0.9583, Test Loss: 0.0048, Test Acc: 0.9525\n",
            "Epoch 3/100 - Train Loss: 0.1494, Train Acc: 0.9530, Valid Loss: 0.1436, Valid Acc: 0.9633, Test Loss: 0.0036, Test Acc: 0.9639\n",
            "Epoch 4/100 - Train Loss: 0.1134, Train Acc: 0.9683, Valid Loss: 0.1022, Valid Acc: 0.9733, Test Loss: 0.0028, Test Acc: 0.9707\n",
            "Epoch 5/100 - Train Loss: 0.0867, Train Acc: 0.9726, Valid Loss: 0.1121, Valid Acc: 0.9667, Test Loss: 0.0028, Test Acc: 0.9743\n",
            "Epoch 6/100 - Train Loss: 0.0733, Train Acc: 0.9765, Valid Loss: 0.0884, Valid Acc: 0.9783, Test Loss: 0.0024, Test Acc: 0.9763\n",
            "Epoch 7/100 - Train Loss: 0.0571, Train Acc: 0.9830, Valid Loss: 0.0944, Valid Acc: 0.9733, Test Loss: 0.0025, Test Acc: 0.9751\n",
            "Epoch 8/100 - Train Loss: 0.0442, Train Acc: 0.9867, Valid Loss: 0.0879, Valid Acc: 0.9833, Test Loss: 0.0025, Test Acc: 0.9747\n",
            "Epoch 9/100 - Train Loss: 0.0433, Train Acc: 0.9852, Valid Loss: 0.0906, Valid Acc: 0.9767, Test Loss: 0.0025, Test Acc: 0.9756\n",
            "Epoch 10/100 - Train Loss: 0.0427, Train Acc: 0.9844, Valid Loss: 0.0717, Valid Acc: 0.9833, Test Loss: 0.0020, Test Acc: 0.9807\n",
            "Epoch 11/100 - Train Loss: 0.0291, Train Acc: 0.9911, Valid Loss: 0.1034, Valid Acc: 0.9700, Test Loss: 0.0026, Test Acc: 0.9750\n",
            "Epoch 12/100 - Train Loss: 0.0286, Train Acc: 0.9898, Valid Loss: 0.0867, Valid Acc: 0.9800, Test Loss: 0.0021, Test Acc: 0.9801\n",
            "Epoch 13/100 - Train Loss: 0.0231, Train Acc: 0.9926, Valid Loss: 0.0936, Valid Acc: 0.9783, Test Loss: 0.0026, Test Acc: 0.9759\n",
            "Epoch 14/100 - Train Loss: 0.0210, Train Acc: 0.9930, Valid Loss: 0.0948, Valid Acc: 0.9750, Test Loss: 0.0026, Test Acc: 0.9779\n",
            "Epoch 15/100 - Train Loss: 0.0210, Train Acc: 0.9920, Valid Loss: 0.1004, Valid Acc: 0.9783, Test Loss: 0.0026, Test Acc: 0.9794\n",
            "Epoch 16/100 - Train Loss: 0.0223, Train Acc: 0.9931, Valid Loss: 0.1100, Valid Acc: 0.9800, Test Loss: 0.0026, Test Acc: 0.9784\n",
            "Epoch 17/100 - Train Loss: 0.0087, Train Acc: 0.9972, Valid Loss: 0.0959, Valid Acc: 0.9867, Test Loss: 0.0021, Test Acc: 0.9815\n",
            "Epoch 18/100 - Train Loss: 0.0065, Train Acc: 0.9989, Valid Loss: 0.0944, Valid Acc: 0.9867, Test Loss: 0.0021, Test Acc: 0.9822\n",
            "Epoch 19/100 - Train Loss: 0.0067, Train Acc: 0.9983, Valid Loss: 0.0950, Valid Acc: 0.9867, Test Loss: 0.0021, Test Acc: 0.9825\n",
            "Epoch 20/100 - Train Loss: 0.0059, Train Acc: 0.9981, Valid Loss: 0.0928, Valid Acc: 0.9850, Test Loss: 0.0022, Test Acc: 0.9816\n",
            "Early stopping at epoch 20\n",
            "Final test set performance for 64 PCA components: Loss 0.0020, Accuracy 0.9807\n",
            "Early stopping at epoch: 20\n",
            "\n",
            "Training with 128 PCA components...\n",
            "Epoch 1/100 - Train Loss: 0.7865, Train Acc: 0.7507, Valid Loss: 0.2118, Valid Acc: 0.9450, Test Loss: 0.0067, Test Acc: 0.9405\n",
            "Epoch 2/100 - Train Loss: 0.2294, Train Acc: 0.9302, Valid Loss: 0.1402, Valid Acc: 0.9567, Test Loss: 0.0043, Test Acc: 0.9576\n",
            "Epoch 3/100 - Train Loss: 0.1432, Train Acc: 0.9543, Valid Loss: 0.1087, Valid Acc: 0.9717, Test Loss: 0.0029, Test Acc: 0.9714\n",
            "Epoch 4/100 - Train Loss: 0.1002, Train Acc: 0.9700, Valid Loss: 0.1068, Valid Acc: 0.9733, Test Loss: 0.0031, Test Acc: 0.9719\n",
            "Epoch 5/100 - Train Loss: 0.0839, Train Acc: 0.9750, Valid Loss: 0.0960, Valid Acc: 0.9733, Test Loss: 0.0025, Test Acc: 0.9747\n",
            "Epoch 6/100 - Train Loss: 0.0667, Train Acc: 0.9798, Valid Loss: 0.1085, Valid Acc: 0.9783, Test Loss: 0.0027, Test Acc: 0.9741\n",
            "Epoch 7/100 - Train Loss: 0.0562, Train Acc: 0.9831, Valid Loss: 0.0932, Valid Acc: 0.9800, Test Loss: 0.0026, Test Acc: 0.9745\n",
            "Epoch 8/100 - Train Loss: 0.0417, Train Acc: 0.9863, Valid Loss: 0.1007, Valid Acc: 0.9767, Test Loss: 0.0027, Test Acc: 0.9732\n",
            "Epoch 9/100 - Train Loss: 0.0439, Train Acc: 0.9863, Valid Loss: 0.0815, Valid Acc: 0.9800, Test Loss: 0.0021, Test Acc: 0.9790\n",
            "Epoch 10/100 - Train Loss: 0.0278, Train Acc: 0.9911, Valid Loss: 0.0936, Valid Acc: 0.9800, Test Loss: 0.0022, Test Acc: 0.9779\n",
            "Epoch 11/100 - Train Loss: 0.0302, Train Acc: 0.9902, Valid Loss: 0.1030, Valid Acc: 0.9767, Test Loss: 0.0023, Test Acc: 0.9777\n",
            "Epoch 12/100 - Train Loss: 0.0230, Train Acc: 0.9922, Valid Loss: 0.0998, Valid Acc: 0.9767, Test Loss: 0.0024, Test Acc: 0.9773\n",
            "Epoch 13/100 - Train Loss: 0.0227, Train Acc: 0.9913, Valid Loss: 0.1062, Valid Acc: 0.9800, Test Loss: 0.0023, Test Acc: 0.9784\n",
            "Epoch 14/100 - Train Loss: 0.0166, Train Acc: 0.9952, Valid Loss: 0.1105, Valid Acc: 0.9833, Test Loss: 0.0023, Test Acc: 0.9797\n",
            "Epoch 15/100 - Train Loss: 0.0187, Train Acc: 0.9939, Valid Loss: 0.1078, Valid Acc: 0.9767, Test Loss: 0.0029, Test Acc: 0.9753\n",
            "Epoch 16/100 - Train Loss: 0.0106, Train Acc: 0.9969, Valid Loss: 0.1062, Valid Acc: 0.9800, Test Loss: 0.0023, Test Acc: 0.9806\n",
            "Epoch 17/100 - Train Loss: 0.0076, Train Acc: 0.9981, Valid Loss: 0.0998, Valid Acc: 0.9767, Test Loss: 0.0023, Test Acc: 0.9809\n",
            "Epoch 18/100 - Train Loss: 0.0062, Train Acc: 0.9993, Valid Loss: 0.1005, Valid Acc: 0.9800, Test Loss: 0.0023, Test Acc: 0.9804\n",
            "Epoch 19/100 - Train Loss: 0.0044, Train Acc: 0.9994, Valid Loss: 0.0982, Valid Acc: 0.9783, Test Loss: 0.0023, Test Acc: 0.9813\n",
            "Early stopping at epoch 19\n",
            "Final test set performance for 128 PCA components: Loss 0.0021, Accuracy 0.9790\n",
            "Early stopping at epoch: 19\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#pca_components_range = [16, 32]  # Define the range of PCA components to try\n",
        "pca_components_range = [8, 16, 32, 64, 128]\n",
        "\n",
        "best_accuracy = 0\n",
        "best_pca_components = None\n",
        "\n",
        "for n_components in pca_components_range:\n",
        "    print(f\"Training with {n_components} PCA components...\")\n",
        "\n",
        "    pca = determine_pca_components(mnist_dataset.data.numpy(), n_components=n_components)\n",
        "    trainloaders, valloaders, testloader, datasets, testset, _, _ = load_datasets(pca, n_components)\n",
        "\n",
        "    train_removed_percentages = []\n",
        "    test_removed_percentages = []\n",
        "    for i in range(NUM_CLIENTS):\n",
        "        _, train_removed_percentage = apply_pca_locally(datasets[i].dataset.data.numpy(), pca, n_components)\n",
        "        _, test_removed_percentage = apply_pca_locally(testset.data.numpy(), pca, n_components)\n",
        "        train_removed_percentages.append(train_removed_percentage)\n",
        "        test_removed_percentages.append(test_removed_percentage)\n",
        "\n",
        "    trainloader = trainloaders[0]\n",
        "    valloader = valloaders[0]\n",
        "    net = Net().to(DEVICE)\n",
        "\n",
        "    # Training and evaluation loop\n",
        "    net, early_stopping_epoch, test_loss, test_accuracy = train(net, trainloader, valloader, testloader, epochs=100, device=DEVICE)\n",
        "    print(f\"Final test set performance for {n_components} PCA components: Loss {test_loss:.4f}, Accuracy {test_accuracy:.4f}\")\n",
        "    print(f\"Early stopping at epoch: {early_stopping_epoch}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFcjct4Z5NaV"
      },
      "source": [
        "## Federated Learning with Flower"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuISCyk1GJhs"
      },
      "source": [
        "This means that multiple clients (simulating a distributed data scenario) will train a neural network (Net) on their local datasets and send the updated parameters to a central server. The server then averages these parameters and distributes the result back to the clients for further training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lMl35ij923P-",
        "outputId": "9b04cdd0-7294-449b-8f3f-455ea0e36b08"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "\n",
        "# Model size calculation function\n",
        "def get_model_size(model):\n",
        "    buffer = io.BytesIO()\n",
        "    torch.save(model.state_dict(), buffer)\n",
        "    size = buffer.tell()  # Size in bytes\n",
        "    return size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u1Rw8e1B8rID"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "def get_parameters(net) -> List[np.ndarray]:\n",
        "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
        "\n",
        "def set_parameters(net, parameters: List[np.ndarray]):\n",
        "    params_dict = zip(net.state_dict().keys(), parameters)\n",
        "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "\n",
        "    # Print the expected parameter shapes and the received parameter shapes\n",
        "    print(\"Expected parameter shapes:\")\n",
        "    for name, param in net.named_parameters():\n",
        "        print(f\"{name}: {param.shape}\")\n",
        "\n",
        "    print(\"Received parameter shapes:\")\n",
        "    for name, param in state_dict.items():\n",
        "        print(f\"{name}: {param.shape}\")\n",
        "\n",
        "    # Try to load the state dictionary and handle shape mismatch\n",
        "    try:\n",
        "        net.load_state_dict(state_dict, strict=True)\n",
        "    except RuntimeError as e:\n",
        "        print(\"Shape mismatch occurred. Attempting to handle gracefully...\")\n",
        "\n",
        "        # Create a new state dictionary with matching shapes\n",
        "        new_state_dict = OrderedDict()\n",
        "        for (name, param), (state_name, state_param) in zip(net.named_parameters(), state_dict.items()):\n",
        "            if param.shape != state_param.shape:\n",
        "                print(f\"Shape mismatch for parameter '{name}'. Expected {param.shape}, got {state_param.shape}.\")\n",
        "                if param.numel() == state_param.numel():\n",
        "                    # Reshape the parameter if the total number of elements matches\n",
        "                    state_param = state_param.view(param.shape)\n",
        "                else:\n",
        "                    print(f\"Cannot reshape parameter '{name}' due to incompatible number of elements.\")\n",
        "                    continue\n",
        "            new_state_dict[name] = state_param\n",
        "\n",
        "        # Load the new state dictionary\n",
        "        net.load_state_dict(new_state_dict, strict=False)\n",
        "        print(\"Shape mismatch handled.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "X25vFAzOXdHl"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import flwr as fl\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, net, trainloader, valloader, testloader, pca, train_removed_percentage, test_removed_percentage, early_stopping_epoch, n_components):\n",
        "        self.net = net\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = valloader\n",
        "        self.testloader = testloader  # Add testloader attribute\n",
        "        self.pca = pca\n",
        "        self.train_removed_percentage = train_removed_percentage\n",
        "        self.test_removed_percentage = test_removed_percentage\n",
        "        self.early_stopping_epoch = early_stopping_epoch\n",
        "        self.n_components = n_components\n",
        "        self.original_model_size = get_model_size(net)\n",
        "        self.communication_rounds = 0\n",
        "        self.communication_cost = 0\n",
        "        self.latency = 0\n",
        "        self.test_accuracy = 0.0\n",
        "        self.test_loss = 0.0\n",
        "\n",
        "    def get_parameters(self, **kwargs):\n",
        "        return get_parameters(self.net)\n",
        "\n",
        "    def set_parameters(self, parameters):\n",
        "        set_parameters(self.net, parameters)\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        start_time = time.time()\n",
        "        self.set_parameters(parameters)\n",
        "        self.test_loss, self.test_accuracy = test(self.net, self.testloader)\n",
        "        epochs = config.get(\"epochs\", self.early_stopping_epoch)  # Use early_stopping_epoch as default if 'epochs' key is not present\n",
        "        self.net, _, _, _ = train(self.net, self.trainloader, self.valloader, self.testloader, epochs, DEVICE)\n",
        "\n",
        "        self.communication_rounds += 1\n",
        "        self.communication_cost += get_model_size(self.net) * (1 - self.train_removed_percentage / 100)\n",
        "        self.latency += (time.time() - start_time) * (1 - self.train_removed_percentage / 100)\n",
        "\n",
        "        return self.get_parameters(), len(self.trainloader.dataset), {\n",
        "            \"pca_reduction_percentage\": self.train_removed_percentage,\n",
        "            \"n_components\": self.n_components,\n",
        "            \"communication_rounds\": self.communication_rounds,\n",
        "            \"communication_cost\": self.communication_cost,\n",
        "            \"test_accuracy\": self.test_accuracy,\n",
        "            \"test_loss\": self.test_loss,\n",
        "            \"latency\": self.latency,\n",
        "        }\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        start_time = time.time()\n",
        "        self.set_parameters(parameters)\n",
        "        loss, accuracy = test(self.net, self.valloader)\n",
        "\n",
        "        self.communication_rounds += 1\n",
        "        self.communication_cost += get_model_size(self.net) * (1 - self.test_removed_percentage / 100)\n",
        "        self.latency += (time.time() - start_time) * (1 - self.test_removed_percentage / 100)\n",
        "\n",
        "        return float(loss), len(self.valloader.dataset), {\n",
        "            \"accuracy\": float(accuracy),\n",
        "            \"pca_reduction_percentage\": float(self.test_removed_percentage),\n",
        "            \"n_components\": float(self.n_components),\n",
        "            \"communication_rounds\": float(self.communication_rounds),\n",
        "            \"communication_cost\": float(self.communication_cost),\n",
        "            \"latency\": float(self.latency),\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0_chN5vWVBkH"
      },
      "outputs": [],
      "source": [
        "def client_fn(cid, pca, train_removed_percentage, test_removed_percentage, testloader, early_stopping_epoch, n_components):\n",
        "    model = Net().to(DEVICE)\n",
        "    trainloaders, valloaders, _, client_datasets, _, _, _ = load_datasets(pca, n_components)\n",
        "    client = FlowerClient(model, trainloaders[int(cid)], valloaders[int(cid)], testloader, pca, train_removed_percentage, test_removed_percentage, early_stopping_epoch, n_components)\n",
        "    return client.to_client()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Wg9jciqI87cm"
      },
      "outputs": [],
      "source": [
        "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
        "    accuracies = [num_examples * m.get(\"accuracy\", 0.0) for num_examples, m in metrics]\n",
        "    examples = [num_examples for num_examples, _ in metrics]\n",
        "    pca_reduction_percentages = [m.get(\"pca_reduction_percentage\", 0.0) for _, m in metrics]\n",
        "    communication_rounds = [m.get(\"communication_rounds\", 0.0) for _, m in metrics]\n",
        "    communication_cost = [m.get(\"communication_cost\", 0.0) for _, m in metrics]\n",
        "    test_accuracies = [m.get(\"test_accuracy\", 0.0) for _, m in metrics]\n",
        "    test_losses = [m.get(\"test_loss\", 0.0) for _, m in metrics]\n",
        "    latency = [m.get(\"latency\", 0.0) for _, m in metrics]\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": sum(accuracies) / sum(examples) if examples else 0.0,\n",
        "        \"pca_reduction_percentage\": sum(pca_reduction_percentages) / len(pca_reduction_percentages) if pca_reduction_percentages else 0.0,\n",
        "        \"communication_rounds\": sum(communication_rounds) / len(communication_rounds) if communication_rounds else 0.0,\n",
        "        \"communication_cost\": sum(communication_cost),\n",
        "        \"test_accuracy\": sum(test_accuracies) / len(test_accuracies) if test_accuracies else 0.0,\n",
        "        \"test_loss\": sum(test_losses) / len(test_losses) if test_losses else 0.0,\n",
        "        \"latency\": sum(latency) / len(latency) if latency else 0.0,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fH37lKV_87fc",
        "outputId": "a4642977-c7c2-4971-8d96-f001d98912a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with 8 PCA components...\n",
            "Epoch 1/100 - Train Loss: 0.8781, Train Acc: 0.7159, Valid Loss: 0.2619, Valid Acc: 0.9300, Test Loss: 0.0086, Test Acc: 0.9281\n",
            "Epoch 2/100 - Train Loss: 0.2685, Train Acc: 0.9170, Valid Loss: 0.1649, Valid Acc: 0.9550, Test Loss: 0.0045, Test Acc: 0.9592\n",
            "Epoch 3/100 - Train Loss: 0.1616, Train Acc: 0.9506, Valid Loss: 0.1327, Valid Acc: 0.9683, Test Loss: 0.0033, Test Acc: 0.9663\n",
            "Epoch 4/100 - Train Loss: 0.1126, Train Acc: 0.9619, Valid Loss: 0.1078, Valid Acc: 0.9717, Test Loss: 0.0031, Test Acc: 0.9690\n",
            "Epoch 5/100 - Train Loss: 0.0966, Train Acc: 0.9691, Valid Loss: 0.1063, Valid Acc: 0.9717, Test Loss: 0.0026, Test Acc: 0.9733\n",
            "Epoch 6/100 - Train Loss: 0.0737, Train Acc: 0.9776, Valid Loss: 0.0933, Valid Acc: 0.9833, Test Loss: 0.0024, Test Acc: 0.9772\n",
            "Epoch 7/100 - Train Loss: 0.0567, Train Acc: 0.9830, Valid Loss: 0.0973, Valid Acc: 0.9800, Test Loss: 0.0023, Test Acc: 0.9749\n",
            "Epoch 8/100 - Train Loss: 0.0507, Train Acc: 0.9811, Valid Loss: 0.0971, Valid Acc: 0.9800, Test Loss: 0.0025, Test Acc: 0.9756\n",
            "Epoch 9/100 - Train Loss: 0.0435, Train Acc: 0.9850, Valid Loss: 0.0836, Valid Acc: 0.9833, Test Loss: 0.0019, Test Acc: 0.9809\n",
            "Epoch 10/100 - Train Loss: 0.0324, Train Acc: 0.9881, Valid Loss: 0.1104, Valid Acc: 0.9717, Test Loss: 0.0027, Test Acc: 0.9750\n",
            "Epoch 11/100 - Train Loss: 0.0337, Train Acc: 0.9894, Valid Loss: 0.0905, Valid Acc: 0.9783, Test Loss: 0.0023, Test Acc: 0.9775\n",
            "Epoch 12/100 - Train Loss: 0.0292, Train Acc: 0.9909, Valid Loss: 0.0891, Valid Acc: 0.9817, Test Loss: 0.0021, Test Acc: 0.9789\n",
            "Epoch 13/100 - Train Loss: 0.0206, Train Acc: 0.9928, Valid Loss: 0.1145, Valid Acc: 0.9833, Test Loss: 0.0023, Test Acc: 0.9796\n",
            "Epoch 14/100 - Train Loss: 0.0225, Train Acc: 0.9930, Valid Loss: 0.0952, Valid Acc: 0.9767, Test Loss: 0.0024, Test Acc: 0.9770\n",
            "Epoch 15/100 - Train Loss: 0.0210, Train Acc: 0.9931, Valid Loss: 0.1208, Valid Acc: 0.9750, Test Loss: 0.0028, Test Acc: 0.9753\n",
            "Epoch 16/100 - Train Loss: 0.0146, Train Acc: 0.9954, Valid Loss: 0.0963, Valid Acc: 0.9817, Test Loss: 0.0022, Test Acc: 0.9809\n",
            "Epoch 17/100 - Train Loss: 0.0093, Train Acc: 0.9980, Valid Loss: 0.0978, Valid Acc: 0.9850, Test Loss: 0.0021, Test Acc: 0.9810\n",
            "Epoch 18/100 - Train Loss: 0.0078, Train Acc: 0.9985, Valid Loss: 0.0970, Valid Acc: 0.9817, Test Loss: 0.0022, Test Acc: 0.9811\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      Starting Flower simulation, config: num_rounds=19, no round_timeout\n",
            "INFO:flwr:Starting Flower simulation, config: num_rounds=19, no round_timeout\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/100 - Train Loss: 0.0068, Train Acc: 0.9985, Valid Loss: 0.0985, Valid Acc: 0.9850, Test Loss: 0.0021, Test Acc: 0.9814\n",
            "Early stopping at epoch 19\n",
            "Early stopping epoch for 8 PCA components: 19\n",
            "Test Loss: 0.0019\n",
            "Test Accuracy: 0.9809\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = _posixsubprocess.fork_exec(\n",
            "2024-04-17 16:09:27,229\tINFO worker.py:1621 -- Started a local Ray instance.\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Ray initialized with resources: {'object_store_memory': 3932070297.0, 'memory': 7864140596.0, 'node:172.28.0.12': 1.0, 'node:__internal_head__': 1.0, 'CPU': 2.0}\n",
            "INFO:flwr:Flower VCE: Ray initialized with resources: {'object_store_memory': 3932070297.0, 'memory': 7864140596.0, 'node:172.28.0.12': 1.0, 'node:__internal_head__': 1.0, 'CPU': 2.0}\n",
            "\u001b[92mINFO \u001b[0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
            "INFO:flwr:Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
            "\u001b[92mINFO \u001b[0m:      No `client_resources` specified. Using minimal resources for clients.\n",
            "INFO:flwr:No `client_resources` specified. Using minimal resources for clients.\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
            "INFO:flwr:Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Creating VirtualClientEngineActorPool with 2 actors\n",
            "INFO:flwr:Flower VCE: Creating VirtualClientEngineActorPool with 2 actors\n",
            "\u001b[92mINFO \u001b[0m:      [INIT]\n",
            "INFO:flwr:[INIT]\n",
            "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
            "INFO:flwr:Requesting initial parameters from one random client\n",
            "\u001b[2m\u001b[36m(pid=8455)\u001b[0m 2024-04-17 16:09:35.564785: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=8455)\u001b[0m 2024-04-17 16:09:35.564873: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=8455)\u001b[0m 2024-04-17 16:09:35.568182: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=8455)\u001b[0m 2024-04-17 16:09:38.546331: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
            "INFO:flwr:Received initial parameters from one random client\n",
            "\u001b[92mINFO \u001b[0m:      Evaluating initial global parameters\n",
            "INFO:flwr:Evaluating initial global parameters\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "INFO:flwr:\n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
            "INFO:flwr:[ROUND 1]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n",
            "INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m fc2.bias: torch.Size([10])\u001b[32m [repeated 16x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 1/19 - Train Loss: 0.7855, Train Acc: 0.7556, Valid Loss: 0.2558, Valid Acc: 0.9200, Test Loss: 0.0067, Test Acc: 0.9388\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 1/19 - Train Loss: 0.7722, Train Acc: 0.7617, Valid Loss: 0.2762, Valid Acc: 0.9100, Test Loss: 0.0075, Test Acc: 0.9290\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 2/19 - Train Loss: 0.2063, Train Acc: 0.9393, Valid Loss: 0.1457, Valid Acc: 0.9483, Test Loss: 0.0038, Test Acc: 0.9638\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 2/19 - Train Loss: 0.2126, Train Acc: 0.9361, Valid Loss: 0.1344, Valid Acc: 0.9600, Test Loss: 0.0038, Test Acc: 0.9624\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 3/19 - Train Loss: 0.1304, Train Acc: 0.9611, Valid Loss: 0.1262, Valid Acc: 0.9500, Test Loss: 0.0031, Test Acc: 0.9695\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 3/19 - Train Loss: 0.1344, Train Acc: 0.9589, Valid Loss: 0.1250, Valid Acc: 0.9650, Test Loss: 0.0031, Test Acc: 0.9670\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 4/19 - Train Loss: 0.1008, Train Acc: 0.9689, Valid Loss: 0.1008, Valid Acc: 0.9650, Test Loss: 0.0029, Test Acc: 0.9718\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 4/19 - Train Loss: 0.1032, Train Acc: 0.9696, Valid Loss: 0.1137, Valid Acc: 0.9650, Test Loss: 0.0026, Test Acc: 0.9728\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 5/19 - Train Loss: 0.0778, Train Acc: 0.9772, Valid Loss: 0.0904, Valid Acc: 0.9733, Test Loss: 0.0023, Test Acc: 0.9775\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 5/19 - Train Loss: 0.0771, Train Acc: 0.9769, Valid Loss: 0.1194, Valid Acc: 0.9633, Test Loss: 0.0026, Test Acc: 0.9720\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 6/19 - Train Loss: 0.0596, Train Acc: 0.9804, Valid Loss: 0.0837, Valid Acc: 0.9717, Test Loss: 0.0023, Test Acc: 0.9771\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 6/19 - Train Loss: 0.0613, Train Acc: 0.9817, Valid Loss: 0.1190, Valid Acc: 0.9700, Test Loss: 0.0023, Test Acc: 0.9764\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 7/19 - Train Loss: 0.0504, Train Acc: 0.9831, Valid Loss: 0.1047, Valid Acc: 0.9683, Test Loss: 0.0026, Test Acc: 0.9746\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 7/19 - Train Loss: 0.0468, Train Acc: 0.9844, Valid Loss: 0.1168, Valid Acc: 0.9650, Test Loss: 0.0027, Test Acc: 0.9741\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 8/19 - Train Loss: 0.0473, Train Acc: 0.9848, Valid Loss: 0.0942, Valid Acc: 0.9700, Test Loss: 0.0022, Test Acc: 0.9775\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 8/19 - Train Loss: 0.0537, Train Acc: 0.9820, Valid Loss: 0.1060, Valid Acc: 0.9733, Test Loss: 0.0022, Test Acc: 0.9761\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 9/19 - Train Loss: 0.0372, Train Acc: 0.9885, Valid Loss: 0.1039, Valid Acc: 0.9750, Test Loss: 0.0023, Test Acc: 0.9790\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 9/19 - Train Loss: 0.0348, Train Acc: 0.9898, Valid Loss: 0.1029, Valid Acc: 0.9717, Test Loss: 0.0021, Test Acc: 0.9790\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 10/19 - Train Loss: 0.0367, Train Acc: 0.9874, Valid Loss: 0.0826, Valid Acc: 0.9700, Test Loss: 0.0023, Test Acc: 0.9777\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 10/19 - Train Loss: 0.0292, Train Acc: 0.9922, Valid Loss: 0.1019, Valid Acc: 0.9683, Test Loss: 0.0023, Test Acc: 0.9762\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 11/19 - Train Loss: 0.0247, Train Acc: 0.9920, Valid Loss: 0.0950, Valid Acc: 0.9700, Test Loss: 0.0022, Test Acc: 0.9794\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 11/19 - Train Loss: 0.0298, Train Acc: 0.9898, Valid Loss: 0.0920, Valid Acc: 0.9783, Test Loss: 0.0024, Test Acc: 0.9775\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 12/19 - Train Loss: 0.0198, Train Acc: 0.9933, Valid Loss: 0.0972, Valid Acc: 0.9700, Test Loss: 0.0023, Test Acc: 0.9791\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 12/19 - Train Loss: 0.0223, Train Acc: 0.9915, Valid Loss: 0.1025, Valid Acc: 0.9717, Test Loss: 0.0022, Test Acc: 0.9791\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 13/19 - Train Loss: 0.0223, Train Acc: 0.9917, Valid Loss: 0.1118, Valid Acc: 0.9767, Test Loss: 0.0025, Test Acc: 0.9772\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 13/19 - Train Loss: 0.0253, Train Acc: 0.9907, Valid Loss: 0.0973, Valid Acc: 0.9750, Test Loss: 0.0022, Test Acc: 0.9793\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 14/19 - Train Loss: 0.0171, Train Acc: 0.9948, Valid Loss: 0.1193, Valid Acc: 0.9650, Test Loss: 0.0024, Test Acc: 0.9789\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 15/19 - Train Loss: 0.0177, Train Acc: 0.9939, Valid Loss: 0.1166, Valid Acc: 0.9667, Test Loss: 0.0023, Test Acc: 0.9804\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 15/19 - Train Loss: 0.0168, Train Acc: 0.9948, Valid Loss: 0.1034, Valid Acc: 0.9767, Test Loss: 0.0028, Test Acc: 0.9748\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 16/19 - Train Loss: 0.0142, Train Acc: 0.9957, Valid Loss: 0.1227, Valid Acc: 0.9750, Test Loss: 0.0023, Test Acc: 0.9793\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 16/19 - Train Loss: 0.0133, Train Acc: 0.9959, Valid Loss: 0.0988, Valid Acc: 0.9783, Test Loss: 0.0024, Test Acc: 0.9786\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 17/19 - Train Loss: 0.0085, Train Acc: 0.9974, Valid Loss: 0.1097, Valid Acc: 0.9750, Test Loss: 0.0022, Test Acc: 0.9805\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 18/19 - Train Loss: 0.0066, Train Acc: 0.9987, Valid Loss: 0.1094, Valid Acc: 0.9767, Test Loss: 0.0022, Test Acc: 0.9806\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 19/19 - Train Loss: 0.0190, Train Acc: 0.9935, Valid Loss: 0.0749, Valid Acc: 0.9800, Test Loss: 0.0023, Test Acc: 0.9788\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 1/19 - Train Loss: 0.7751, Train Acc: 0.7589, Valid Loss: 0.2311, Valid Acc: 0.9150, Test Loss: 0.0073, Test Acc: 0.9279\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m fc2.bias: torch.Size([10])\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 2/19 - Train Loss: 0.2134, Train Acc: 0.9356, Valid Loss: 0.1246, Valid Acc: 0.9533, Test Loss: 0.0041, Test Acc: 0.9620\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 3/19 - Train Loss: 0.1399, Train Acc: 0.9581, Valid Loss: 0.0833, Valid Acc: 0.9750, Test Loss: 0.0030, Test Acc: 0.9692\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 4/19 - Train Loss: 0.0947, Train Acc: 0.9724, Valid Loss: 0.0772, Valid Acc: 0.9783, Test Loss: 0.0028, Test Acc: 0.9704\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 5/19 - Train Loss: 0.0753, Train Acc: 0.9772, Valid Loss: 0.0737, Valid Acc: 0.9767, Test Loss: 0.0026, Test Acc: 0.9719\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 6/19 - Train Loss: 0.0611, Train Acc: 0.9815, Valid Loss: 0.0489, Valid Acc: 0.9817, Test Loss: 0.0023, Test Acc: 0.9775\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 7/19 - Train Loss: 0.0486, Train Acc: 0.9852, Valid Loss: 0.0529, Valid Acc: 0.9817, Test Loss: 0.0021, Test Acc: 0.9784\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 8/19 - Train Loss: 0.0439, Train Acc: 0.9850, Valid Loss: 0.0793, Valid Acc: 0.9717, Test Loss: 0.0022, Test Acc: 0.9757\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 9/19 - Train Loss: 0.0335, Train Acc: 0.9898, Valid Loss: 0.0642, Valid Acc: 0.9817, Test Loss: 0.0028, Test Acc: 0.9718\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 10/19 - Train Loss: 0.0395, Train Acc: 0.9857, Valid Loss: 0.0600, Valid Acc: 0.9833, Test Loss: 0.0022, Test Acc: 0.9783\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 11/19 - Train Loss: 0.0231, Train Acc: 0.9930, Valid Loss: 0.0472, Valid Acc: 0.9850, Test Loss: 0.0021, Test Acc: 0.9804\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 12/19 - Train Loss: 0.0224, Train Acc: 0.9926, Valid Loss: 0.0993, Valid Acc: 0.9733, Test Loss: 0.0030, Test Acc: 0.9715\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 13/19 - Train Loss: 0.0213, Train Acc: 0.9928, Valid Loss: 0.0494, Valid Acc: 0.9817, Test Loss: 0.0022, Test Acc: 0.9798\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 14/19 - Train Loss: 0.0167, Train Acc: 0.9952, Valid Loss: 0.0423, Valid Acc: 0.9833, Test Loss: 0.0021, Test Acc: 0.9797\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 15/19 - Train Loss: 0.0175, Train Acc: 0.9939, Valid Loss: 0.0574, Valid Acc: 0.9817, Test Loss: 0.0021, Test Acc: 0.9812\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 16/19 - Train Loss: 0.0125, Train Acc: 0.9965, Valid Loss: 0.0482, Valid Acc: 0.9867, Test Loss: 0.0023, Test Acc: 0.9819\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 17/19 - Train Loss: 0.0193, Train Acc: 0.9939, Valid Loss: 0.0421, Valid Acc: 0.9883, Test Loss: 0.0022, Test Acc: 0.9813\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 18/19 - Train Loss: 0.0087, Train Acc: 0.9981, Valid Loss: 0.0513, Valid Acc: 0.9850, Test Loss: 0.0025, Test Acc: 0.9794\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 19/19 - Train Loss: 0.0169, Train Acc: 0.9941, Valid Loss: 0.0586, Valid Acc: 0.9833, Test Loss: 0.0022, Test Acc: 0.9817\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 19/19 - Train Loss: 0.0118, Train Acc: 0.9961, Valid Loss: 0.0473, Valid Acc: 0.9817, Test Loss: 0.0024, Test Acc: 0.9799\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 1/19 - Train Loss: 0.7266, Train Acc: 0.7706, Valid Loss: 0.2057, Valid Acc: 0.9417, Test Loss: 0.0066, Test Acc: 0.9423\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc2.bias: torch.Size([10])\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 2/19 - Train Loss: 0.1954, Train Acc: 0.9431, Valid Loss: 0.1253, Valid Acc: 0.9617, Test Loss: 0.0041, Test Acc: 0.9596\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 2/19 - Train Loss: 0.1940, Train Acc: 0.9406, Valid Loss: 0.1478, Valid Acc: 0.9500, Test Loss: 0.0039, Test Acc: 0.9634\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 3/19 - Train Loss: 0.1198, Train Acc: 0.9619, Valid Loss: 0.1266, Valid Acc: 0.9633, Test Loss: 0.0035, Test Acc: 0.9635\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 4/19 - Train Loss: 0.0903, Train Acc: 0.9702, Valid Loss: 0.1118, Valid Acc: 0.9683, Test Loss: 0.0032, Test Acc: 0.9687\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 5/19 - Train Loss: 0.0640, Train Acc: 0.9794, Valid Loss: 0.1189, Valid Acc: 0.9600, Test Loss: 0.0027, Test Acc: 0.9712\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 5/19 - Train Loss: 0.0766, Train Acc: 0.9750, Valid Loss: 0.0966, Valid Acc: 0.9750, Test Loss: 0.0025, Test Acc: 0.9736\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 6/19 - Train Loss: 0.0476, Train Acc: 0.9859, Valid Loss: 0.0975, Valid Acc: 0.9683, Test Loss: 0.0026, Test Acc: 0.9772\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 7/19 - Train Loss: 0.0401, Train Acc: 0.9867, Valid Loss: 0.0852, Valid Acc: 0.9717, Test Loss: 0.0023, Test Acc: 0.9777\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 7/19 - Train Loss: 0.0466, Train Acc: 0.9844, Valid Loss: 0.1084, Valid Acc: 0.9733, Test Loss: 0.0026, Test Acc: 0.9745\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 8/19 - Train Loss: 0.0363, Train Acc: 0.9880, Valid Loss: 0.0825, Valid Acc: 0.9750, Test Loss: 0.0023, Test Acc: 0.9764\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 9/19 - Train Loss: 0.0291, Train Acc: 0.9902, Valid Loss: 0.1154, Valid Acc: 0.9717, Test Loss: 0.0030, Test Acc: 0.9722\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 9/19 - Train Loss: 0.0377, Train Acc: 0.9870, Valid Loss: 0.1021, Valid Acc: 0.9717, Test Loss: 0.0022, Test Acc: 0.9790\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 10/19 - Train Loss: 0.0283, Train Acc: 0.9906, Valid Loss: 0.1015, Valid Acc: 0.9667, Test Loss: 0.0024, Test Acc: 0.9752\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 11/19 - Train Loss: 0.0254, Train Acc: 0.9911, Valid Loss: 0.1342, Valid Acc: 0.9567, Test Loss: 0.0027, Test Acc: 0.9760\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 11/19 - Train Loss: 0.0265, Train Acc: 0.9913, Valid Loss: 0.1008, Valid Acc: 0.9700, Test Loss: 0.0021, Test Acc: 0.9785\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 12/19 - Train Loss: 0.0217, Train Acc: 0.9922, Valid Loss: 0.0977, Valid Acc: 0.9783, Test Loss: 0.0025, Test Acc: 0.9788\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 12/19 - Train Loss: 0.0137, Train Acc: 0.9970, Valid Loss: 0.0938, Valid Acc: 0.9733, Test Loss: 0.0019, Test Acc: 0.9810\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 13/19 - Train Loss: 0.0192, Train Acc: 0.9944, Valid Loss: 0.0890, Valid Acc: 0.9817, Test Loss: 0.0023, Test Acc: 0.9782\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 13/19 - Train Loss: 0.0104, Train Acc: 0.9978, Valid Loss: 0.0973, Valid Acc: 0.9733, Test Loss: 0.0019, Test Acc: 0.9812\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 14/19 - Train Loss: 0.0137, Train Acc: 0.9959, Valid Loss: 0.1004, Valid Acc: 0.9750, Test Loss: 0.0027, Test Acc: 0.9780\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 14/19 - Train Loss: 0.0089, Train Acc: 0.9985, Valid Loss: 0.0949, Valid Acc: 0.9733, Test Loss: 0.0020, Test Acc: 0.9813\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 15/19 - Train Loss: 0.0086, Train Acc: 0.9978, Valid Loss: 0.0862, Valid Acc: 0.9800, Test Loss: 0.0023, Test Acc: 0.9806\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 15/19 - Train Loss: 0.0072, Train Acc: 0.9993, Valid Loss: 0.1022, Valid Acc: 0.9717, Test Loss: 0.0020, Test Acc: 0.9811\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 16/19 - Train Loss: 0.0067, Train Acc: 0.9976, Valid Loss: 0.0858, Valid Acc: 0.9750, Test Loss: 0.0022, Test Acc: 0.9824\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 17/19 - Train Loss: 0.0052, Train Acc: 0.9985, Valid Loss: 0.0848, Valid Acc: 0.9783, Test Loss: 0.0022, Test Acc: 0.9828\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 17/19 - Train Loss: 0.0060, Train Acc: 0.9993, Valid Loss: 0.1047, Valid Acc: 0.9733, Test Loss: 0.0020, Test Acc: 0.9811\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 18/19 - Train Loss: 0.0051, Train Acc: 0.9987, Valid Loss: 0.0829, Valid Acc: 0.9783, Test Loss: 0.0022, Test Acc: 0.9826\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Early stopping at epoch 18\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 19/19 - Train Loss: 0.0057, Train Acc: 0.9993, Valid Loss: 0.1035, Valid Acc: 0.9733, Test Loss: 0.0021, Test Acc: 0.9808\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 2/19 - Train Loss: 0.2066, Train Acc: 0.9380, Valid Loss: 0.0995, Valid Acc: 0.9717, Test Loss: 0.0036, Test Acc: 0.9628\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 1/19 - Train Loss: 0.7778, Train Acc: 0.7565, Valid Loss: 0.2033, Valid Acc: 0.9417, Test Loss: 0.0066, Test Acc: 0.9434\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 3/19 - Train Loss: 0.1442, Train Acc: 0.9543, Valid Loss: 0.0952, Valid Acc: 0.9733, Test Loss: 0.0032, Test Acc: 0.9684\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 2/19 - Train Loss: 0.2084, Train Acc: 0.9391, Valid Loss: 0.1332, Valid Acc: 0.9633, Test Loss: 0.0044, Test Acc: 0.9582\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 4/19 - Train Loss: 0.1110, Train Acc: 0.9650, Valid Loss: 0.0898, Valid Acc: 0.9750, Test Loss: 0.0027, Test Acc: 0.9722\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 3/19 - Train Loss: 0.1384, Train Acc: 0.9593, Valid Loss: 0.0877, Valid Acc: 0.9700, Test Loss: 0.0031, Test Acc: 0.9691\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 5/19 - Train Loss: 0.0859, Train Acc: 0.9704, Valid Loss: 0.0858, Valid Acc: 0.9733, Test Loss: 0.0027, Test Acc: 0.9728\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 4/19 - Train Loss: 0.1145, Train Acc: 0.9657, Valid Loss: 0.0837, Valid Acc: 0.9733, Test Loss: 0.0029, Test Acc: 0.9701\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 6/19 - Train Loss: 0.0689, Train Acc: 0.9780, Valid Loss: 0.0583, Valid Acc: 0.9783, Test Loss: 0.0022, Test Acc: 0.9774\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 5/19 - Train Loss: 0.0825, Train Acc: 0.9756, Valid Loss: 0.1000, Valid Acc: 0.9650, Test Loss: 0.0031, Test Acc: 0.9681\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 7/19 - Train Loss: 0.0584, Train Acc: 0.9800, Valid Loss: 0.0720, Valid Acc: 0.9800, Test Loss: 0.0025, Test Acc: 0.9751\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 6/19 - Train Loss: 0.0702, Train Acc: 0.9772, Valid Loss: 0.0763, Valid Acc: 0.9733, Test Loss: 0.0025, Test Acc: 0.9722\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 8/19 - Train Loss: 0.0461, Train Acc: 0.9865, Valid Loss: 0.0532, Valid Acc: 0.9817, Test Loss: 0.0020, Test Acc: 0.9788\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 7/19 - Train Loss: 0.0521, Train Acc: 0.9856, Valid Loss: 0.0958, Valid Acc: 0.9667, Test Loss: 0.0027, Test Acc: 0.9717\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 9/19 - Train Loss: 0.0394, Train Acc: 0.9885, Valid Loss: 0.0597, Valid Acc: 0.9783, Test Loss: 0.0022, Test Acc: 0.9762\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 8/19 - Train Loss: 0.0477, Train Acc: 0.9839, Valid Loss: 0.0772, Valid Acc: 0.9700, Test Loss: 0.0025, Test Acc: 0.9730\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 10/19 - Train Loss: 0.0342, Train Acc: 0.9891, Valid Loss: 0.0727, Valid Acc: 0.9750, Test Loss: 0.0023, Test Acc: 0.9772\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 9/19 - Train Loss: 0.0369, Train Acc: 0.9891, Valid Loss: 0.0832, Valid Acc: 0.9683, Test Loss: 0.0025, Test Acc: 0.9756\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 11/19 - Train Loss: 0.0300, Train Acc: 0.9900, Valid Loss: 0.0882, Valid Acc: 0.9717, Test Loss: 0.0027, Test Acc: 0.9734\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 10/19 - Train Loss: 0.0324, Train Acc: 0.9893, Valid Loss: 0.0580, Valid Acc: 0.9783, Test Loss: 0.0022, Test Acc: 0.9764\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 12/19 - Train Loss: 0.0237, Train Acc: 0.9922, Valid Loss: 0.0571, Valid Acc: 0.9850, Test Loss: 0.0021, Test Acc: 0.9797\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8456)\u001b[0m Epoch 11/19 - Train Loss: 0.0270, Train Acc: 0.9913, Valid Loss: 0.0668, Valid Acc: 0.9733, Test Loss: 0.0024, Test Acc: 0.9768\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=8455)\u001b[0m Epoch 13/19 - Train Loss: 0.0235, Train Acc: 0.9924, Valid Loss: 0.0693, Valid Acc: 0.9783, Test Loss: 0.0022, Test Acc: 0.9773\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for n_components in pca_components_range:\n",
        "    print(f\"Training with {n_components} PCA components...\")\n",
        "\n",
        "    pca = determine_pca_components(mnist_dataset.data.numpy(), n_components=n_components)\n",
        "    trainloaders, valloaders, testloader, datasets, testset, _, _ = load_datasets(pca, n_components)\n",
        "\n",
        "    train_removed_percentages = []\n",
        "    test_removed_percentages = []\n",
        "    for i in range(NUM_CLIENTS):\n",
        "        _, train_removed_percentage = apply_pca_locally(datasets[i].dataset.data.numpy(), pca, n_components)\n",
        "        _, test_removed_percentage = apply_pca_locally(testset.data.numpy(), pca, n_components)\n",
        "        train_removed_percentages.append(train_removed_percentage)\n",
        "        test_removed_percentages.append(test_removed_percentage)\n",
        "\n",
        "    model = Net().to(DEVICE)\n",
        "    trainloader = trainloaders[0]\n",
        "    valloader = valloaders[0]\n",
        "    model, early_stopping_epoch, test_loss, test_accuracy = train(model, trainloader, valloader, testloader, epochs=100, device=DEVICE)\n",
        "\n",
        "    print(f\"Early stopping epoch for {n_components} PCA components: {early_stopping_epoch}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "    def client_fn_with_pca(cid):\n",
        "        return client_fn(cid, pca, train_removed_percentages[int(cid)], test_removed_percentages[int(cid)], testloader, early_stopping_epoch, n_components)\n",
        "\n",
        "    strategy = fl.server.strategy.FedAvg(\n",
        "        fraction_fit=1.0,\n",
        "        fraction_evaluate=0.5,\n",
        "        min_fit_clients=10,\n",
        "        min_evaluate_clients=5,\n",
        "        min_available_clients=10,\n",
        "        evaluate_metrics_aggregation_fn=weighted_average,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        history = fl.simulation.start_simulation(\n",
        "            client_fn=client_fn_with_pca,\n",
        "            num_clients=NUM_CLIENTS,\n",
        "            config=fl.server.ServerConfig(num_rounds=early_stopping_epoch),\n",
        "            strategy=strategy,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Simulation failed with error: {str(e)}\")\n",
        "        continue  # Continue with the next iteration if simulation fails\n",
        "\n",
        "    try:\n",
        "        if history.metrics_distributed and history.metrics_distributed[-1]:\n",
        "            final_metrics = history.metrics_distributed[-1][-1]\n",
        "            print(f\"Metrics at early stopping epoch {early_stopping_epoch} for {n_components} PCA components:\")\n",
        "            print(f\"  Accuracy: {final_metrics['accuracy']:.4f}\")\n",
        "            print(f\"  PCA Reduction Percentage: {final_metrics['pca_reduction_percentage']:.2f}%\")\n",
        "            print(f\"  Communication Rounds: {final_metrics['communication_rounds']:.2f}\")\n",
        "            print(f\"  Communication Cost: {final_metrics['communication_cost']:.2f} bytes\")\n",
        "            print(f\"  Test Loss: {final_metrics['test_loss']:.4f}\")\n",
        "            print(f\"  Test Accuracy: {final_metrics['test_accuracy']:.4f}\")\n",
        "            print(f\"  Latency: {final_metrics['latency']:.2f} seconds\")\n",
        "        else:\n",
        "            print(\"No metrics available.\")\n",
        "    except (KeyError, IndexError) as e:\n",
        "        print(f\"Error accessing metrics_distributed: {str(e)}\")\n",
        "\n",
        "    # Print the contents of history.metrics_distributed for debugging\n",
        "    print(\"Contents of history.metrics_distributed:\")\n",
        "    print(history.metrics_distributed)\n",
        "\n",
        "# Calculate average metrics for each PCA component\n",
        "\n",
        "# Calculate average metrics for each PCA component\n",
        "for n_components in pca_components_range:\n",
        "    print(f\"Metrics for {n_components} PCA components:\")\n",
        "    accuracies = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "    communication_costs = []\n",
        "    latencies = []\n",
        "\n",
        "    for epoch in range(1, early_stopping_epoch + 1):\n",
        "        if epoch <= len(history.metrics_distributed):\n",
        "            epoch_metrics = history.metrics_distributed[epoch - 1]\n",
        "            if epoch_metrics:\n",
        "                epoch_accuracies = [client[1].get(\"accuracy\", 0.0) for client in epoch_metrics]\n",
        "                accuracies.extend(epoch_accuracies)\n",
        "\n",
        "                epoch_test_losses = [client[1].get(\"test_loss\", 0.0) for client in epoch_metrics]\n",
        "                test_losses.extend(epoch_test_losses)\n",
        "\n",
        "                epoch_test_accuracies = [client[1].get(\"test_accuracy\", 0.0) for client in epoch_metrics]\n",
        "                test_accuracies.extend(epoch_test_accuracies)\n",
        "\n",
        "                epoch_communication_costs = [client[1].get(\"communication_cost\", 0.0) for client in epoch_metrics]\n",
        "                communication_costs.extend(epoch_communication_costs)\n",
        "\n",
        "                epoch_latencies = [client[1].get(\"latency\", 0.0) for client in epoch_metrics]\n",
        "                latencies.extend(epoch_latencies)\n",
        "\n",
        "    if accuracies:\n",
        "        avg_accuracy = sum(accuracies) / len(accuracies)\n",
        "        avg_test_loss = sum(test_losses) / len(test_losses)\n",
        "        avg_test_accuracy = sum(test_accuracies) / len(test_accuracies)\n",
        "        avg_communication_cost = sum(communication_costs) / len(communication_costs)\n",
        "        avg_latency = sum(latencies) / len(latencies)\n",
        "\n",
        "        print(f\"Average metrics for {n_components} PCA components (up to early stopping epoch {early_stopping_epoch}):\")\n",
        "        print(f\"  Average Accuracy: {avg_accuracy:.4f}\")\n",
        "        print(f\"  Average Test Loss: {avg_test_loss:.4f}\")\n",
        "        print(f\"  Average Test Accuracy: {avg_test_accuracy:.4f}\")\n",
        "        print(f\"  Average Communication Cost: {avg_communication_cost:.2f} bytes\")\n",
        "        print(f\"  Average Latency: {avg_latency:.2f} seconds\")\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"No metrics available for {n_components} PCA components.\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhjXLhZPDkU9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJo4TolpF2zB"
      },
      "source": [
        "##data_neglected_percentage = (1 - (n_components / original_dimensions)) * 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TaDqfBKF4Fz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bNTi9Y7F72e"
      },
      "source": [
        "###original_bandwidth_utilization = (original_data_size * num_rounds * num_clients) / total_bandwidth\n",
        "###reduced_bandwidth_utilization = (reduced_data_size * num_rounds * num_clients) / total_bandwidth\n",
        "###bandwidth_utilization_reduction = (1 - (reduced_bandwidth_utilization / original_bandwidth_utilization)) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtA1net0F75u"
      },
      "source": [
        "###original_communication_cost = original_data_size * num_rounds * num_clients\n",
        "###reduced_communication_cost = reduced_data_size * num_rounds * num_clients\n",
        "###communication_cost_reduction = (1 - (reduced_communication_cost / original_communication_cost)) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEOlJ3GFF781"
      },
      "source": [
        "###original_latency = original_data_size * transfer_time\n",
        "###reduced_latency = reduced_data_size * transfer_time\n",
        "###latency_reduction = (1 - (reduced_latency / original_latency)) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxIIe3qEF_ZK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}