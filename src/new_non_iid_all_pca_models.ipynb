{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjf-WB35QiDr",
        "outputId": "d642c6d7-8955-4c07-dc6f-e3d0e3506e52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q flwr[simulation] torch torchvision matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97UtJ4_eRQvs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import zipfile\n",
        "\n",
        "from collections import OrderedDict\n",
        "from typing import List, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "\n",
        "import flwr as fl\n",
        "from flwr.common import Metrics\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6BPtnTEque1",
        "outputId": "07b8fefb-d724-4863-84ce-62b31ddd02af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "CLASSES = (\n",
        "    \"0\",\n",
        "    \"1\",\n",
        "    \"2\",\n",
        "    \"3\",\n",
        "    \"4\",\n",
        "    \"5\",\n",
        "    \"6\",\n",
        "    \"7\",\n",
        "    \"8\",\n",
        "    \"9\",\n",
        ")\n",
        "\n",
        "NUM_CLIENTS = 10\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHafWP3zqzPp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def determine_pca_components(public_dataset, n_components):\n",
        "    # Reshape the public dataset to 2D (num_samples, num_features)\n",
        "    public_data = public_dataset.reshape(public_dataset.shape[0], -1)\n",
        "\n",
        "    # Fit PCA on the public dataset\n",
        "    pca = PCA(n_components=n_components)\n",
        "    pca.fit(public_data)\n",
        "\n",
        "    return pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiLQs1iIrSU6"
      },
      "outputs": [],
      "source": [
        "\n",
        "def apply_pca_locally(data, pca, n_components):\n",
        "    assert data is not None, \"Data cannot be None\"\n",
        "    assert n_components is not None and isinstance(n_components, int), \"n_components must be a valid integer\"\n",
        "\n",
        "    # Reshape the data to 2D (num_samples, num_features)\n",
        "    data_2d = data.reshape(data.shape[0], -1)\n",
        "\n",
        "    # Apply PCA transformation\n",
        "    data_transformed = pca.transform(data_2d)[:, :n_components]\n",
        "\n",
        "    # Calculate the percentage of data reduced or removed\n",
        "    original_size = data_2d.size\n",
        "    transformed_size = data_transformed.size\n",
        "    data_removed_percentage = (1 - transformed_size / original_size) * 100\n",
        "\n",
        "    # Reshape the transformed data to (num_samples, 1, n_components)\n",
        "    data_transformed = data_transformed.reshape(-1, 1, n_components)\n",
        "\n",
        "    return data_transformed, data_removed_percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB-4SHD5rU77",
        "outputId": "10777766-c311-4753-d102-51b3b20d08cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./dataset/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 102256645.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./dataset/MNIST/raw/train-images-idx3-ubyte.gz to ./dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 13413320.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./dataset/MNIST/raw/train-labels-idx1-ubyte.gz to ./dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 29814246.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to ./dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 12211877.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 79348866.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 18482711.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 32247480.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3850147.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import MNIST\n",
        "\n",
        "# Define the transform to preprocess the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "mnist_dataset = MNIST(\"./dataset\", train=True, download=True, transform=transform)\n",
        "\n",
        "# Load the MNIST dataset\n",
        "trainset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = MNIST(root='./data', train=False, download=True, transform=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiaLL0RNrzes"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import random\n",
        "\n",
        "def custom_distributed_split(indices, num_clients, class_idx, distribution_ratio):\n",
        "    data_len = len(indices)\n",
        "    class_data_range = int(data_len * distribution_ratio)\n",
        "    non_class_data_range = int((data_len * (1 - distribution_ratio)) // (num_clients-1))\n",
        "\n",
        "    out = []\n",
        "    non_class_data_start = class_data_range + 1\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        if i == class_idx:\n",
        "            out.append(indices[:class_data_range + 1])\n",
        "        else:\n",
        "            non_class_data_end = non_class_data_start + non_class_data_range + 1\n",
        "            out.append(indices[non_class_data_start : non_class_data_end])\n",
        "            non_class_data_start += non_class_data_range\n",
        "\n",
        "    if non_class_data_end != data_len:\n",
        "        clients = [i for i in range(num_clients)]\n",
        "        clients.pop(class_idx)\n",
        "        for i in range(non_class_data_end, data_len):\n",
        "            rand_class_idx = random.choice(clients)\n",
        "            out[rand_class_idx].append(i)\n",
        "\n",
        "    return out\n",
        "\n",
        "def distribute_data_non_iid(dataset, num_clients):\n",
        "    class_indices = [[] for _ in range(10)]  # 10 classes in MNIST\n",
        "    for idx, (_, label) in enumerate(dataset):\n",
        "        class_indices[label].append(idx)\n",
        "\n",
        "    client_data_indices = [[] for _ in range(num_clients)]\n",
        "    for class_idx, indices in enumerate(class_indices):\n",
        "        np.random.shuffle(indices)\n",
        "        split = custom_distributed_split(indices, num_clients, class_idx, 0.6)\n",
        "        for client_idx in range(num_clients):\n",
        "            client_data_indices[client_idx].extend(split[client_idx])\n",
        "\n",
        "    client_datasets = [Subset(dataset, indices) for indices in client_data_indices]\n",
        "    return client_datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKI0WuRWMqIt"
      },
      "outputs": [],
      "source": [
        "def determine_compression_parameters(client_data_variabilities):\n",
        "    avg_variability = np.mean(client_data_variabilities)\n",
        "    pruning_amount = np.interp(avg_variability, [0, 1], [0.2, 0.8])\n",
        "    quantization_type = 'dynamic' if avg_variability > 0.5 else 'static'\n",
        "    return pruning_amount, quantization_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjsZM0YSuDuw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "\n",
        "def determine_pca_components(public_dataset, client_data_variabilities, n_components_range=(8, 128)):\n",
        "    # Reshape the public dataset to 2D (num_samples, num_features)\n",
        "    public_data = public_dataset.reshape(public_dataset.shape[0], -1)\n",
        "\n",
        "    # Determine the number of PCA components based on client data variability\n",
        "    avg_variability = np.mean(client_data_variabilities)\n",
        "    n_components = int(np.interp(avg_variability, [0, 1], n_components_range))\n",
        "\n",
        "    # Fit PCA on the public dataset\n",
        "    pca = PCA(n_components=n_components)\n",
        "    pca.fit(public_data)\n",
        "\n",
        "    return pca, n_components, avg_variability\n",
        "\n",
        "def characterize_client_data(data):\n",
        "    # Reshape the data to 2D (num_samples, num_features)\n",
        "    data_2d = data.reshape(data.shape[0], -1)\n",
        "\n",
        "    # Calculate feature variances\n",
        "    feature_variances = np.var(data_2d, axis=0)\n",
        "\n",
        "    # Calculate class distribution\n",
        "    unique_classes, class_counts = np.unique(data_2d, return_counts=True)\n",
        "    class_distribution = class_counts / np.sum(class_counts)\n",
        "\n",
        "    # Calculate overall data variability\n",
        "    data_variability = np.mean(feature_variances) * np.sum(class_distribution ** 2)\n",
        "\n",
        "    return data_variability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD7uEx_3uiC0"
      },
      "outputs": [],
      "source": [
        "pca_components_range = [8, 16, 32, 64, 128]\n",
        "#pca_components_range = [8, 16, 32, 64]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhs-gLucr1R2"
      },
      "outputs": [],
      "source": [
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "def load_datasets(pca, n_components):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    trainset = MNIST(\"./dataset\", train=True, download=True, transform=transform)\n",
        "    testset = MNIST(\"./dataset\", train=False, download=True, transform=transform)\n",
        "\n",
        "    trainset_transformed, train_removed_percentage = apply_pca_locally(trainset.data.numpy(), pca, n_components)\n",
        "    testset_transformed, test_removed_percentage = apply_pca_locally(testset.data.numpy(), pca, n_components)\n",
        "\n",
        "    # Determine compression parameters based on client data variability\n",
        "    avg_variability = np.mean(client_data_variabilities)\n",
        "    pruning_amount = np.interp(avg_variability, [0, 1], [0.2, 0.8])\n",
        "    quantization_type = 'dynamic' if avg_variability > 0.5 else 'static'\n",
        "\n",
        "    # Split the dataset into non-IID client datasets\n",
        "    client_datasets = distribute_data_non_iid(trainset, NUM_CLIENTS)\n",
        "\n",
        "    trainloaders = []\n",
        "    valloaders = []\n",
        "    for ds in client_datasets:\n",
        "        len_val = len(ds) // 10\n",
        "        len_train = len(ds) - len_val\n",
        "        ds_train, ds_val = random_split(ds, [len_train, len_val], generator=torch.Generator().manual_seed(42))\n",
        "        trainloaders.append(DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True))\n",
        "        valloaders.append(DataLoader(ds_val, batch_size=BATCH_SIZE))\n",
        "\n",
        "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
        "    return trainloaders, valloaders, testloader, client_datasets, testset, train_removed_percentage, test_removed_percentage\n",
        "    #return trainloaders, valloaders, testloader, client_datasets, testset, train_removed_percentage, test_removed_percentage, pruning_amount, quantization_type\n",
        "\n",
        "def train_and_evaluate(pca, n_components, trainloader, valloader, testloader):\n",
        "    net = Net().to(DEVICE)\n",
        "    net, early_stopping_epoch, test_loss, test_accuracy = train(net, trainloader, valloader, testloader, epochs=100, device=DEVICE)\n",
        "    print(f\"Final test set performance for {n_components} PCA components: Loss {test_loss:.4f}, Accuracy {test_accuracy:.4f}\")\n",
        "    print(f\"Early stopping at epoch: {early_stopping_epoch}\")\n",
        "    print()\n",
        "    return test_accuracy, test_loss, early_stopping_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT1q-EkCrYSB"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 7 * 7)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhxdKTXNrbkV"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.optim import Adam\n",
        "from torch.nn.functional import relu, softmax\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def train(net, trainloader, valloader, testloader, epochs, device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "\n",
        "    best_valid_loss = float('inf')\n",
        "    best_model = None\n",
        "    best_test_loss = float('inf')\n",
        "    best_test_accuracy = 0.0\n",
        "    patience_counter = 0\n",
        "    early_stopping_epoch = 0\n",
        "\n",
        "    # Set requires_grad = True for all model parameters\n",
        "    for param in net.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in trainloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Check if the loss tensor requires gradients\n",
        "            if not loss.requires_grad:\n",
        "                print(\"Warning: Loss tensor does not require gradients.\")\n",
        "                print(\"Skipping backward pass for this batch.\")\n",
        "                continue\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(trainloader)\n",
        "        train_acc = correct / total\n",
        "\n",
        "        net.eval()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in valloader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = net(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        valid_loss = running_loss / len(valloader)\n",
        "        valid_acc = correct / total\n",
        "\n",
        "        # Calculate test metrics for every epoch\n",
        "        with torch.no_grad():\n",
        "            test_loss, test_accuracy = test(net, testloader)\n",
        "\n",
        "        # Print metrics for every epoch\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.4f}\")\n",
        "\n",
        "        scheduler.step(valid_loss)\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            best_model = net.state_dict()\n",
        "            best_test_loss = test_loss\n",
        "            best_test_accuracy = test_accuracy\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= 10:\n",
        "            early_stopping_epoch = epoch + 1\n",
        "            break\n",
        "\n",
        "    net.load_state_dict(best_model)\n",
        "    return net, early_stopping_epoch, best_test_loss, best_test_accuracy\n",
        "\n",
        "\n",
        "def test(net, testloader):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    correct, total, loss = 0, 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    loss /= len(testloader.dataset)\n",
        "    accuracy = correct / total\n",
        "    return loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTVJxxkvunoX",
        "outputId": "0f2f727d-2d54-4b0a-f742-bf04d0d5d2f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with 8 PCA components...\n",
            "Average variability: 2867.7677\n",
            "Determined number of PCA components: 8\n",
            "Epoch 1/100 - Train Loss: 0.6457, Train Acc: 0.7975, Valid Loss: 0.1498, Valid Acc: 0.9681, Test Loss: 0.0106, Test Acc: 0.9082\n",
            "Epoch 2/100 - Train Loss: 0.1626, Train Acc: 0.9486, Valid Loss: 0.0801, Valid Acc: 0.9815, Test Loss: 0.0060, Test Acc: 0.9408\n",
            "Epoch 3/100 - Train Loss: 0.0969, Train Acc: 0.9713, Valid Loss: 0.0801, Valid Acc: 0.9782, Test Loss: 0.0057, Test Acc: 0.9499\n",
            "Epoch 4/100 - Train Loss: 0.0781, Train Acc: 0.9765, Valid Loss: 0.0636, Valid Acc: 0.9799, Test Loss: 0.0051, Test Acc: 0.9554\n",
            "Epoch 5/100 - Train Loss: 0.0590, Train Acc: 0.9799, Valid Loss: 0.0540, Valid Acc: 0.9866, Test Loss: 0.0041, Test Acc: 0.9617\n",
            "Epoch 6/100 - Train Loss: 0.0463, Train Acc: 0.9844, Valid Loss: 0.0467, Valid Acc: 0.9849, Test Loss: 0.0043, Test Acc: 0.9622\n",
            "Epoch 7/100 - Train Loss: 0.0330, Train Acc: 0.9886, Valid Loss: 0.0675, Valid Acc: 0.9799, Test Loss: 0.0050, Test Acc: 0.9567\n",
            "Epoch 8/100 - Train Loss: 0.0351, Train Acc: 0.9881, Valid Loss: 0.0358, Valid Acc: 0.9916, Test Loss: 0.0037, Test Acc: 0.9684\n",
            "Epoch 9/100 - Train Loss: 0.0253, Train Acc: 0.9924, Valid Loss: 0.0563, Valid Acc: 0.9866, Test Loss: 0.0050, Test Acc: 0.9568\n",
            "Epoch 10/100 - Train Loss: 0.0279, Train Acc: 0.9911, Valid Loss: 0.0463, Valid Acc: 0.9849, Test Loss: 0.0035, Test Acc: 0.9709\n",
            "Epoch 11/100 - Train Loss: 0.0197, Train Acc: 0.9946, Valid Loss: 0.0376, Valid Acc: 0.9916, Test Loss: 0.0036, Test Acc: 0.9707\n",
            "Epoch 12/100 - Train Loss: 0.0177, Train Acc: 0.9940, Valid Loss: 0.0371, Valid Acc: 0.9883, Test Loss: 0.0045, Test Acc: 0.9664\n",
            "Epoch 13/100 - Train Loss: 0.0163, Train Acc: 0.9948, Valid Loss: 0.0490, Valid Acc: 0.9832, Test Loss: 0.0046, Test Acc: 0.9645\n",
            "Epoch 14/100 - Train Loss: 0.0211, Train Acc: 0.9931, Valid Loss: 0.0450, Valid Acc: 0.9866, Test Loss: 0.0053, Test Acc: 0.9601\n",
            "Epoch 15/100 - Train Loss: 0.0093, Train Acc: 0.9980, Valid Loss: 0.0331, Valid Acc: 0.9883, Test Loss: 0.0042, Test Acc: 0.9675\n",
            "Epoch 16/100 - Train Loss: 0.0067, Train Acc: 0.9983, Valid Loss: 0.0323, Valid Acc: 0.9899, Test Loss: 0.0041, Test Acc: 0.9680\n",
            "Epoch 17/100 - Train Loss: 0.0067, Train Acc: 0.9985, Valid Loss: 0.0337, Valid Acc: 0.9883, Test Loss: 0.0042, Test Acc: 0.9690\n",
            "Epoch 18/100 - Train Loss: 0.0040, Train Acc: 0.9994, Valid Loss: 0.0327, Valid Acc: 0.9899, Test Loss: 0.0042, Test Acc: 0.9697\n",
            "Epoch 19/100 - Train Loss: 0.0059, Train Acc: 0.9989, Valid Loss: 0.0322, Valid Acc: 0.9899, Test Loss: 0.0041, Test Acc: 0.9699\n",
            "Epoch 20/100 - Train Loss: 0.0033, Train Acc: 0.9998, Valid Loss: 0.0316, Valid Acc: 0.9883, Test Loss: 0.0040, Test Acc: 0.9705\n",
            "Epoch 21/100 - Train Loss: 0.0036, Train Acc: 0.9998, Valid Loss: 0.0329, Valid Acc: 0.9899, Test Loss: 0.0041, Test Acc: 0.9708\n",
            "Epoch 22/100 - Train Loss: 0.0047, Train Acc: 0.9987, Valid Loss: 0.0342, Valid Acc: 0.9899, Test Loss: 0.0041, Test Acc: 0.9709\n",
            "Epoch 23/100 - Train Loss: 0.0038, Train Acc: 0.9994, Valid Loss: 0.0331, Valid Acc: 0.9899, Test Loss: 0.0041, Test Acc: 0.9700\n",
            "Epoch 24/100 - Train Loss: 0.0032, Train Acc: 0.9998, Valid Loss: 0.0344, Valid Acc: 0.9899, Test Loss: 0.0043, Test Acc: 0.9695\n",
            "Epoch 25/100 - Train Loss: 0.0036, Train Acc: 0.9991, Valid Loss: 0.0351, Valid Acc: 0.9899, Test Loss: 0.0042, Test Acc: 0.9706\n",
            "Epoch 26/100 - Train Loss: 0.0037, Train Acc: 0.9993, Valid Loss: 0.0352, Valid Acc: 0.9883, Test Loss: 0.0044, Test Acc: 0.9697\n",
            "Epoch 27/100 - Train Loss: 0.0020, Train Acc: 1.0000, Valid Loss: 0.0349, Valid Acc: 0.9883, Test Loss: 0.0044, Test Acc: 0.9697\n",
            "Epoch 28/100 - Train Loss: 0.0028, Train Acc: 0.9998, Valid Loss: 0.0345, Valid Acc: 0.9899, Test Loss: 0.0044, Test Acc: 0.9699\n",
            "Epoch 29/100 - Train Loss: 0.0024, Train Acc: 0.9996, Valid Loss: 0.0345, Valid Acc: 0.9883, Test Loss: 0.0044, Test Acc: 0.9701\n",
            "Epoch 30/100 - Train Loss: 0.0021, Train Acc: 1.0000, Valid Loss: 0.0339, Valid Acc: 0.9899, Test Loss: 0.0043, Test Acc: 0.9701\n",
            "Final test set performance for 8 PCA components: Loss 0.0040, Accuracy 0.9705\n",
            "Early stopping at epoch: 30\n",
            "\n",
            "Training with 16 PCA components...\n",
            "Average variability: 2867.7677\n",
            "Determined number of PCA components: 16\n",
            "Epoch 1/100 - Train Loss: 0.7026, Train Acc: 0.7914, Valid Loss: 0.2411, Valid Acc: 0.9295, Test Loss: 0.0162, Test Acc: 0.8346\n",
            "Epoch 2/100 - Train Loss: 0.2142, Train Acc: 0.9380, Valid Loss: 0.1313, Valid Acc: 0.9614, Test Loss: 0.0081, Test Acc: 0.9297\n",
            "Epoch 3/100 - Train Loss: 0.1360, Train Acc: 0.9570, Valid Loss: 0.1166, Valid Acc: 0.9631, Test Loss: 0.0075, Test Acc: 0.9290\n",
            "Epoch 4/100 - Train Loss: 0.0948, Train Acc: 0.9713, Valid Loss: 0.0977, Valid Acc: 0.9648, Test Loss: 0.0054, Test Acc: 0.9497\n",
            "Epoch 5/100 - Train Loss: 0.0720, Train Acc: 0.9778, Valid Loss: 0.0893, Valid Acc: 0.9664, Test Loss: 0.0048, Test Acc: 0.9558\n",
            "Epoch 6/100 - Train Loss: 0.0583, Train Acc: 0.9821, Valid Loss: 0.0762, Valid Acc: 0.9698, Test Loss: 0.0049, Test Acc: 0.9561\n",
            "Epoch 7/100 - Train Loss: 0.0470, Train Acc: 0.9864, Valid Loss: 0.0801, Valid Acc: 0.9765, Test Loss: 0.0038, Test Acc: 0.9648\n",
            "Epoch 8/100 - Train Loss: 0.0409, Train Acc: 0.9873, Valid Loss: 0.0697, Valid Acc: 0.9715, Test Loss: 0.0040, Test Acc: 0.9616\n",
            "Epoch 9/100 - Train Loss: 0.0321, Train Acc: 0.9885, Valid Loss: 0.0772, Valid Acc: 0.9715, Test Loss: 0.0039, Test Acc: 0.9634\n",
            "Epoch 10/100 - Train Loss: 0.0240, Train Acc: 0.9918, Valid Loss: 0.0663, Valid Acc: 0.9815, Test Loss: 0.0037, Test Acc: 0.9670\n",
            "Epoch 11/100 - Train Loss: 0.0222, Train Acc: 0.9927, Valid Loss: 0.0729, Valid Acc: 0.9748, Test Loss: 0.0036, Test Acc: 0.9671\n",
            "Epoch 12/100 - Train Loss: 0.0188, Train Acc: 0.9935, Valid Loss: 0.0735, Valid Acc: 0.9832, Test Loss: 0.0043, Test Acc: 0.9644\n",
            "Epoch 13/100 - Train Loss: 0.0209, Train Acc: 0.9922, Valid Loss: 0.0919, Valid Acc: 0.9748, Test Loss: 0.0039, Test Acc: 0.9687\n",
            "Epoch 14/100 - Train Loss: 0.0168, Train Acc: 0.9959, Valid Loss: 0.0861, Valid Acc: 0.9765, Test Loss: 0.0045, Test Acc: 0.9641\n",
            "Epoch 15/100 - Train Loss: 0.0157, Train Acc: 0.9942, Valid Loss: 0.0705, Valid Acc: 0.9799, Test Loss: 0.0040, Test Acc: 0.9671\n",
            "Epoch 16/100 - Train Loss: 0.0147, Train Acc: 0.9955, Valid Loss: 0.0731, Valid Acc: 0.9765, Test Loss: 0.0041, Test Acc: 0.9662\n",
            "Epoch 17/100 - Train Loss: 0.0090, Train Acc: 0.9972, Valid Loss: 0.0642, Valid Acc: 0.9799, Test Loss: 0.0036, Test Acc: 0.9709\n",
            "Epoch 18/100 - Train Loss: 0.0058, Train Acc: 0.9987, Valid Loss: 0.0671, Valid Acc: 0.9799, Test Loss: 0.0036, Test Acc: 0.9723\n",
            "Epoch 19/100 - Train Loss: 0.0051, Train Acc: 0.9994, Valid Loss: 0.0702, Valid Acc: 0.9765, Test Loss: 0.0037, Test Acc: 0.9716\n",
            "Epoch 20/100 - Train Loss: 0.0045, Train Acc: 0.9991, Valid Loss: 0.0663, Valid Acc: 0.9782, Test Loss: 0.0036, Test Acc: 0.9722\n",
            "Epoch 21/100 - Train Loss: 0.0043, Train Acc: 0.9987, Valid Loss: 0.0672, Valid Acc: 0.9799, Test Loss: 0.0038, Test Acc: 0.9711\n",
            "Epoch 22/100 - Train Loss: 0.0042, Train Acc: 0.9993, Valid Loss: 0.0627, Valid Acc: 0.9782, Test Loss: 0.0036, Test Acc: 0.9724\n",
            "Epoch 23/100 - Train Loss: 0.0033, Train Acc: 0.9993, Valid Loss: 0.0695, Valid Acc: 0.9782, Test Loss: 0.0037, Test Acc: 0.9724\n",
            "Epoch 24/100 - Train Loss: 0.0038, Train Acc: 0.9991, Valid Loss: 0.0702, Valid Acc: 0.9799, Test Loss: 0.0039, Test Acc: 0.9701\n",
            "Epoch 25/100 - Train Loss: 0.0031, Train Acc: 0.9993, Valid Loss: 0.0685, Valid Acc: 0.9799, Test Loss: 0.0038, Test Acc: 0.9721\n",
            "Epoch 26/100 - Train Loss: 0.0030, Train Acc: 0.9991, Valid Loss: 0.0696, Valid Acc: 0.9799, Test Loss: 0.0036, Test Acc: 0.9726\n",
            "Epoch 27/100 - Train Loss: 0.0025, Train Acc: 0.9996, Valid Loss: 0.0701, Valid Acc: 0.9799, Test Loss: 0.0038, Test Acc: 0.9727\n",
            "Epoch 28/100 - Train Loss: 0.0037, Train Acc: 0.9987, Valid Loss: 0.0830, Valid Acc: 0.9782, Test Loss: 0.0039, Test Acc: 0.9717\n",
            "Epoch 29/100 - Train Loss: 0.0032, Train Acc: 0.9991, Valid Loss: 0.0831, Valid Acc: 0.9782, Test Loss: 0.0039, Test Acc: 0.9716\n",
            "Epoch 30/100 - Train Loss: 0.0031, Train Acc: 0.9993, Valid Loss: 0.0823, Valid Acc: 0.9782, Test Loss: 0.0039, Test Acc: 0.9714\n",
            "Epoch 31/100 - Train Loss: 0.0025, Train Acc: 0.9996, Valid Loss: 0.0799, Valid Acc: 0.9782, Test Loss: 0.0039, Test Acc: 0.9718\n",
            "Epoch 32/100 - Train Loss: 0.0022, Train Acc: 0.9996, Valid Loss: 0.0779, Valid Acc: 0.9799, Test Loss: 0.0038, Test Acc: 0.9717\n",
            "Final test set performance for 16 PCA components: Loss 0.0036, Accuracy 0.9724\n",
            "Early stopping at epoch: 32\n",
            "\n",
            "Training with 32 PCA components...\n",
            "Average variability: 2867.7677\n",
            "Determined number of PCA components: 32\n",
            "Epoch 1/100 - Train Loss: 0.8827, Train Acc: 0.7364, Valid Loss: 0.2870, Valid Acc: 0.9279, Test Loss: 0.0176, Test Acc: 0.8353\n",
            "Epoch 2/100 - Train Loss: 0.2291, Train Acc: 0.9324, Valid Loss: 0.1553, Valid Acc: 0.9547, Test Loss: 0.0085, Test Acc: 0.9140\n",
            "Epoch 3/100 - Train Loss: 0.1418, Train Acc: 0.9551, Valid Loss: 0.1054, Valid Acc: 0.9732, Test Loss: 0.0059, Test Acc: 0.9455\n",
            "Epoch 4/100 - Train Loss: 0.0944, Train Acc: 0.9711, Valid Loss: 0.0932, Valid Acc: 0.9782, Test Loss: 0.0045, Test Acc: 0.9587\n",
            "Epoch 5/100 - Train Loss: 0.0722, Train Acc: 0.9763, Valid Loss: 0.0825, Valid Acc: 0.9832, Test Loss: 0.0046, Test Acc: 0.9562\n",
            "Epoch 6/100 - Train Loss: 0.0594, Train Acc: 0.9823, Valid Loss: 0.0926, Valid Acc: 0.9799, Test Loss: 0.0046, Test Acc: 0.9586\n",
            "Epoch 7/100 - Train Loss: 0.0558, Train Acc: 0.9814, Valid Loss: 0.0825, Valid Acc: 0.9866, Test Loss: 0.0040, Test Acc: 0.9640\n",
            "Epoch 8/100 - Train Loss: 0.0439, Train Acc: 0.9857, Valid Loss: 0.0846, Valid Acc: 0.9849, Test Loss: 0.0039, Test Acc: 0.9651\n",
            "Epoch 9/100 - Train Loss: 0.0398, Train Acc: 0.9881, Valid Loss: 0.0730, Valid Acc: 0.9832, Test Loss: 0.0032, Test Acc: 0.9708\n",
            "Epoch 10/100 - Train Loss: 0.0298, Train Acc: 0.9896, Valid Loss: 0.0781, Valid Acc: 0.9849, Test Loss: 0.0042, Test Acc: 0.9624\n",
            "Epoch 11/100 - Train Loss: 0.0233, Train Acc: 0.9927, Valid Loss: 0.0918, Valid Acc: 0.9883, Test Loss: 0.0041, Test Acc: 0.9662\n",
            "Epoch 12/100 - Train Loss: 0.0265, Train Acc: 0.9909, Valid Loss: 0.0857, Valid Acc: 0.9866, Test Loss: 0.0043, Test Acc: 0.9641\n",
            "Epoch 13/100 - Train Loss: 0.0234, Train Acc: 0.9914, Valid Loss: 0.0667, Valid Acc: 0.9849, Test Loss: 0.0038, Test Acc: 0.9664\n",
            "Epoch 14/100 - Train Loss: 0.0217, Train Acc: 0.9931, Valid Loss: 0.0659, Valid Acc: 0.9866, Test Loss: 0.0031, Test Acc: 0.9737\n",
            "Epoch 15/100 - Train Loss: 0.0172, Train Acc: 0.9940, Valid Loss: 0.0831, Valid Acc: 0.9866, Test Loss: 0.0034, Test Acc: 0.9719\n",
            "Epoch 16/100 - Train Loss: 0.0165, Train Acc: 0.9948, Valid Loss: 0.0755, Valid Acc: 0.9883, Test Loss: 0.0037, Test Acc: 0.9711\n",
            "Epoch 17/100 - Train Loss: 0.0103, Train Acc: 0.9968, Valid Loss: 0.0768, Valid Acc: 0.9883, Test Loss: 0.0039, Test Acc: 0.9708\n",
            "Epoch 18/100 - Train Loss: 0.0125, Train Acc: 0.9965, Valid Loss: 0.0759, Valid Acc: 0.9883, Test Loss: 0.0036, Test Acc: 0.9721\n",
            "Epoch 19/100 - Train Loss: 0.0111, Train Acc: 0.9963, Valid Loss: 0.0867, Valid Acc: 0.9849, Test Loss: 0.0036, Test Acc: 0.9707\n",
            "Epoch 20/100 - Train Loss: 0.0181, Train Acc: 0.9942, Valid Loss: 0.0828, Valid Acc: 0.9899, Test Loss: 0.0036, Test Acc: 0.9717\n",
            "Epoch 21/100 - Train Loss: 0.0106, Train Acc: 0.9972, Valid Loss: 0.0864, Valid Acc: 0.9866, Test Loss: 0.0035, Test Acc: 0.9731\n",
            "Epoch 22/100 - Train Loss: 0.0063, Train Acc: 0.9980, Valid Loss: 0.0870, Valid Acc: 0.9866, Test Loss: 0.0035, Test Acc: 0.9732\n",
            "Epoch 23/100 - Train Loss: 0.0077, Train Acc: 0.9970, Valid Loss: 0.0880, Valid Acc: 0.9866, Test Loss: 0.0035, Test Acc: 0.9733\n",
            "Epoch 24/100 - Train Loss: 0.0052, Train Acc: 0.9991, Valid Loss: 0.0917, Valid Acc: 0.9866, Test Loss: 0.0036, Test Acc: 0.9730\n",
            "Final test set performance for 32 PCA components: Loss 0.0031, Accuracy 0.9737\n",
            "Early stopping at epoch: 24\n",
            "\n",
            "Training with 64 PCA components...\n",
            "Average variability: 2867.7677\n",
            "Determined number of PCA components: 64\n",
            "Epoch 1/100 - Train Loss: 0.6093, Train Acc: 0.8133, Valid Loss: 0.2130, Valid Acc: 0.9379, Test Loss: 0.0129, Test Acc: 0.8843\n",
            "Epoch 2/100 - Train Loss: 0.1605, Train Acc: 0.9542, Valid Loss: 0.1063, Valid Acc: 0.9681, Test Loss: 0.0065, Test Acc: 0.9403\n",
            "Epoch 3/100 - Train Loss: 0.0935, Train Acc: 0.9695, Valid Loss: 0.0998, Valid Acc: 0.9715, Test Loss: 0.0058, Test Acc: 0.9523\n",
            "Epoch 4/100 - Train Loss: 0.0681, Train Acc: 0.9797, Valid Loss: 0.0819, Valid Acc: 0.9715, Test Loss: 0.0047, Test Acc: 0.9579\n",
            "Epoch 5/100 - Train Loss: 0.0538, Train Acc: 0.9827, Valid Loss: 0.0766, Valid Acc: 0.9782, Test Loss: 0.0049, Test Acc: 0.9564\n",
            "Epoch 6/100 - Train Loss: 0.0449, Train Acc: 0.9851, Valid Loss: 0.0816, Valid Acc: 0.9732, Test Loss: 0.0048, Test Acc: 0.9574\n",
            "Epoch 7/100 - Train Loss: 0.0316, Train Acc: 0.9894, Valid Loss: 0.0605, Valid Acc: 0.9832, Test Loss: 0.0043, Test Acc: 0.9641\n",
            "Epoch 8/100 - Train Loss: 0.0324, Train Acc: 0.9896, Valid Loss: 0.0698, Valid Acc: 0.9748, Test Loss: 0.0041, Test Acc: 0.9654\n",
            "Epoch 9/100 - Train Loss: 0.0215, Train Acc: 0.9931, Valid Loss: 0.0695, Valid Acc: 0.9815, Test Loss: 0.0038, Test Acc: 0.9659\n",
            "Epoch 10/100 - Train Loss: 0.0179, Train Acc: 0.9935, Valid Loss: 0.0627, Valid Acc: 0.9832, Test Loss: 0.0039, Test Acc: 0.9669\n",
            "Epoch 11/100 - Train Loss: 0.0171, Train Acc: 0.9942, Valid Loss: 0.0700, Valid Acc: 0.9815, Test Loss: 0.0046, Test Acc: 0.9592\n",
            "Epoch 12/100 - Train Loss: 0.0118, Train Acc: 0.9970, Valid Loss: 0.0589, Valid Acc: 0.9883, Test Loss: 0.0043, Test Acc: 0.9648\n",
            "Epoch 13/100 - Train Loss: 0.0177, Train Acc: 0.9952, Valid Loss: 0.0341, Valid Acc: 0.9933, Test Loss: 0.0038, Test Acc: 0.9691\n",
            "Epoch 14/100 - Train Loss: 0.0082, Train Acc: 0.9972, Valid Loss: 0.0479, Valid Acc: 0.9866, Test Loss: 0.0038, Test Acc: 0.9702\n",
            "Epoch 15/100 - Train Loss: 0.0114, Train Acc: 0.9957, Valid Loss: 0.0567, Valid Acc: 0.9866, Test Loss: 0.0036, Test Acc: 0.9709\n",
            "Epoch 16/100 - Train Loss: 0.0173, Train Acc: 0.9948, Valid Loss: 0.0685, Valid Acc: 0.9815, Test Loss: 0.0035, Test Acc: 0.9678\n",
            "Epoch 17/100 - Train Loss: 0.0101, Train Acc: 0.9976, Valid Loss: 0.0637, Valid Acc: 0.9832, Test Loss: 0.0039, Test Acc: 0.9704\n",
            "Epoch 18/100 - Train Loss: 0.0077, Train Acc: 0.9978, Valid Loss: 0.0731, Valid Acc: 0.9866, Test Loss: 0.0052, Test Acc: 0.9630\n",
            "Epoch 19/100 - Train Loss: 0.0133, Train Acc: 0.9952, Valid Loss: 0.0856, Valid Acc: 0.9698, Test Loss: 0.0052, Test Acc: 0.9600\n",
            "Epoch 20/100 - Train Loss: 0.0067, Train Acc: 0.9985, Valid Loss: 0.0494, Valid Acc: 0.9866, Test Loss: 0.0037, Test Acc: 0.9724\n",
            "Epoch 21/100 - Train Loss: 0.0032, Train Acc: 0.9993, Valid Loss: 0.0495, Valid Acc: 0.9883, Test Loss: 0.0037, Test Acc: 0.9718\n",
            "Epoch 22/100 - Train Loss: 0.0040, Train Acc: 0.9993, Valid Loss: 0.0476, Valid Acc: 0.9883, Test Loss: 0.0038, Test Acc: 0.9720\n",
            "Epoch 23/100 - Train Loss: 0.0045, Train Acc: 0.9981, Valid Loss: 0.0504, Valid Acc: 0.9899, Test Loss: 0.0040, Test Acc: 0.9705\n",
            "Final test set performance for 64 PCA components: Loss 0.0038, Accuracy 0.9691\n",
            "Early stopping at epoch: 23\n",
            "\n",
            "Training with 128 PCA components...\n",
            "Average variability: 2867.7677\n",
            "Determined number of PCA components: 128\n",
            "Epoch 1/100 - Train Loss: 0.6715, Train Acc: 0.7984, Valid Loss: 0.2303, Valid Acc: 0.9430, Test Loss: 0.0158, Test Acc: 0.8539\n",
            "Epoch 2/100 - Train Loss: 0.2121, Train Acc: 0.9380, Valid Loss: 0.1285, Valid Acc: 0.9681, Test Loss: 0.0079, Test Acc: 0.9279\n",
            "Epoch 3/100 - Train Loss: 0.1227, Train Acc: 0.9639, Valid Loss: 0.0809, Valid Acc: 0.9765, Test Loss: 0.0060, Test Acc: 0.9454\n",
            "Epoch 4/100 - Train Loss: 0.0838, Train Acc: 0.9773, Valid Loss: 0.0729, Valid Acc: 0.9765, Test Loss: 0.0050, Test Acc: 0.9524\n",
            "Epoch 5/100 - Train Loss: 0.0664, Train Acc: 0.9792, Valid Loss: 0.0574, Valid Acc: 0.9832, Test Loss: 0.0046, Test Acc: 0.9572\n",
            "Epoch 6/100 - Train Loss: 0.0564, Train Acc: 0.9803, Valid Loss: 0.0541, Valid Acc: 0.9866, Test Loss: 0.0043, Test Acc: 0.9579\n",
            "Epoch 7/100 - Train Loss: 0.0402, Train Acc: 0.9879, Valid Loss: 0.0876, Valid Acc: 0.9748, Test Loss: 0.0052, Test Acc: 0.9506\n",
            "Epoch 8/100 - Train Loss: 0.0355, Train Acc: 0.9901, Valid Loss: 0.0499, Valid Acc: 0.9883, Test Loss: 0.0035, Test Acc: 0.9666\n",
            "Epoch 9/100 - Train Loss: 0.0292, Train Acc: 0.9899, Valid Loss: 0.0409, Valid Acc: 0.9866, Test Loss: 0.0045, Test Acc: 0.9598\n",
            "Epoch 10/100 - Train Loss: 0.0283, Train Acc: 0.9905, Valid Loss: 0.0591, Valid Acc: 0.9866, Test Loss: 0.0042, Test Acc: 0.9648\n",
            "Epoch 11/100 - Train Loss: 0.0222, Train Acc: 0.9926, Valid Loss: 0.0410, Valid Acc: 0.9883, Test Loss: 0.0044, Test Acc: 0.9629\n",
            "Epoch 12/100 - Train Loss: 0.0185, Train Acc: 0.9937, Valid Loss: 0.0466, Valid Acc: 0.9849, Test Loss: 0.0034, Test Acc: 0.9703\n",
            "Epoch 13/100 - Train Loss: 0.0132, Train Acc: 0.9957, Valid Loss: 0.0538, Valid Acc: 0.9866, Test Loss: 0.0035, Test Acc: 0.9702\n",
            "Epoch 14/100 - Train Loss: 0.0164, Train Acc: 0.9946, Valid Loss: 0.0469, Valid Acc: 0.9883, Test Loss: 0.0043, Test Acc: 0.9685\n",
            "Epoch 15/100 - Train Loss: 0.0146, Train Acc: 0.9952, Valid Loss: 0.0604, Valid Acc: 0.9883, Test Loss: 0.0052, Test Acc: 0.9615\n",
            "Epoch 16/100 - Train Loss: 0.0115, Train Acc: 0.9965, Valid Loss: 0.0422, Valid Acc: 0.9899, Test Loss: 0.0036, Test Acc: 0.9707\n",
            "Epoch 17/100 - Train Loss: 0.0081, Train Acc: 0.9987, Valid Loss: 0.0448, Valid Acc: 0.9916, Test Loss: 0.0035, Test Acc: 0.9708\n",
            "Epoch 18/100 - Train Loss: 0.0060, Train Acc: 0.9983, Valid Loss: 0.0440, Valid Acc: 0.9899, Test Loss: 0.0037, Test Acc: 0.9710\n",
            "Epoch 19/100 - Train Loss: 0.0049, Train Acc: 0.9993, Valid Loss: 0.0447, Valid Acc: 0.9916, Test Loss: 0.0036, Test Acc: 0.9714\n",
            "Final test set performance for 128 PCA components: Loss 0.0045, Accuracy 0.9598\n",
            "Early stopping at epoch: 19\n",
            "\n",
            "Best PCA Components: 32\n",
            "Best Test Accuracy: 0.9737\n",
            "Best Test Loss: 0.0031\n",
            "Best Early Stopping Epoch: 24\n",
            "Quantization Type: dynamic\n",
            "Pruning Amount: 0.8\n",
            "Results:\n",
            "PCA Components: 8, Test Loss: 0.0040, Test Accuracy: 0.9705, Compression Ratio: 98.98%\n",
            "PCA Components: 16, Test Loss: 0.0036, Test Accuracy: 0.9724, Compression Ratio: 97.96%\n",
            "PCA Components: 32, Test Loss: 0.0031, Test Accuracy: 0.9737, Compression Ratio: 95.92%\n",
            "PCA Components: 64, Test Loss: 0.0038, Test Accuracy: 0.9691, Compression Ratio: 91.84%\n",
            "PCA Components: 128, Test Loss: 0.0045, Test Accuracy: 0.9598, Compression Ratio: 83.67%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "pca_components_range = [8, 16, 32, 64, 128]  # Define the range of PCA components to try\n",
        "\n",
        "best_accuracy = 0\n",
        "best_pca_components = None\n",
        "best_test_loss = float('inf')\n",
        "best_compression = 0\n",
        "\n",
        "# ... (previous code remains the same)\n",
        "\n",
        "# Split the dataset into NUM_CLIENTS partitions\n",
        "partition_size = len(trainset) // NUM_CLIENTS\n",
        "lengths = [partition_size] * NUM_CLIENTS\n",
        "datasets = random_split(trainset, lengths, torch.Generator().manual_seed(42))\n",
        "\n",
        "\n",
        "client_data_variabilities = []\n",
        "for i in range(NUM_CLIENTS):\n",
        "    data_variability = characterize_client_data(datasets[i].dataset.data.numpy())\n",
        "    client_data_variabilities.append(data_variability)\n",
        "\n",
        "# Determine PCA components dynamically based on client data variability\n",
        "pca, n_components, avg_variability = determine_pca_components(mnist_dataset.data.numpy(), client_data_variabilities)\n",
        "\n",
        "# Load the datasets using the PCA object\n",
        "trainloaders, valloaders, testloader, datasets, testset, _, _ = load_datasets(pca, n_components)\n",
        "\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "\n",
        "for n_components in pca_components_range:\n",
        "    print(f\"Training with {n_components} PCA components...\")\n",
        "    pca, n_components, avg_variability = determine_pca_components(mnist_dataset.data.numpy(), client_data_variabilities, n_components_range=(n_components, n_components))\n",
        "    print(f\"Average variability: {avg_variability:.4f}\")\n",
        "    print(f\"Determined number of PCA components: {n_components}\")\n",
        "\n",
        "    trainloaders, valloaders, testloader, datasets, testset, _, _ = load_datasets(pca, n_components)\n",
        "\n",
        "    train_removed_percentages = []\n",
        "    test_removed_percentages = []\n",
        "    for i in range(NUM_CLIENTS):\n",
        "        _, train_removed_percentage = apply_pca_locally(datasets[i].dataset.data.numpy(), pca, n_components)\n",
        "        _, test_removed_percentage = apply_pca_locally(testset.data.numpy(), pca, n_components)\n",
        "        train_removed_percentages.append(train_removed_percentage)\n",
        "        test_removed_percentages.append(test_removed_percentage)\n",
        "\n",
        "    # Determine compression parameters based on client data variability\n",
        "    pruning_amount, quantization_type = determine_compression_parameters(client_data_variabilities)\n",
        "\n",
        "    trainloader = trainloaders[0]\n",
        "    valloader = valloaders[0]\n",
        "\n",
        "    test_accuracy, test_loss, early_stopping_epoch = train_and_evaluate(pca, n_components, trainloader, valloader, testloader)\n",
        "\n",
        "    compression_ratio = (1 - (n_components * 784) / (28 * 28 * 784)) * 100\n",
        "\n",
        "    results.append({\n",
        "        'n_components': n_components,\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'compression_ratio': compression_ratio\n",
        "    })\n",
        "\n",
        "    if test_accuracy > best_accuracy:\n",
        "        best_accuracy = test_accuracy\n",
        "        best_pca_components = n_components\n",
        "        best_test_loss = test_loss\n",
        "        best_compression = compression_ratio\n",
        "        best_early_stopping_epoch = early_stopping_epoch\n",
        "\n",
        "\n",
        "print(f\"Best PCA Components: {best_pca_components}\")\n",
        "print(f\"Best Test Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"Best Test Loss: {best_test_loss:.4f}\")\n",
        "print(f\"Best Early Stopping Epoch: {best_early_stopping_epoch}\")\n",
        "print(f\"Quantization Type: {quantization_type}\")\n",
        "print(f\"Pruning Amount: {pruning_amount}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"Results:\")\n",
        "for result in results:\n",
        "    print(f\"PCA Components: {result['n_components']}, Test Loss: {result['test_loss']:.4f}, Test Accuracy: {result['test_accuracy']:.4f}, Compression Ratio: {result['compression_ratio']:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaQrgaPzv8Vk",
        "outputId": "751da9b9-e904-4fe9-e77b-9c6b9e920a73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "\n",
        "# Model size calculation function\n",
        "def get_model_size(model):\n",
        "    buffer = io.BytesIO()\n",
        "    torch.save(model.state_dict(), buffer)\n",
        "    size = buffer.tell()  # Size in bytes\n",
        "    return size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MYLWWM0-m30"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "def get_parameters(net) -> List[np.ndarray]:\n",
        "    parameters = []\n",
        "    for name, param in net.named_parameters():\n",
        "        print(f\"Get parameters: {name}: {param.shape}\")\n",
        "        parameters.append(param.detach().cpu().numpy())\n",
        "    return parameters\n",
        "\n",
        "def set_parameters(net, parameters: List[np.ndarray]):\n",
        "    params_dict = zip(net.state_dict().keys(), parameters)\n",
        "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "\n",
        "    # Print the expected parameter shapes and the received parameter shapes\n",
        "    print(\"Expected parameter shapes:\")\n",
        "    for name, param in net.named_parameters():\n",
        "        print(f\"{name}: {param.shape}\")\n",
        "\n",
        "    print(\"Received parameter shapes:\")\n",
        "    for name, param in state_dict.items():\n",
        "        print(f\"{name}: {param.shape}\")\n",
        "\n",
        "    # Try to load the state dictionary and handle shape mismatch\n",
        "    try:\n",
        "        net.load_state_dict(state_dict, strict=True)\n",
        "    except RuntimeError as e:\n",
        "        print(\"Shape mismatch occurred. Attempting to handle gracefully...\")\n",
        "\n",
        "        # Create a new state dictionary with matching shapes\n",
        "        new_state_dict = OrderedDict()\n",
        "        for (name, param), (state_name, state_param) in zip(net.named_parameters(), state_dict.items()):\n",
        "            if param.shape != state_param.shape:\n",
        "                print(f\"Shape mismatch for parameter '{name}'. Expected {param.shape}, got {state_param.shape}.\")\n",
        "                if param.numel() == state_param.numel():\n",
        "                    # Reshape the parameter if the total number of elements matches\n",
        "                    state_param = state_param.view(param.shape)\n",
        "                else:\n",
        "                    print(f\"Cannot reshape parameter '{name}' due to incompatible number of elements.\")\n",
        "                    continue\n",
        "            new_state_dict[name] = state_param\n",
        "\n",
        "        # Load the new state dictionary\n",
        "        net.load_state_dict(new_state_dict, strict=False)\n",
        "        print(\"Shape mismatch handled.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KCIzXjz-o9a"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.quantization\n",
        "import torch.nn.utils.prune as prune\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "\n",
        "def apply_pruning(model, pruning_technique, amount):\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "            if pruning_technique == 'l1_unstructured':\n",
        "                prune.l1_unstructured(module, name='weight', amount=amount)\n",
        "            elif pruning_technique == 'l1_structured':\n",
        "                prune.ln_structured(module, name='weight', amount=amount, n=1, dim=0)\n",
        "            elif pruning_technique == 'global_unstructured':\n",
        "                prune.global_unstructured(module, name='weight', amount=amount)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported pruning technique: {pruning_technique}\")\n",
        "            prune.remove(module, 'weight')\n",
        "\n",
        "def apply_quantization(model, quantization_type):\n",
        "    if quantization_type == 'dynamic':\n",
        "        quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "    elif quantization_type == 'static':\n",
        "        model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "        torch.quantization.prepare(model, inplace=True)\n",
        "        torch.quantization.convert(model, inplace=True)\n",
        "        quantized_model = model\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported quantization type: {quantization_type}\")\n",
        "    return quantized_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "057ETU_2-rCc"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "def fine_tune(net, trainloader, valloader, device):\n",
        "    # Define the loss function and the optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "    # Initialize the learning rate scheduler\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "\n",
        "    for epoch in range(3):  # Run for 3 epochs\n",
        "        net.train()  # Set the network to training mode\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in trainloader:  # Iterate over the training data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()  # Zero the parameter gradients\n",
        "            outputs = net(images)  # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Compute the loss\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Optimize\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(trainloader)\n",
        "        train_acc = correct / total\n",
        "\n",
        "        net.eval()  # Set the network to evaluation mode\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient computation\n",
        "            for images, labels in valloader:  # Iterate over the validation data\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = net(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        valid_loss = running_loss / len(valloader)\n",
        "        valid_acc = correct / total\n",
        "\n",
        "        # Print the training and validation statistics\n",
        "        print(f\"Epoch {epoch+1}/3 - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}\")\n",
        "\n",
        "        scheduler.step(valid_loss)  # Adjust the learning rate based on the validation loss\n",
        "\n",
        "    return net  # Return the trained network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qUI0TTu-tA_"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "def compress_model(net, pruning_technique, pruning_amount, quantization_type):\n",
        "    # Apply pruning\n",
        "    apply_pruning(net, pruning_technique, pruning_amount)\n",
        "\n",
        "    # Re-enable gradients for the remaining parameters\n",
        "    for param in net.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # Fine-tune the pruned model for 3 epochs\n",
        "    fine_tune(net, trainloader, valloader, DEVICE)\n",
        "\n",
        "    # Apply quantization\n",
        "    quantized_model = apply_quantization(net, quantization_type)\n",
        "\n",
        "    return quantized_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx_dtIlv-u0Z"
      },
      "outputs": [],
      "source": [
        "def determine_compression_parameters(client_data_variabilities):\n",
        "    avg_variability = np.mean(client_data_variabilities)\n",
        "    pruning_amount = np.interp(avg_variability, [0, 1], [0.2, 0.8])\n",
        "    quantization_type = 'dynamic' if avg_variability > 0.5 else 'static'\n",
        "    return pruning_amount, quantization_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhHTvSYv-w3t"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import flwr as fl\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, net, trainloader, valloader, testloader, pca, train_removed_percentage, test_removed_percentage, early_stopping_epoch, n_components, available_bandwidth=1000, pruning_amount=None, quantization_type=None):\n",
        "        self.net = net\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = valloader\n",
        "        self.testloader = testloader\n",
        "        self.pca = pca\n",
        "        self.train_removed_percentage = train_removed_percentage\n",
        "        self.test_removed_percentage = test_removed_percentage\n",
        "        self.early_stopping_epoch = early_stopping_epoch\n",
        "        self.n_components = n_components\n",
        "        self.original_model_size = get_model_size(net)\n",
        "        self.communication_rounds = 0\n",
        "        self.communication_cost = 0\n",
        "        self.bandwidth_utilization = 0\n",
        "        self.latency = 0\n",
        "        self.available_bandwidth = float(available_bandwidth)\n",
        "        self.pruning_amount = pruning_amount\n",
        "        self.quantization_type = quantization_type\n",
        "\n",
        "\n",
        "    def get_parameters(self, **kwargs):\n",
        "        parameters = get_parameters(self.net)\n",
        "        return parameters\n",
        "\n",
        "    def set_parameters(self, parameters):\n",
        "        set_parameters(self.net, parameters)\n",
        "\n",
        "    def calculate_bandwidth_utilization(self, model_size_before_compression, model_size_after_compression):\n",
        "        # Calculate the size of the data sent and received in bytes\n",
        "        data_sent = model_size_after_compression\n",
        "        data_received = model_size_after_compression\n",
        "\n",
        "        # Calculate the total data exchanged\n",
        "        total_data = data_sent + data_received\n",
        "\n",
        "        # Convert the total data to megabits (Mb)\n",
        "        total_data_mb = total_data / (1024 * 1024) * 8\n",
        "\n",
        "        # Calculate the bandwidth utilization percentage\n",
        "        bandwidth_utilization = (total_data_mb / float(self.available_bandwidth)) * 100\n",
        "\n",
        "        return bandwidth_utilization\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        start_time = time.time()\n",
        "        self.set_parameters(parameters)\n",
        "        epochs = config.get(\"epochs\", self.early_stopping_epoch)\n",
        "\n",
        "        if self.pruning_amount is None or self.quantization_type is None:\n",
        "            self.pruning_amount, self.quantization_type = determine_compression_parameters(client_data_variabilities)\n",
        "\n",
        "        model_size_before_compression = get_model_size(self.net)\n",
        "        compressed_model = compress_model(self.net, pruning_technique='l1_unstructured', pruning_amount=self.pruning_amount, quantization_type=self.quantization_type)\n",
        "        model_size_after_compression = get_model_size(compressed_model)\n",
        "\n",
        "        # Enable gradients for the compressed model parameters\n",
        "        for param in compressed_model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        compressed_model, _, _, _ = train(compressed_model, self.trainloader, self.valloader, self.testloader, epochs, DEVICE)\n",
        "\n",
        "        reduced_size = model_size_after_compression\n",
        "        compression_reduction_percentage = (1 - reduced_size / model_size_before_compression) * 100\n",
        "\n",
        "        self.communication_rounds += 1\n",
        "        self.communication_cost += reduced_size * (1 - self.train_removed_percentage / 100) * (1 - compression_reduction_percentage / 100)\n",
        "        self.bandwidth_utilization += self.calculate_bandwidth_utilization(model_size_before_compression, model_size_after_compression) * (1 - self.train_removed_percentage / 100) * (1 - compression_reduction_percentage / 100)\n",
        "        self.latency += (time.time() - start_time) * (1 - self.train_removed_percentage / 100) * (1 - compression_reduction_percentage / 100)\n",
        "\n",
        "        return get_parameters(compressed_model), len(self.trainloader.dataset), {\n",
        "            \"pca_reduction_percentage\": self.train_removed_percentage,\n",
        "            \"compression_reduction_percentage\": compression_reduction_percentage,\n",
        "            \"n_components\": self.n_components,\n",
        "            \"communication_rounds\": self.communication_rounds,\n",
        "            \"communication_cost\": self.communication_cost,\n",
        "            \"bandwidth_utilization\": self.bandwidth_utilization,\n",
        "            \"latency\": self.latency,\n",
        "        }\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        start_time = time.time()\n",
        "        self.set_parameters(parameters)\n",
        "\n",
        "        if self.pruning_amount is None or self.quantization_type is None:\n",
        "            # Determine compression parameters based on client data variability\n",
        "            self.pruning_amount, self.quantization_type = determine_compression_parameters(client_data_variabilities)\n",
        "\n",
        "        model_size_before_compression = get_model_size(self.net)\n",
        "        compressed_model = compress_model(self.net, pruning_technique='l1_unstructured', pruning_amount=self.pruning_amount, quantization_type=self.quantization_type)\n",
        "        model_size_after_compression = get_model_size(compressed_model)\n",
        "\n",
        "        reduced_size = model_size_after_compression\n",
        "        compression_reduction_percentage = (1 - reduced_size / model_size_before_compression) * 100\n",
        "        loss, accuracy = test(compressed_model, self.testloader)\n",
        "\n",
        "        self.communication_rounds += 1\n",
        "        self.communication_cost += reduced_size * (1 - self.test_removed_percentage / 100) * (1 - compression_reduction_percentage / 100)\n",
        "        self.bandwidth_utilization += self.calculate_bandwidth_utilization(model_size_before_compression, model_size_after_compression) * (1 - self.test_removed_percentage / 100) * (1 - compression_reduction_percentage / 100)\n",
        "        self.latency += (time.time() - start_time) * (1 - self.test_removed_percentage / 100) * (1 - compression_reduction_percentage / 100)\n",
        "\n",
        "        return float(loss), len(self.testloader.dataset), {\n",
        "            \"accuracy\": float(accuracy),\n",
        "            \"pca_reduction_percentage\": float(self.test_removed_percentage),\n",
        "            \"compression_reduction_percentage\": compression_reduction_percentage,\n",
        "            \"n_components\": float(self.n_components),\n",
        "            \"communication_rounds\": float(self.communication_rounds),\n",
        "            \"communication_cost\": float(self.communication_cost),\n",
        "            \"bandwidth_utilization\": float(self.bandwidth_utilization),\n",
        "            \"latency\": float(self.latency),\n",
        "        }\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrVZ_Hjs-y0-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
        "    # Multiply accuracy of each client by number of examples used\n",
        "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
        "    examples = [num_examples for num_examples, _ in metrics]\n",
        "\n",
        "    # Extract PCA reduction percentages from metrics\n",
        "    pca_reduction_percentages = [m[\"pca_reduction_percentage\"] for _, m in metrics]\n",
        "\n",
        "    # Extract compression reduction percentages from metrics\n",
        "    compression_reduction_percentages = [m[\"compression_reduction_percentage\"] for _, m in metrics]\n",
        "\n",
        "    communication_rounds = [m[\"communication_rounds\"] for _, m in metrics]\n",
        "    communication_cost = [m[\"communication_cost\"] for _, m in metrics]\n",
        "    bandwidth_utilization = [m[\"bandwidth_utilization\"] for _, m in metrics]\n",
        "    latency = [m[\"latency\"] for _, m in metrics]\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": sum(accuracies) / sum(examples),\n",
        "        \"pca_reduction_percentage\": sum(pca_reduction_percentages) / len(pca_reduction_percentages),\n",
        "        \"compression_reduction_percentage\": sum(compression_reduction_percentages) / len(compression_reduction_percentages),\n",
        "        \"communication_rounds\": sum(communication_rounds) / len(communication_rounds),\n",
        "        \"communication_cost\": sum(communication_cost),\n",
        "        \"bandwidth_utilization\": sum(bandwidth_utilization) / len(bandwidth_utilization),\n",
        "        \"latency\": sum(latency) / len(latency),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZM8RyNN-0yT",
        "outputId": "155a79af-74df-4ec8-84ad-4dd609c99ed5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with 8 PCA components...\n",
            "Average variability: 2867.7677\n",
            "Determined number of PCA components: 8\n",
            "Epoch 1/100 - Train Loss: 0.7079, Train Acc: 0.7874, Valid Loss: 0.2165, Valid Acc: 0.9396, Test Loss: 0.0151, Test Acc: 0.8567\n",
            "Epoch 2/100 - Train Loss: 0.1816, Train Acc: 0.9437, Valid Loss: 0.1002, Valid Acc: 0.9715, Test Loss: 0.0084, Test Acc: 0.9188\n",
            "Epoch 3/100 - Train Loss: 0.1093, Train Acc: 0.9670, Valid Loss: 0.0673, Valid Acc: 0.9815, Test Loss: 0.0053, Test Acc: 0.9489\n",
            "Epoch 4/100 - Train Loss: 0.0794, Train Acc: 0.9760, Valid Loss: 0.0579, Valid Acc: 0.9799, Test Loss: 0.0054, Test Acc: 0.9502\n",
            "Epoch 5/100 - Train Loss: 0.0579, Train Acc: 0.9812, Valid Loss: 0.0537, Valid Acc: 0.9883, Test Loss: 0.0037, Test Acc: 0.9632\n",
            "Epoch 6/100 - Train Loss: 0.0493, Train Acc: 0.9842, Valid Loss: 0.0400, Valid Acc: 0.9916, Test Loss: 0.0043, Test Acc: 0.9603\n",
            "Epoch 7/100 - Train Loss: 0.0354, Train Acc: 0.9896, Valid Loss: 0.0769, Valid Acc: 0.9698, Test Loss: 0.0049, Test Acc: 0.9547\n",
            "Epoch 8/100 - Train Loss: 0.0281, Train Acc: 0.9916, Valid Loss: 0.0552, Valid Acc: 0.9832, Test Loss: 0.0044, Test Acc: 0.9597\n",
            "Epoch 9/100 - Train Loss: 0.0290, Train Acc: 0.9909, Valid Loss: 0.0362, Valid Acc: 0.9899, Test Loss: 0.0032, Test Acc: 0.9712\n",
            "Epoch 10/100 - Train Loss: 0.0202, Train Acc: 0.9940, Valid Loss: 0.0333, Valid Acc: 0.9933, Test Loss: 0.0032, Test Acc: 0.9717\n",
            "Epoch 11/100 - Train Loss: 0.0150, Train Acc: 0.9953, Valid Loss: 0.0333, Valid Acc: 0.9916, Test Loss: 0.0033, Test Acc: 0.9706\n",
            "Epoch 12/100 - Train Loss: 0.0180, Train Acc: 0.9952, Valid Loss: 0.0403, Valid Acc: 0.9849, Test Loss: 0.0045, Test Acc: 0.9650\n",
            "Epoch 13/100 - Train Loss: 0.0105, Train Acc: 0.9963, Valid Loss: 0.0319, Valid Acc: 0.9950, Test Loss: 0.0039, Test Acc: 0.9679\n",
            "Epoch 14/100 - Train Loss: 0.0113, Train Acc: 0.9961, Valid Loss: 0.0584, Valid Acc: 0.9815, Test Loss: 0.0051, Test Acc: 0.9607\n",
            "Epoch 15/100 - Train Loss: 0.0115, Train Acc: 0.9963, Valid Loss: 0.0287, Valid Acc: 0.9933, Test Loss: 0.0034, Test Acc: 0.9724\n",
            "Epoch 16/100 - Train Loss: 0.0127, Train Acc: 0.9959, Valid Loss: 0.0498, Valid Acc: 0.9883, Test Loss: 0.0035, Test Acc: 0.9698\n",
            "Epoch 17/100 - Train Loss: 0.0081, Train Acc: 0.9981, Valid Loss: 0.0372, Valid Acc: 0.9832, Test Loss: 0.0042, Test Acc: 0.9675\n",
            "Epoch 18/100 - Train Loss: 0.0058, Train Acc: 0.9983, Valid Loss: 0.0364, Valid Acc: 0.9899, Test Loss: 0.0034, Test Acc: 0.9740\n",
            "Epoch 19/100 - Train Loss: 0.0115, Train Acc: 0.9961, Valid Loss: 0.0645, Valid Acc: 0.9815, Test Loss: 0.0047, Test Acc: 0.9619\n",
            "Epoch 20/100 - Train Loss: 0.0107, Train Acc: 0.9972, Valid Loss: 0.0305, Valid Acc: 0.9899, Test Loss: 0.0033, Test Acc: 0.9741\n",
            "Epoch 21/100 - Train Loss: 0.0074, Train Acc: 0.9972, Valid Loss: 0.0496, Valid Acc: 0.9815, Test Loss: 0.0041, Test Acc: 0.9682\n",
            "Epoch 22/100 - Train Loss: 0.0077, Train Acc: 0.9980, Valid Loss: 0.0371, Valid Acc: 0.9899, Test Loss: 0.0034, Test Acc: 0.9728\n",
            "Epoch 23/100 - Train Loss: 0.0043, Train Acc: 0.9989, Valid Loss: 0.0330, Valid Acc: 0.9916, Test Loss: 0.0032, Test Acc: 0.9750\n",
            "Epoch 24/100 - Train Loss: 0.0030, Train Acc: 0.9998, Valid Loss: 0.0341, Valid Acc: 0.9899, Test Loss: 0.0032, Test Acc: 0.9754\n",
            "Epoch 25/100 - Train Loss: 0.0033, Train Acc: 0.9991, Valid Loss: 0.0281, Valid Acc: 0.9916, Test Loss: 0.0033, Test Acc: 0.9751\n",
            "Epoch 26/100 - Train Loss: 0.0035, Train Acc: 0.9989, Valid Loss: 0.0281, Valid Acc: 0.9933, Test Loss: 0.0036, Test Acc: 0.9738\n",
            "Epoch 27/100 - Train Loss: 0.0021, Train Acc: 0.9996, Valid Loss: 0.0276, Valid Acc: 0.9933, Test Loss: 0.0034, Test Acc: 0.9749\n",
            "Epoch 28/100 - Train Loss: 0.0019, Train Acc: 0.9998, Valid Loss: 0.0291, Valid Acc: 0.9933, Test Loss: 0.0035, Test Acc: 0.9755\n",
            "Epoch 29/100 - Train Loss: 0.0017, Train Acc: 0.9996, Valid Loss: 0.0324, Valid Acc: 0.9933, Test Loss: 0.0035, Test Acc: 0.9748\n",
            "Epoch 30/100 - Train Loss: 0.0026, Train Acc: 0.9993, Valid Loss: 0.0301, Valid Acc: 0.9933, Test Loss: 0.0034, Test Acc: 0.9751\n",
            "Epoch 31/100 - Train Loss: 0.0013, Train Acc: 0.9998, Valid Loss: 0.0321, Valid Acc: 0.9933, Test Loss: 0.0035, Test Acc: 0.9738\n",
            "Epoch 32/100 - Train Loss: 0.0020, Train Acc: 0.9994, Valid Loss: 0.0305, Valid Acc: 0.9933, Test Loss: 0.0034, Test Acc: 0.9754\n",
            "Epoch 33/100 - Train Loss: 0.0012, Train Acc: 1.0000, Valid Loss: 0.0307, Valid Acc: 0.9933, Test Loss: 0.0034, Test Acc: 0.9758\n",
            "Epoch 34/100 - Train Loss: 0.0013, Train Acc: 1.0000, Valid Loss: 0.0305, Valid Acc: 0.9933, Test Loss: 0.0034, Test Acc: 0.9756\n",
            "Epoch 35/100 - Train Loss: 0.0011, Train Acc: 1.0000, Valid Loss: 0.0310, Valid Acc: 0.9933, Test Loss: 0.0034, Test Acc: 0.9753\n",
            "Epoch 36/100 - Train Loss: 0.0017, Train Acc: 0.9998, Valid Loss: 0.0312, Valid Acc: 0.9933, Test Loss: 0.0034, Test Acc: 0.9753\n",
            "Epoch 37/100 - Train Loss: 0.0016, Train Acc: 0.9998, Valid Loss: 0.0310, Valid Acc: 0.9933, Test Loss: 0.0034, Test Acc: 0.9752\n",
            "Final test set performance for 8 PCA components: Loss 0.0034, Accuracy 0.9749\n",
            "Early stopping at epoch: 37\n",
            "\n",
            "Training with 16 PCA components...\n",
            "Average variability: 2867.7677\n",
            "Determined number of PCA components: 16\n",
            "Epoch 1/100 - Train Loss: 0.6905, Train Acc: 0.7934, Valid Loss: 0.2241, Valid Acc: 0.9430, Test Loss: 0.0138, Test Acc: 0.8824\n",
            "Epoch 2/100 - Train Loss: 0.1842, Train Acc: 0.9462, Valid Loss: 0.1227, Valid Acc: 0.9681, Test Loss: 0.0069, Test Acc: 0.9382\n",
            "Epoch 3/100 - Train Loss: 0.1102, Train Acc: 0.9650, Valid Loss: 0.1022, Valid Acc: 0.9681, Test Loss: 0.0058, Test Acc: 0.9454\n",
            "Epoch 4/100 - Train Loss: 0.0787, Train Acc: 0.9762, Valid Loss: 0.0975, Valid Acc: 0.9681, Test Loss: 0.0048, Test Acc: 0.9508\n",
            "Epoch 5/100 - Train Loss: 0.0637, Train Acc: 0.9784, Valid Loss: 0.1053, Valid Acc: 0.9698, Test Loss: 0.0048, Test Acc: 0.9514\n",
            "Epoch 6/100 - Train Loss: 0.0519, Train Acc: 0.9843, Valid Loss: 0.0893, Valid Acc: 0.9715, Test Loss: 0.0042, Test Acc: 0.9594\n",
            "Epoch 7/100 - Train Loss: 0.0409, Train Acc: 0.9842, Valid Loss: 0.0730, Valid Acc: 0.9732, Test Loss: 0.0042, Test Acc: 0.9594\n",
            "Epoch 8/100 - Train Loss: 0.0282, Train Acc: 0.9907, Valid Loss: 0.0816, Valid Acc: 0.9748, Test Loss: 0.0037, Test Acc: 0.9663\n",
            "Epoch 9/100 - Train Loss: 0.0282, Train Acc: 0.9901, Valid Loss: 0.0855, Valid Acc: 0.9732, Test Loss: 0.0043, Test Acc: 0.9620\n",
            "Epoch 10/100 - Train Loss: 0.0182, Train Acc: 0.9950, Valid Loss: 0.0877, Valid Acc: 0.9748, Test Loss: 0.0042, Test Acc: 0.9638\n",
            "Epoch 11/100 - Train Loss: 0.0192, Train Acc: 0.9955, Valid Loss: 0.1080, Valid Acc: 0.9748, Test Loss: 0.0052, Test Acc: 0.9550\n",
            "Epoch 12/100 - Train Loss: 0.0159, Train Acc: 0.9950, Valid Loss: 0.0976, Valid Acc: 0.9765, Test Loss: 0.0046, Test Acc: 0.9599\n",
            "Epoch 13/100 - Train Loss: 0.0178, Train Acc: 0.9950, Valid Loss: 0.0899, Valid Acc: 0.9732, Test Loss: 0.0040, Test Acc: 0.9672\n",
            "Epoch 14/100 - Train Loss: 0.0074, Train Acc: 0.9972, Valid Loss: 0.0831, Valid Acc: 0.9748, Test Loss: 0.0036, Test Acc: 0.9689\n",
            "Epoch 15/100 - Train Loss: 0.0051, Train Acc: 0.9994, Valid Loss: 0.0845, Valid Acc: 0.9765, Test Loss: 0.0034, Test Acc: 0.9695\n",
            "Epoch 16/100 - Train Loss: 0.0052, Train Acc: 0.9987, Valid Loss: 0.0847, Valid Acc: 0.9765, Test Loss: 0.0035, Test Acc: 0.9716\n",
            "Epoch 17/100 - Train Loss: 0.0056, Train Acc: 0.9981, Valid Loss: 0.0830, Valid Acc: 0.9765, Test Loss: 0.0035, Test Acc: 0.9710\n",
            "Final test set performance for 16 PCA components: Loss 0.0042, Accuracy 0.9594\n",
            "Early stopping at epoch: 17\n",
            "\n",
            "Training with 32 PCA components...\n",
            "Average variability: 2867.7677\n",
            "Determined number of PCA components: 32\n",
            "Epoch 1/100 - Train Loss: 0.7109, Train Acc: 0.7896, Valid Loss: 0.2401, Valid Acc: 0.9312, Test Loss: 0.0136, Test Acc: 0.8858\n",
            "Epoch 2/100 - Train Loss: 0.2019, Train Acc: 0.9402, Valid Loss: 0.1548, Valid Acc: 0.9413, Test Loss: 0.0079, Test Acc: 0.9292\n",
            "Epoch 3/100 - Train Loss: 0.1298, Train Acc: 0.9599, Valid Loss: 0.1189, Valid Acc: 0.9631, Test Loss: 0.0065, Test Acc: 0.9424\n",
            "Epoch 4/100 - Train Loss: 0.0986, Train Acc: 0.9693, Valid Loss: 0.0927, Valid Acc: 0.9664, Test Loss: 0.0051, Test Acc: 0.9512\n",
            "Epoch 5/100 - Train Loss: 0.0753, Train Acc: 0.9775, Valid Loss: 0.0852, Valid Acc: 0.9715, Test Loss: 0.0050, Test Acc: 0.9519\n",
            "Epoch 6/100 - Train Loss: 0.0565, Train Acc: 0.9845, Valid Loss: 0.0890, Valid Acc: 0.9681, Test Loss: 0.0052, Test Acc: 0.9450\n",
            "Epoch 7/100 - Train Loss: 0.0563, Train Acc: 0.9806, Valid Loss: 0.0767, Valid Acc: 0.9715, Test Loss: 0.0040, Test Acc: 0.9634\n",
            "Epoch 8/100 - Train Loss: 0.0402, Train Acc: 0.9875, Valid Loss: 0.0674, Valid Acc: 0.9715, Test Loss: 0.0045, Test Acc: 0.9599\n",
            "Epoch 9/100 - Train Loss: 0.0329, Train Acc: 0.9901, Valid Loss: 0.0716, Valid Acc: 0.9715, Test Loss: 0.0039, Test Acc: 0.9641\n",
            "Epoch 10/100 - Train Loss: 0.0308, Train Acc: 0.9892, Valid Loss: 0.0650, Valid Acc: 0.9698, Test Loss: 0.0041, Test Acc: 0.9633\n",
            "Epoch 11/100 - Train Loss: 0.0231, Train Acc: 0.9925, Valid Loss: 0.0699, Valid Acc: 0.9748, Test Loss: 0.0036, Test Acc: 0.9682\n",
            "Epoch 12/100 - Train Loss: 0.0207, Train Acc: 0.9924, Valid Loss: 0.0592, Valid Acc: 0.9782, Test Loss: 0.0033, Test Acc: 0.9697\n",
            "Epoch 13/100 - Train Loss: 0.0211, Train Acc: 0.9937, Valid Loss: 0.0959, Valid Acc: 0.9748, Test Loss: 0.0038, Test Acc: 0.9662\n",
            "Epoch 14/100 - Train Loss: 0.0181, Train Acc: 0.9937, Valid Loss: 0.0797, Valid Acc: 0.9748, Test Loss: 0.0038, Test Acc: 0.9670\n",
            "Epoch 15/100 - Train Loss: 0.0211, Train Acc: 0.9933, Valid Loss: 0.0700, Valid Acc: 0.9782, Test Loss: 0.0033, Test Acc: 0.9701\n",
            "Epoch 16/100 - Train Loss: 0.0161, Train Acc: 0.9957, Valid Loss: 0.0661, Valid Acc: 0.9782, Test Loss: 0.0037, Test Acc: 0.9691\n",
            "Epoch 17/100 - Train Loss: 0.0099, Train Acc: 0.9966, Valid Loss: 0.0759, Valid Acc: 0.9748, Test Loss: 0.0039, Test Acc: 0.9690\n",
            "Epoch 18/100 - Train Loss: 0.0119, Train Acc: 0.9968, Valid Loss: 0.0868, Valid Acc: 0.9732, Test Loss: 0.0057, Test Acc: 0.9578\n",
            "Epoch 19/100 - Train Loss: 0.0089, Train Acc: 0.9966, Valid Loss: 0.0719, Valid Acc: 0.9732, Test Loss: 0.0044, Test Acc: 0.9675\n",
            "Epoch 20/100 - Train Loss: 0.0079, Train Acc: 0.9968, Valid Loss: 0.0685, Valid Acc: 0.9748, Test Loss: 0.0039, Test Acc: 0.9709\n",
            "Epoch 21/100 - Train Loss: 0.0067, Train Acc: 0.9983, Valid Loss: 0.0695, Valid Acc: 0.9748, Test Loss: 0.0039, Test Acc: 0.9703\n",
            "Epoch 22/100 - Train Loss: 0.0045, Train Acc: 0.9989, Valid Loss: 0.0677, Valid Acc: 0.9748, Test Loss: 0.0038, Test Acc: 0.9711\n",
            "Final test set performance for 32 PCA components: Loss 0.0033, Accuracy 0.9697\n",
            "Early stopping at epoch: 22\n",
            "\n",
            "Training with 64 PCA components...\n",
            "Average variability: 2867.7677\n",
            "Determined number of PCA components: 64\n",
            "Epoch 1/100 - Train Loss: 0.7003, Train Acc: 0.7942, Valid Loss: 0.2289, Valid Acc: 0.9295, Test Loss: 0.0141, Test Acc: 0.8619\n",
            "Epoch 2/100 - Train Loss: 0.2033, Train Acc: 0.9424, Valid Loss: 0.1312, Valid Acc: 0.9597, Test Loss: 0.0073, Test Acc: 0.9359\n",
            "Epoch 3/100 - Train Loss: 0.1240, Train Acc: 0.9622, Valid Loss: 0.1021, Valid Acc: 0.9631, Test Loss: 0.0059, Test Acc: 0.9455\n",
            "Epoch 4/100 - Train Loss: 0.0955, Train Acc: 0.9696, Valid Loss: 0.0856, Valid Acc: 0.9732, Test Loss: 0.0060, Test Acc: 0.9456\n",
            "Epoch 5/100 - Train Loss: 0.0679, Train Acc: 0.9803, Valid Loss: 0.0641, Valid Acc: 0.9799, Test Loss: 0.0047, Test Acc: 0.9537\n",
            "Epoch 6/100 - Train Loss: 0.0616, Train Acc: 0.9832, Valid Loss: 0.0656, Valid Acc: 0.9832, Test Loss: 0.0038, Test Acc: 0.9620\n",
            "Epoch 7/100 - Train Loss: 0.0475, Train Acc: 0.9857, Valid Loss: 0.0475, Valid Acc: 0.9815, Test Loss: 0.0036, Test Acc: 0.9661\n",
            "Epoch 8/100 - Train Loss: 0.0425, Train Acc: 0.9864, Valid Loss: 0.0527, Valid Acc: 0.9815, Test Loss: 0.0039, Test Acc: 0.9629\n",
            "Epoch 9/100 - Train Loss: 0.0282, Train Acc: 0.9901, Valid Loss: 0.0469, Valid Acc: 0.9832, Test Loss: 0.0033, Test Acc: 0.9677\n",
            "Epoch 10/100 - Train Loss: 0.0244, Train Acc: 0.9918, Valid Loss: 0.0429, Valid Acc: 0.9916, Test Loss: 0.0041, Test Acc: 0.9647\n",
            "Epoch 11/100 - Train Loss: 0.0262, Train Acc: 0.9916, Valid Loss: 0.0395, Valid Acc: 0.9899, Test Loss: 0.0032, Test Acc: 0.9685\n",
            "Epoch 12/100 - Train Loss: 0.0197, Train Acc: 0.9944, Valid Loss: 0.0476, Valid Acc: 0.9883, Test Loss: 0.0043, Test Acc: 0.9631\n",
            "Epoch 13/100 - Train Loss: 0.0203, Train Acc: 0.9935, Valid Loss: 0.0714, Valid Acc: 0.9799, Test Loss: 0.0039, Test Acc: 0.9653\n",
            "Epoch 14/100 - Train Loss: 0.0185, Train Acc: 0.9937, Valid Loss: 0.0289, Valid Acc: 0.9933, Test Loss: 0.0040, Test Acc: 0.9673\n",
            "Epoch 15/100 - Train Loss: 0.0105, Train Acc: 0.9978, Valid Loss: 0.0474, Valid Acc: 0.9849, Test Loss: 0.0034, Test Acc: 0.9725\n",
            "Epoch 16/100 - Train Loss: 0.0130, Train Acc: 0.9963, Valid Loss: 0.0322, Valid Acc: 0.9916, Test Loss: 0.0039, Test Acc: 0.9677\n",
            "Epoch 17/100 - Train Loss: 0.0140, Train Acc: 0.9952, Valid Loss: 0.0401, Valid Acc: 0.9883, Test Loss: 0.0036, Test Acc: 0.9684\n",
            "Epoch 18/100 - Train Loss: 0.0112, Train Acc: 0.9966, Valid Loss: 0.0659, Valid Acc: 0.9799, Test Loss: 0.0059, Test Acc: 0.9526\n",
            "Epoch 19/100 - Train Loss: 0.0139, Train Acc: 0.9950, Valid Loss: 0.0437, Valid Acc: 0.9815, Test Loss: 0.0039, Test Acc: 0.9696\n",
            "Epoch 20/100 - Train Loss: 0.0117, Train Acc: 0.9950, Valid Loss: 0.0467, Valid Acc: 0.9866, Test Loss: 0.0033, Test Acc: 0.9717\n",
            "Epoch 21/100 - Train Loss: 0.0057, Train Acc: 0.9978, Valid Loss: 0.0409, Valid Acc: 0.9866, Test Loss: 0.0032, Test Acc: 0.9735\n",
            "Epoch 22/100 - Train Loss: 0.0044, Train Acc: 0.9989, Valid Loss: 0.0392, Valid Acc: 0.9866, Test Loss: 0.0033, Test Acc: 0.9732\n",
            "Epoch 23/100 - Train Loss: 0.0038, Train Acc: 0.9993, Valid Loss: 0.0374, Valid Acc: 0.9866, Test Loss: 0.0033, Test Acc: 0.9738\n",
            "Epoch 24/100 - Train Loss: 0.0022, Train Acc: 1.0000, Valid Loss: 0.0368, Valid Acc: 0.9866, Test Loss: 0.0033, Test Acc: 0.9739\n",
            "Final test set performance for 64 PCA components: Loss 0.0040, Accuracy 0.9673\n",
            "Early stopping at epoch: 24\n",
            "\n",
            "Training with 128 PCA components...\n",
            "Average variability: 2867.7677\n",
            "Determined number of PCA components: 128\n",
            "Epoch 1/100 - Train Loss: 0.6593, Train Acc: 0.8022, Valid Loss: 0.2191, Valid Acc: 0.9346, Test Loss: 0.0134, Test Acc: 0.8914\n",
            "Epoch 2/100 - Train Loss: 0.1930, Train Acc: 0.9410, Valid Loss: 0.1288, Valid Acc: 0.9581, Test Loss: 0.0076, Test Acc: 0.9226\n",
            "Epoch 3/100 - Train Loss: 0.1170, Train Acc: 0.9672, Valid Loss: 0.0880, Valid Acc: 0.9765, Test Loss: 0.0058, Test Acc: 0.9447\n",
            "Epoch 4/100 - Train Loss: 0.0748, Train Acc: 0.9790, Valid Loss: 0.0760, Valid Acc: 0.9782, Test Loss: 0.0047, Test Acc: 0.9541\n",
            "Epoch 5/100 - Train Loss: 0.0697, Train Acc: 0.9791, Valid Loss: 0.0851, Valid Acc: 0.9765, Test Loss: 0.0048, Test Acc: 0.9524\n",
            "Epoch 6/100 - Train Loss: 0.0486, Train Acc: 0.9857, Valid Loss: 0.0667, Valid Acc: 0.9832, Test Loss: 0.0037, Test Acc: 0.9666\n",
            "Epoch 7/100 - Train Loss: 0.0374, Train Acc: 0.9894, Valid Loss: 0.0816, Valid Acc: 0.9782, Test Loss: 0.0040, Test Acc: 0.9613\n",
            "Epoch 8/100 - Train Loss: 0.0329, Train Acc: 0.9896, Valid Loss: 0.0736, Valid Acc: 0.9765, Test Loss: 0.0033, Test Acc: 0.9675\n",
            "Epoch 9/100 - Train Loss: 0.0266, Train Acc: 0.9927, Valid Loss: 0.0866, Valid Acc: 0.9715, Test Loss: 0.0036, Test Acc: 0.9660\n",
            "Epoch 10/100 - Train Loss: 0.0257, Train Acc: 0.9907, Valid Loss: 0.0680, Valid Acc: 0.9832, Test Loss: 0.0036, Test Acc: 0.9687\n",
            "Epoch 11/100 - Train Loss: 0.0193, Train Acc: 0.9937, Valid Loss: 0.0708, Valid Acc: 0.9849, Test Loss: 0.0033, Test Acc: 0.9710\n",
            "Epoch 12/100 - Train Loss: 0.0109, Train Acc: 0.9978, Valid Loss: 0.0735, Valid Acc: 0.9815, Test Loss: 0.0036, Test Acc: 0.9697\n",
            "Epoch 13/100 - Train Loss: 0.0087, Train Acc: 0.9976, Valid Loss: 0.0697, Valid Acc: 0.9815, Test Loss: 0.0032, Test Acc: 0.9734\n",
            "Epoch 14/100 - Train Loss: 0.0067, Train Acc: 0.9981, Valid Loss: 0.0707, Valid Acc: 0.9815, Test Loss: 0.0031, Test Acc: 0.9726\n",
            "Epoch 15/100 - Train Loss: 0.0041, Train Acc: 0.9996, Valid Loss: 0.0728, Valid Acc: 0.9815, Test Loss: 0.0031, Test Acc: 0.9732\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      Starting Flower simulation, config: num_rounds=37, no round_timeout\n",
            "INFO:flwr:Starting Flower simulation, config: num_rounds=37, no round_timeout\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/100 - Train Loss: 0.0057, Train Acc: 0.9983, Valid Loss: 0.0724, Valid Acc: 0.9815, Test Loss: 0.0032, Test Acc: 0.9732\n",
            "Final test set performance for 128 PCA components: Loss 0.0037, Accuracy 0.9666\n",
            "Early stopping at epoch: 16\n",
            "\n",
            "Best PCA Components: 8\n",
            "Best Test Accuracy: 0.9749\n",
            "Best Test Loss: 0.0034\n",
            "Best Early Stopping Epoch: 37\n",
            "Quantization Type: dynamic\n",
            "Pruning Amount: 0.8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-17 01:42:49,874\tINFO worker.py:1621 -- Started a local Ray instance.\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'node:__internal_head__': 1.0, 'node:172.28.0.12': 1.0, 'object_store_memory': 3918724300.0, 'memory': 7837448603.0}\n",
            "INFO:flwr:Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'node:__internal_head__': 1.0, 'node:172.28.0.12': 1.0, 'object_store_memory': 3918724300.0, 'memory': 7837448603.0}\n",
            "\u001b[92mINFO \u001b[0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
            "INFO:flwr:Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
            "\u001b[92mINFO \u001b[0m:      No `client_resources` specified. Using minimal resources for clients.\n",
            "INFO:flwr:No `client_resources` specified. Using minimal resources for clients.\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
            "INFO:flwr:Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Creating VirtualClientEngineActorPool with 2 actors\n",
            "INFO:flwr:Flower VCE: Creating VirtualClientEngineActorPool with 2 actors\n",
            "\u001b[92mINFO \u001b[0m:      [INIT]\n",
            "INFO:flwr:[INIT]\n",
            "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
            "INFO:flwr:Requesting initial parameters from one random client\n",
            "\u001b[2m\u001b[36m(pid=16051)\u001b[0m 2024-04-17 01:42:56.934096: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=16051)\u001b[0m 2024-04-17 01:42:56.934196: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=16051)\u001b[0m 2024-04-17 01:42:56.937897: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
            "INFO:flwr:Received initial parameters from one random client\n",
            "\u001b[92mINFO \u001b[0m:      Evaluating initial global parameters\n",
            "INFO:flwr:Evaluating initial global parameters\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "INFO:flwr:\n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
            "INFO:flwr:[ROUND 1]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n",
            "INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.bias: torch.Size([10])\u001b[32m [repeated 16x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 1/3 - Train Loss: 0.8470, Train Acc: 0.7461, Valid Loss: 0.3365, Valid Acc: 0.8859\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/3 - Train Loss: 0.8461, Train Acc: 0.7433, Valid Loss: 0.3685, Valid Acc: 0.8943\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 2/3 - Train Loss: 0.2831, Train Acc: 0.9119, Valid Loss: 0.1922, Valid Acc: 0.9463\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/3 - Train Loss: 0.2725, Train Acc: 0.9136, Valid Loss: 0.1756, Valid Acc: 0.9530\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/3 - Train Loss: 0.1784, Train Acc: 0.9477, Valid Loss: 0.1340, Valid Acc: 0.9597\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: quantized::linear_dynamic: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[2m\u001b[36m(pid=16053)\u001b[0m 2024-04-17 01:42:56.987102: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=16053)\u001b[0m 2024-04-17 01:42:56.987174: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=16053)\u001b[0m 2024-04-17 01:42:56.989027: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/3 - Train Loss: 0.1590, Train Acc: 0.9531, Valid Loss: 0.1016, Valid Acc: 0.9698\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: quantized::linear_dynamic: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 1/37 - Train Loss: 0.3938, Train Acc: 0.8800, Valid Loss: 0.3924, Valid Acc: 0.8863, Test Loss: 0.0088, Test Acc: 0.9233\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/37 - Train Loss: 0.3233, Train Acc: 0.8982, Valid Loss: 0.2656, Valid Acc: 0.9145, Test Loss: 0.0070, Test Acc: 0.9329\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 2/37 - Train Loss: 0.4059, Train Acc: 0.8811, Valid Loss: 0.3924, Valid Acc: 0.8863, Test Loss: 0.0088, Test Acc: 0.9233\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/37 - Train Loss: 0.3277, Train Acc: 0.9034, Valid Loss: 0.2656, Valid Acc: 0.9145, Test Loss: 0.0070, Test Acc: 0.9329\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/37 - Train Loss: 0.4022, Train Acc: 0.8808, Valid Loss: 0.3924, Valid Acc: 0.8863, Test Loss: 0.0088, Test Acc: 0.9233\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/37 - Train Loss: 0.3116, Train Acc: 0.9054, Valid Loss: 0.2656, Valid Acc: 0.9145, Test Loss: 0.0070, Test Acc: 0.9329\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 4/37 - Train Loss: 0.4077, Train Acc: 0.8778, Valid Loss: 0.3924, Valid Acc: 0.8863, Test Loss: 0.0088, Test Acc: 0.9233\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 4/37 - Train Loss: 0.3179, Train Acc: 0.9035, Valid Loss: 0.2656, Valid Acc: 0.9145, Test Loss: 0.0070, Test Acc: 0.9329\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 5/37 - Train Loss: 0.4078, Train Acc: 0.8783, Valid Loss: 0.3924, Valid Acc: 0.8863, Test Loss: 0.0088, Test Acc: 0.9233\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 5/37 - Train Loss: 0.3091, Train Acc: 0.9077, Valid Loss: 0.2656, Valid Acc: 0.9145, Test Loss: 0.0070, Test Acc: 0.9329\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 6/37 - Train Loss: 0.3930, Train Acc: 0.8861, Valid Loss: 0.3924, Valid Acc: 0.8863, Test Loss: 0.0088, Test Acc: 0.9233\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 6/37 - Train Loss: 0.3160, Train Acc: 0.9024, Valid Loss: 0.2656, Valid Acc: 0.9145, Test Loss: 0.0070, Test Acc: 0.9329\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 7/37 - Train Loss: 0.3994, Train Acc: 0.8813, Valid Loss: 0.3924, Valid Acc: 0.8863, Test Loss: 0.0088, Test Acc: 0.9233\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 7/37 - Train Loss: 0.3159, Train Acc: 0.9019, Valid Loss: 0.2656, Valid Acc: 0.9145, Test Loss: 0.0070, Test Acc: 0.9329\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 8/37 - Train Loss: 0.3958, Train Acc: 0.8819, Valid Loss: 0.3924, Valid Acc: 0.8863, Test Loss: 0.0088, Test Acc: 0.9233\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 8/37 - Train Loss: 0.3184, Train Acc: 0.9008, Valid Loss: 0.2656, Valid Acc: 0.9145, Test Loss: 0.0070, Test Acc: 0.9329\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 9/37 - Train Loss: 0.3974, Train Acc: 0.8811, Valid Loss: 0.3924, Valid Acc: 0.8863, Test Loss: 0.0088, Test Acc: 0.9233\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 9/37 - Train Loss: 0.3160, Train Acc: 0.9026, Valid Loss: 0.2656, Valid Acc: 0.9145, Test Loss: 0.0070, Test Acc: 0.9329\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 10/37 - Train Loss: 0.4045, Train Acc: 0.8804, Valid Loss: 0.3924, Valid Acc: 0.8863, Test Loss: 0.0088, Test Acc: 0.9233\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 10/37 - Train Loss: 0.3169, Train Acc: 0.9023, Valid Loss: 0.2656, Valid Acc: 0.9145, Test Loss: 0.0070, Test Acc: 0.9329\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 11/37 - Train Loss: 0.4019, Train Acc: 0.8826, Valid Loss: 0.3924, Valid Acc: 0.8863, Test Loss: 0.0088, Test Acc: 0.9233\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 11/37 - Train Loss: 0.3167, Train Acc: 0.9004, Valid Loss: 0.2656, Valid Acc: 0.9145, Test Loss: 0.0070, Test Acc: 0.9329\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.bias: torch.Size([10])\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 1/3 - Train Loss: 0.8084, Train Acc: 0.7556, Valid Loss: 0.2874, Valid Acc: 0.9111\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/3 - Train Loss: 0.8513, Train Acc: 0.7398, Valid Loss: 0.3196, Valid Acc: 0.9128\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 2/3 - Train Loss: 0.2634, Train Acc: 0.9199, Valid Loss: 0.1832, Valid Acc: 0.9413\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/3 - Train Loss: 0.2884, Train Acc: 0.9115, Valid Loss: 0.2221, Valid Acc: 0.9228\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/3 - Train Loss: 0.1613, Train Acc: 0.9508, Valid Loss: 0.1034, Valid Acc: 0.9698\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: quantized::linear_dynamic: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/3 - Train Loss: 0.1751, Train Acc: 0.9452, Valid Loss: 0.1137, Valid Acc: 0.9648\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: quantized::linear_dynamic: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 1/37 - Train Loss: 0.2519, Train Acc: 0.9219, Valid Loss: 0.2020, Valid Acc: 0.9362, Test Loss: 0.0079, Test Acc: 0.9289\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/37 - Train Loss: 0.1637, Train Acc: 0.9532, Valid Loss: 0.1081, Valid Acc: 0.9698, Test Loss: 0.0085, Test Acc: 0.9207\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 2/37 - Train Loss: 0.2623, Train Acc: 0.9163, Valid Loss: 0.2020, Valid Acc: 0.9362, Test Loss: 0.0079, Test Acc: 0.9289\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/37 - Train Loss: 0.1675, Train Acc: 0.9516, Valid Loss: 0.1081, Valid Acc: 0.9698, Test Loss: 0.0085, Test Acc: 0.9207\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/37 - Train Loss: 0.2527, Train Acc: 0.9193, Valid Loss: 0.2020, Valid Acc: 0.9362, Test Loss: 0.0079, Test Acc: 0.9289\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/37 - Train Loss: 0.1611, Train Acc: 0.9538, Valid Loss: 0.1081, Valid Acc: 0.9698, Test Loss: 0.0085, Test Acc: 0.9207\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 4/37 - Train Loss: 0.2543, Train Acc: 0.9202, Valid Loss: 0.2020, Valid Acc: 0.9362, Test Loss: 0.0079, Test Acc: 0.9289\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 4/37 - Train Loss: 0.1591, Train Acc: 0.9527, Valid Loss: 0.1081, Valid Acc: 0.9698, Test Loss: 0.0085, Test Acc: 0.9207\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 5/37 - Train Loss: 0.2576, Train Acc: 0.9189, Valid Loss: 0.2020, Valid Acc: 0.9362, Test Loss: 0.0079, Test Acc: 0.9289\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 5/37 - Train Loss: 0.1635, Train Acc: 0.9521, Valid Loss: 0.1081, Valid Acc: 0.9698, Test Loss: 0.0085, Test Acc: 0.9207\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 6/37 - Train Loss: 0.2512, Train Acc: 0.9232, Valid Loss: 0.2020, Valid Acc: 0.9362, Test Loss: 0.0079, Test Acc: 0.9289\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 6/37 - Train Loss: 0.1615, Train Acc: 0.9542, Valid Loss: 0.1081, Valid Acc: 0.9698, Test Loss: 0.0085, Test Acc: 0.9207\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 7/37 - Train Loss: 0.2509, Train Acc: 0.9219, Valid Loss: 0.2020, Valid Acc: 0.9362, Test Loss: 0.0079, Test Acc: 0.9289\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 7/37 - Train Loss: 0.1617, Train Acc: 0.9519, Valid Loss: 0.1081, Valid Acc: 0.9698, Test Loss: 0.0085, Test Acc: 0.9207\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 8/37 - Train Loss: 0.2577, Train Acc: 0.9217, Valid Loss: 0.2020, Valid Acc: 0.9362, Test Loss: 0.0079, Test Acc: 0.9289\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 8/37 - Train Loss: 0.1715, Train Acc: 0.9491, Valid Loss: 0.1081, Valid Acc: 0.9698, Test Loss: 0.0085, Test Acc: 0.9207\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 9/37 - Train Loss: 0.2577, Train Acc: 0.9230, Valid Loss: 0.2020, Valid Acc: 0.9362, Test Loss: 0.0079, Test Acc: 0.9289\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 9/37 - Train Loss: 0.1582, Train Acc: 0.9542, Valid Loss: 0.1081, Valid Acc: 0.9698, Test Loss: 0.0085, Test Acc: 0.9207\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 10/37 - Train Loss: 0.2542, Train Acc: 0.9184, Valid Loss: 0.2020, Valid Acc: 0.9362, Test Loss: 0.0079, Test Acc: 0.9289\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 10/37 - Train Loss: 0.1617, Train Acc: 0.9521, Valid Loss: 0.1081, Valid Acc: 0.9698, Test Loss: 0.0085, Test Acc: 0.9207\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 11/37 - Train Loss: 0.2614, Train Acc: 0.9167, Valid Loss: 0.2020, Valid Acc: 0.9362, Test Loss: 0.0079, Test Acc: 0.9289\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 11/37 - Train Loss: 0.1575, Train Acc: 0.9542, Valid Loss: 0.1081, Valid Acc: 0.9698, Test Loss: 0.0085, Test Acc: 0.9207\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.bias: torch.Size([10])\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 1/3 - Train Loss: 0.8312, Train Acc: 0.7471, Valid Loss: 0.3078, Valid Acc: 0.9044\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/3 - Train Loss: 0.8082, Train Acc: 0.7502, Valid Loss: 0.2886, Valid Acc: 0.9128\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 2/3 - Train Loss: 0.2609, Train Acc: 0.9184, Valid Loss: 0.1566, Valid Acc: 0.9463\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/3 - Train Loss: 0.2482, Train Acc: 0.9233, Valid Loss: 0.1575, Valid Acc: 0.9530\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/3 - Train Loss: 0.1595, Train Acc: 0.9527, Valid Loss: 0.1049, Valid Acc: 0.9614\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: quantized::linear_dynamic: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/3 - Train Loss: 0.1544, Train Acc: 0.9542, Valid Loss: 0.0907, Valid Acc: 0.9715\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: quantized::linear_dynamic: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 1/37 - Train Loss: 0.3230, Train Acc: 0.8934, Valid Loss: 0.2940, Valid Acc: 0.9139, Test Loss: 0.0075, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/37 - Train Loss: 0.3284, Train Acc: 0.9004, Valid Loss: 0.2865, Valid Acc: 0.9122, Test Loss: 0.0063, Test Acc: 0.9434\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 2/37 - Train Loss: 0.3217, Train Acc: 0.8962, Valid Loss: 0.2940, Valid Acc: 0.9139, Test Loss: 0.0075, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/37 - Train Loss: 0.3315, Train Acc: 0.8982, Valid Loss: 0.2865, Valid Acc: 0.9122, Test Loss: 0.0063, Test Acc: 0.9434\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/37 - Train Loss: 0.3220, Train Acc: 0.8970, Valid Loss: 0.2940, Valid Acc: 0.9139, Test Loss: 0.0075, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/37 - Train Loss: 0.3284, Train Acc: 0.8967, Valid Loss: 0.2865, Valid Acc: 0.9122, Test Loss: 0.0063, Test Acc: 0.9434\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 4/37 - Train Loss: 0.3224, Train Acc: 0.8953, Valid Loss: 0.2940, Valid Acc: 0.9139, Test Loss: 0.0075, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 4/37 - Train Loss: 0.3252, Train Acc: 0.8999, Valid Loss: 0.2865, Valid Acc: 0.9122, Test Loss: 0.0063, Test Acc: 0.9434\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 5/37 - Train Loss: 0.3181, Train Acc: 0.8977, Valid Loss: 0.2940, Valid Acc: 0.9139, Test Loss: 0.0075, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 5/37 - Train Loss: 0.3260, Train Acc: 0.8935, Valid Loss: 0.2865, Valid Acc: 0.9122, Test Loss: 0.0063, Test Acc: 0.9434\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 6/37 - Train Loss: 0.3272, Train Acc: 0.8921, Valid Loss: 0.2940, Valid Acc: 0.9139, Test Loss: 0.0075, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 6/37 - Train Loss: 0.3360, Train Acc: 0.8946, Valid Loss: 0.2865, Valid Acc: 0.9122, Test Loss: 0.0063, Test Acc: 0.9434\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 7/37 - Train Loss: 0.3245, Train Acc: 0.8962, Valid Loss: 0.2940, Valid Acc: 0.9139, Test Loss: 0.0075, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 7/37 - Train Loss: 0.3288, Train Acc: 0.8975, Valid Loss: 0.2865, Valid Acc: 0.9122, Test Loss: 0.0063, Test Acc: 0.9434\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 8/37 - Train Loss: 0.3267, Train Acc: 0.8938, Valid Loss: 0.2940, Valid Acc: 0.9139, Test Loss: 0.0075, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 8/37 - Train Loss: 0.3252, Train Acc: 0.8973, Valid Loss: 0.2865, Valid Acc: 0.9122, Test Loss: 0.0063, Test Acc: 0.9434\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 9/37 - Train Loss: 0.3252, Train Acc: 0.8919, Valid Loss: 0.2940, Valid Acc: 0.9139, Test Loss: 0.0075, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 9/37 - Train Loss: 0.3256, Train Acc: 0.8990, Valid Loss: 0.2865, Valid Acc: 0.9122, Test Loss: 0.0063, Test Acc: 0.9434\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 10/37 - Train Loss: 0.3285, Train Acc: 0.8940, Valid Loss: 0.2940, Valid Acc: 0.9139, Test Loss: 0.0075, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 10/37 - Train Loss: 0.3281, Train Acc: 0.8965, Valid Loss: 0.2865, Valid Acc: 0.9122, Test Loss: 0.0063, Test Acc: 0.9434\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 11/37 - Train Loss: 0.3232, Train Acc: 0.8906, Valid Loss: 0.2940, Valid Acc: 0.9139, Test Loss: 0.0075, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 11/37 - Train Loss: 0.3247, Train Acc: 0.8976, Valid Loss: 0.2865, Valid Acc: 0.9122, Test Loss: 0.0063, Test Acc: 0.9434\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 1/3 - Train Loss: 0.8295, Train Acc: 0.7519, Valid Loss: 0.3586, Valid Acc: 0.8742\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 2/3 - Train Loss: 0.2749, Train Acc: 0.9117, Valid Loss: 0.1795, Valid Acc: 0.9446\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/3 - Train Loss: 0.8184, Train Acc: 0.7525, Valid Loss: 0.3377, Valid Acc: 0.9010\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/3 - Train Loss: 0.1670, Train Acc: 0.9508, Valid Loss: 0.1064, Valid Acc: 0.9698\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: quantized::linear_dynamic: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/3 - Train Loss: 0.2663, Train Acc: 0.9184, Valid Loss: 0.1879, Valid Acc: 0.9581\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/3 - Train Loss: 0.1649, Train Acc: 0.9542, Valid Loss: 0.1098, Valid Acc: 0.9681\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: quantized::linear_dynamic: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 1/37 - Train Loss: 0.5506, Train Acc: 0.7995, Valid Loss: 0.4426, Valid Acc: 0.8415, Test Loss: 0.0085, Test Acc: 0.9223\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/37 - Train Loss: 0.3581, Train Acc: 0.8775, Valid Loss: 0.2604, Valid Acc: 0.9089, Test Loss: 0.0076, Test Acc: 0.9237\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 2/37 - Train Loss: 0.5415, Train Acc: 0.7981, Valid Loss: 0.4426, Valid Acc: 0.8415, Test Loss: 0.0085, Test Acc: 0.9223\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/37 - Train Loss: 0.3613, Train Acc: 0.8798, Valid Loss: 0.2604, Valid Acc: 0.9089, Test Loss: 0.0076, Test Acc: 0.9237\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/37 - Train Loss: 0.5399, Train Acc: 0.8045, Valid Loss: 0.4426, Valid Acc: 0.8415, Test Loss: 0.0085, Test Acc: 0.9223\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/37 - Train Loss: 0.3569, Train Acc: 0.8814, Valid Loss: 0.2604, Valid Acc: 0.9089, Test Loss: 0.0076, Test Acc: 0.9237\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 4/37 - Train Loss: 0.5498, Train Acc: 0.8028, Valid Loss: 0.4426, Valid Acc: 0.8415, Test Loss: 0.0085, Test Acc: 0.9223\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 4/37 - Train Loss: 0.3514, Train Acc: 0.8794, Valid Loss: 0.2604, Valid Acc: 0.9089, Test Loss: 0.0076, Test Acc: 0.9237\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 5/37 - Train Loss: 0.5451, Train Acc: 0.8045, Valid Loss: 0.4426, Valid Acc: 0.8415, Test Loss: 0.0085, Test Acc: 0.9223\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 5/37 - Train Loss: 0.3483, Train Acc: 0.8841, Valid Loss: 0.2604, Valid Acc: 0.9089, Test Loss: 0.0076, Test Acc: 0.9237\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 6/37 - Train Loss: 0.5394, Train Acc: 0.8088, Valid Loss: 0.4426, Valid Acc: 0.8415, Test Loss: 0.0085, Test Acc: 0.9223\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 6/37 - Train Loss: 0.3469, Train Acc: 0.8840, Valid Loss: 0.2604, Valid Acc: 0.9089, Test Loss: 0.0076, Test Acc: 0.9237\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 7/37 - Train Loss: 0.5414, Train Acc: 0.8057, Valid Loss: 0.4426, Valid Acc: 0.8415, Test Loss: 0.0085, Test Acc: 0.9223\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 7/37 - Train Loss: 0.3540, Train Acc: 0.8785, Valid Loss: 0.2604, Valid Acc: 0.9089, Test Loss: 0.0076, Test Acc: 0.9237\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 8/37 - Train Loss: 0.5297, Train Acc: 0.8081, Valid Loss: 0.4426, Valid Acc: 0.8415, Test Loss: 0.0085, Test Acc: 0.9223\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 8/37 - Train Loss: 0.3506, Train Acc: 0.8840, Valid Loss: 0.2604, Valid Acc: 0.9089, Test Loss: 0.0076, Test Acc: 0.9237\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 9/37 - Train Loss: 0.5414, Train Acc: 0.8067, Valid Loss: 0.4426, Valid Acc: 0.8415, Test Loss: 0.0085, Test Acc: 0.9223\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 9/37 - Train Loss: 0.3526, Train Acc: 0.8834, Valid Loss: 0.2604, Valid Acc: 0.9089, Test Loss: 0.0076, Test Acc: 0.9237\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 10/37 - Train Loss: 0.5382, Train Acc: 0.8059, Valid Loss: 0.4426, Valid Acc: 0.8415, Test Loss: 0.0085, Test Acc: 0.9223\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 10/37 - Train Loss: 0.3517, Train Acc: 0.8832, Valid Loss: 0.2604, Valid Acc: 0.9089, Test Loss: 0.0076, Test Acc: 0.9237\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 11/37 - Train Loss: 0.5408, Train Acc: 0.8032, Valid Loss: 0.4426, Valid Acc: 0.8415, Test Loss: 0.0085, Test Acc: 0.9223\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 11/37 - Train Loss: 0.3542, Train Acc: 0.8814, Valid Loss: 0.2604, Valid Acc: 0.9089, Test Loss: 0.0076, Test Acc: 0.9237\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 1/3 - Train Loss: 0.8156, Train Acc: 0.7528, Valid Loss: 0.3666, Valid Acc: 0.8926\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 2/3 - Train Loss: 0.2741, Train Acc: 0.9143, Valid Loss: 0.1803, Valid Acc: 0.9480\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/3 - Train Loss: 0.8516, Train Acc: 0.7355, Valid Loss: 0.2999, Valid Acc: 0.8993\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/3 - Train Loss: 0.1674, Train Acc: 0.9512, Valid Loss: 0.1192, Valid Acc: 0.9681\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: quantized::linear_dynamic: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/3 - Train Loss: 0.2730, Train Acc: 0.9104, Valid Loss: 0.1821, Valid Acc: 0.9262\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/3 - Train Loss: 0.1799, Train Acc: 0.9419, Valid Loss: 0.1374, Valid Acc: 0.9513\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: quantized::linear_dynamic: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 1/37 - Train Loss: 0.2446, Train Acc: 0.9344, Valid Loss: 0.1845, Valid Acc: 0.9548, Test Loss: 0.0077, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/37 - Train Loss: 0.6675, Train Acc: 0.7605, Valid Loss: 0.6034, Valid Acc: 0.7889, Test Loss: 0.0090, Test Acc: 0.9121\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 2/37 - Train Loss: 0.2345, Train Acc: 0.9408, Valid Loss: 0.1845, Valid Acc: 0.9548, Test Loss: 0.0077, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/37 - Train Loss: 0.6677, Train Acc: 0.7658, Valid Loss: 0.6034, Valid Acc: 0.7889, Test Loss: 0.0090, Test Acc: 0.9121\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/37 - Train Loss: 0.2460, Train Acc: 0.9334, Valid Loss: 0.1845, Valid Acc: 0.9548, Test Loss: 0.0077, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/37 - Train Loss: 0.6596, Train Acc: 0.7645, Valid Loss: 0.6034, Valid Acc: 0.7889, Test Loss: 0.0090, Test Acc: 0.9121\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 4/37 - Train Loss: 0.2382, Train Acc: 0.9400, Valid Loss: 0.1845, Valid Acc: 0.9548, Test Loss: 0.0077, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 4/37 - Train Loss: 0.6680, Train Acc: 0.7584, Valid Loss: 0.6034, Valid Acc: 0.7889, Test Loss: 0.0090, Test Acc: 0.9121\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 5/37 - Train Loss: 0.2406, Train Acc: 0.9391, Valid Loss: 0.1845, Valid Acc: 0.9548, Test Loss: 0.0077, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 5/37 - Train Loss: 0.6706, Train Acc: 0.7606, Valid Loss: 0.6034, Valid Acc: 0.7889, Test Loss: 0.0090, Test Acc: 0.9121\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 6/37 - Train Loss: 0.2385, Train Acc: 0.9361, Valid Loss: 0.1845, Valid Acc: 0.9548, Test Loss: 0.0077, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 6/37 - Train Loss: 0.6608, Train Acc: 0.7625, Valid Loss: 0.6034, Valid Acc: 0.7889, Test Loss: 0.0090, Test Acc: 0.9121\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 7/37 - Train Loss: 0.2367, Train Acc: 0.9396, Valid Loss: 0.1845, Valid Acc: 0.9548, Test Loss: 0.0077, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 7/37 - Train Loss: 0.6538, Train Acc: 0.7634, Valid Loss: 0.6034, Valid Acc: 0.7889, Test Loss: 0.0090, Test Acc: 0.9121\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 8/37 - Train Loss: 0.2412, Train Acc: 0.9400, Valid Loss: 0.1845, Valid Acc: 0.9548, Test Loss: 0.0077, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 8/37 - Train Loss: 0.6696, Train Acc: 0.7606, Valid Loss: 0.6034, Valid Acc: 0.7889, Test Loss: 0.0090, Test Acc: 0.9121\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 9/37 - Train Loss: 0.2442, Train Acc: 0.9367, Valid Loss: 0.1845, Valid Acc: 0.9548, Test Loss: 0.0077, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 9/37 - Train Loss: 0.6632, Train Acc: 0.7634, Valid Loss: 0.6034, Valid Acc: 0.7889, Test Loss: 0.0090, Test Acc: 0.9121\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 10/37 - Train Loss: 0.2328, Train Acc: 0.9381, Valid Loss: 0.1845, Valid Acc: 0.9548, Test Loss: 0.0077, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 10/37 - Train Loss: 0.6647, Train Acc: 0.7612, Valid Loss: 0.6034, Valid Acc: 0.7889, Test Loss: 0.0090, Test Acc: 0.9121\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 11/37 - Train Loss: 0.2337, Train Acc: 0.9400, Valid Loss: 0.1845, Valid Acc: 0.9548, Test Loss: 0.0077, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 10 results and 0 failures\n",
            "INFO:flwr:aggregate_fit: received 10 results and 0 failures\n",
            "\u001b[93mWARNING \u001b[0m:   No fit_metrics_aggregation_fn provided\n",
            "WARNING:flwr:No fit_metrics_aggregation_fn provided\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 10)\n",
            "INFO:flwr:configure_evaluate: strategy sampled 5 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 11/37 - Train Loss: 0.6673, Train Acc: 0.7619, Valid Loss: 0.6034, Valid Acc: 0.7889, Test Loss: 0.0090, Test Acc: 0.9121\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv1.weight'. Expected torch.Size([16, 1, 3, 3]), got torch.Size([16]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv1.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv1.bias'. Expected torch.Size([16]), got torch.Size([16, 1, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv1.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.weight'. Expected torch.Size([32, 16, 3, 3]), got torch.Size([32]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 1/3 - Train Loss: 0.9203, Train Acc: 0.7223, Valid Loss: 0.3593, Valid Acc: 0.8926\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 2/3 - Train Loss: 0.3057, Train Acc: 0.9048, Valid Loss: 0.2016, Valid Acc: 0.9430\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/3 - Train Loss: 0.2067, Train Acc: 0.9356, Valid Loss: 0.1446, Valid Acc: 0.9564\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.weight: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.bias: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.weight: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv1.weight'. Expected torch.Size([16, 1, 3, 3]), got torch.Size([16]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv1.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv1.bias'. Expected torch.Size([16]), got torch.Size([16, 1, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv1.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv2.weight'. Expected torch.Size([32, 16, 3, 3]), got torch.Size([32]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv2.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/3 - Train Loss: 0.1466, Train Acc: 0.9577, Valid Loss: 0.0981, Valid Acc: 0.9698\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/3 - Train Loss: 0.8338, Train Acc: 0.7435, Valid Loss: 0.3083, Valid Acc: 0.9195\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/3 - Train Loss: 0.2742, Train Acc: 0.9197, Valid Loss: 0.1571, Valid Acc: 0.9597\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/3 - Train Loss: 0.1742, Train Acc: 0.9490, Valid Loss: 0.1247, Valid Acc: 0.9581\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.weight: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.bias: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.weight: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv1.weight'. Expected torch.Size([16, 1, 3, 3]), got torch.Size([16]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv1.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv1.bias'. Expected torch.Size([16]), got torch.Size([16, 1, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv1.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv2.weight'. Expected torch.Size([32, 16, 3, 3]), got torch.Size([32]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv2.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/3 - Train Loss: 0.1774, Train Acc: 0.9469, Valid Loss: 0.1378, Valid Acc: 0.9564\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/3 - Train Loss: 0.9060, Train Acc: 0.7249, Valid Loss: 0.3936, Valid Acc: 0.8876\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/3 - Train Loss: 0.3071, Train Acc: 0.9076, Valid Loss: 0.2336, Valid Acc: 0.9262\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/3 - Train Loss: 0.1979, Train Acc: 0.9393, Valid Loss: 0.1548, Valid Acc: 0.9463\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
            "INFO:flwr:aggregate_evaluate: received 5 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "INFO:flwr:\n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
            "INFO:flwr:[ROUND 2]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n",
            "INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.weight: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.bias: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.weight: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv1.weight'. Expected torch.Size([16, 1, 3, 3]), got torch.Size([16]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv1.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv1.bias'. Expected torch.Size([16]), got torch.Size([16, 1, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv1.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv2.weight'. Expected torch.Size([32, 16, 3, 3]), got torch.Size([32]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv2.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/3 - Train Loss: 0.8288, Train Acc: 0.7603, Valid Loss: 0.3365, Valid Acc: 0.8842\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/3 - Train Loss: 0.2560, Train Acc: 0.9221, Valid Loss: 0.1589, Valid Acc: 0.9513\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/3 - Train Loss: 0.1595, Train Acc: 0.9510, Valid Loss: 0.1171, Valid Acc: 0.9614\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: quantized::linear_dynamic: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/37 - Train Loss: 0.3369, Train Acc: 0.8896, Valid Loss: 0.2425, Valid Acc: 0.9243, Test Loss: 0.0076, Test Acc: 0.9262\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/3 - Train Loss: 0.1420, Train Acc: 0.9542, Valid Loss: 0.1055, Valid Acc: 0.9732\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/37 - Train Loss: 0.3283, Train Acc: 0.8945, Valid Loss: 0.2425, Valid Acc: 0.9243, Test Loss: 0.0076, Test Acc: 0.9262\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/37 - Train Loss: 0.3387, Train Acc: 0.8888, Valid Loss: 0.2425, Valid Acc: 0.9243, Test Loss: 0.0076, Test Acc: 0.9262\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 4/37 - Train Loss: 0.3380, Train Acc: 0.8925, Valid Loss: 0.2425, Valid Acc: 0.9243, Test Loss: 0.0076, Test Acc: 0.9262\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 5/37 - Train Loss: 0.3343, Train Acc: 0.8943, Valid Loss: 0.2425, Valid Acc: 0.9243, Test Loss: 0.0076, Test Acc: 0.9262\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 6/37 - Train Loss: 0.3338, Train Acc: 0.8933, Valid Loss: 0.2425, Valid Acc: 0.9243, Test Loss: 0.0076, Test Acc: 0.9262\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 7/37 - Train Loss: 0.3415, Train Acc: 0.8873, Valid Loss: 0.2425, Valid Acc: 0.9243, Test Loss: 0.0076, Test Acc: 0.9262\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 8/37 - Train Loss: 0.3426, Train Acc: 0.8916, Valid Loss: 0.2425, Valid Acc: 0.9243, Test Loss: 0.0076, Test Acc: 0.9262\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 9/37 - Train Loss: 0.3279, Train Acc: 0.8953, Valid Loss: 0.2425, Valid Acc: 0.9243, Test Loss: 0.0076, Test Acc: 0.9262\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 10/37 - Train Loss: 0.3322, Train Acc: 0.8935, Valid Loss: 0.2425, Valid Acc: 0.9243, Test Loss: 0.0076, Test Acc: 0.9262\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 11/37 - Train Loss: 0.3307, Train Acc: 0.8933, Valid Loss: 0.2425, Valid Acc: 0.9243, Test Loss: 0.0076, Test Acc: 0.9262\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.weight: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.bias: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.weight: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv1.weight'. Expected torch.Size([16, 1, 3, 3]), got torch.Size([16]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv1.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv1.bias'. Expected torch.Size([16]), got torch.Size([16, 1, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv1.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv2.weight'. Expected torch.Size([32, 16, 3, 3]), got torch.Size([32]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv2.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 11/37 - Train Loss: 0.1274, Train Acc: 0.9603, Valid Loss: 0.0949, Valid Acc: 0.9648, Test Loss: 0.0067, Test Acc: 0.9340\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/3 - Train Loss: 0.8962, Train Acc: 0.7342, Valid Loss: 0.3318, Valid Acc: 0.8909\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/3 - Train Loss: 0.2736, Train Acc: 0.9160, Valid Loss: 0.1651, Valid Acc: 0.9547\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/3 - Train Loss: 0.1725, Train Acc: 0.9478, Valid Loss: 0.1074, Valid Acc: 0.9631\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: quantized::linear_dynamic: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/37 - Train Loss: 0.3279, Train Acc: 0.9016, Valid Loss: 0.2327, Valid Acc: 0.9211, Test Loss: 0.0082, Test Acc: 0.9209\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/3 - Train Loss: 0.1684, Train Acc: 0.9495, Valid Loss: 0.1138, Valid Acc: 0.9631\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/37 - Train Loss: 0.3280, Train Acc: 0.8999, Valid Loss: 0.2327, Valid Acc: 0.9211, Test Loss: 0.0082, Test Acc: 0.9209\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/37 - Train Loss: 0.3282, Train Acc: 0.9012, Valid Loss: 0.2327, Valid Acc: 0.9211, Test Loss: 0.0082, Test Acc: 0.9209\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 4/37 - Train Loss: 0.3209, Train Acc: 0.9021, Valid Loss: 0.2327, Valid Acc: 0.9211, Test Loss: 0.0082, Test Acc: 0.9209\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 5/37 - Train Loss: 0.3170, Train Acc: 0.9045, Valid Loss: 0.2327, Valid Acc: 0.9211, Test Loss: 0.0082, Test Acc: 0.9209\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 6/37 - Train Loss: 0.3257, Train Acc: 0.9034, Valid Loss: 0.2327, Valid Acc: 0.9211, Test Loss: 0.0082, Test Acc: 0.9209\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 7/37 - Train Loss: 0.3138, Train Acc: 0.9087, Valid Loss: 0.2327, Valid Acc: 0.9211, Test Loss: 0.0082, Test Acc: 0.9209\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 8/37 - Train Loss: 0.3319, Train Acc: 0.9014, Valid Loss: 0.2327, Valid Acc: 0.9211, Test Loss: 0.0082, Test Acc: 0.9209\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 9/37 - Train Loss: 0.3230, Train Acc: 0.9018, Valid Loss: 0.2327, Valid Acc: 0.9211, Test Loss: 0.0082, Test Acc: 0.9209\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 10/37 - Train Loss: 0.3220, Train Acc: 0.9034, Valid Loss: 0.2327, Valid Acc: 0.9211, Test Loss: 0.0082, Test Acc: 0.9209\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 11/37 - Train Loss: 0.2783, Train Acc: 0.9210, Valid Loss: 0.1955, Valid Acc: 0.9426, Test Loss: 0.0083, Test Acc: 0.9255\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.weight: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.bias: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.weight: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv1.weight'. Expected torch.Size([16, 1, 3, 3]), got torch.Size([16]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv1.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv1.bias'. Expected torch.Size([16]), got torch.Size([16, 1, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv1.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv2.weight'. Expected torch.Size([32, 16, 3, 3]), got torch.Size([32]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv2.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 11/37 - Train Loss: 0.3242, Train Acc: 0.8988, Valid Loss: 0.2327, Valid Acc: 0.9211, Test Loss: 0.0082, Test Acc: 0.9209\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/3 - Train Loss: 0.8286, Train Acc: 0.7554, Valid Loss: 0.3204, Valid Acc: 0.8993\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/3 - Train Loss: 0.2712, Train Acc: 0.9194, Valid Loss: 0.1745, Valid Acc: 0.9497\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/3 - Train Loss: 0.1737, Train Acc: 0.9503, Valid Loss: 0.0992, Valid Acc: 0.9597\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: quantized::linear_dynamic: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/37 - Train Loss: 0.2392, Train Acc: 0.9370, Valid Loss: 0.2090, Valid Acc: 0.9252, Test Loss: 0.0075, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/3 - Train Loss: 0.1785, Train Acc: 0.9458, Valid Loss: 0.1444, Valid Acc: 0.9497\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 2/37 - Train Loss: 0.5752, Train Acc: 0.7906, Valid Loss: 0.5753, Valid Acc: 0.8094, Test Loss: 0.0098, Test Acc: 0.9003\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/37 - Train Loss: 0.5814, Train Acc: 0.7800, Valid Loss: 0.5753, Valid Acc: 0.8094, Test Loss: 0.0098, Test Acc: 0.9003\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 4/37 - Train Loss: 0.5848, Train Acc: 0.7878, Valid Loss: 0.5753, Valid Acc: 0.8094, Test Loss: 0.0098, Test Acc: 0.9003\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 5/37 - Train Loss: 0.5813, Train Acc: 0.7861, Valid Loss: 0.5753, Valid Acc: 0.8094, Test Loss: 0.0098, Test Acc: 0.9003\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 6/37 - Train Loss: 0.5796, Train Acc: 0.7826, Valid Loss: 0.5753, Valid Acc: 0.8094, Test Loss: 0.0098, Test Acc: 0.9003\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 7/37 - Train Loss: 0.5809, Train Acc: 0.7835, Valid Loss: 0.5753, Valid Acc: 0.8094, Test Loss: 0.0098, Test Acc: 0.9003\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 8/37 - Train Loss: 0.5809, Train Acc: 0.7817, Valid Loss: 0.5753, Valid Acc: 0.8094, Test Loss: 0.0098, Test Acc: 0.9003\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 9/37 - Train Loss: 0.5760, Train Acc: 0.7886, Valid Loss: 0.5753, Valid Acc: 0.8094, Test Loss: 0.0098, Test Acc: 0.9003\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 10/37 - Train Loss: 0.5786, Train Acc: 0.7796, Valid Loss: 0.5753, Valid Acc: 0.8094, Test Loss: 0.0098, Test Acc: 0.9003\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 11/37 - Train Loss: 0.5873, Train Acc: 0.7811, Valid Loss: 0.5753, Valid Acc: 0.8094, Test Loss: 0.0098, Test Acc: 0.9003\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv1.weight'. Expected torch.Size([16, 1, 3, 3]), got torch.Size([16]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv1.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv1.bias'. Expected torch.Size([16]), got torch.Size([16, 1, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv1.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.weight'. Expected torch.Size([32, 16, 3, 3]), got torch.Size([32]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 11/37 - Train Loss: 0.2520, Train Acc: 0.9349, Valid Loss: 0.2090, Valid Acc: 0.9252, Test Loss: 0.0075, Test Acc: 0.9308\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 1/3 - Train Loss: 0.8407, Train Acc: 0.7469, Valid Loss: 0.2957, Valid Acc: 0.9195\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 2/3 - Train Loss: 0.2614, Train Acc: 0.9192, Valid Loss: 0.1624, Valid Acc: 0.9530\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/3 - Train Loss: 0.1622, Train Acc: 0.9495, Valid Loss: 0.1064, Valid Acc: 0.9631\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: quantized::linear_dynamic: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 1/37 - Train Loss: 0.3013, Train Acc: 0.9091, Valid Loss: 0.2853, Valid Acc: 0.9138, Test Loss: 0.0066, Test Acc: 0.9426\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/3 - Train Loss: 0.2164, Train Acc: 0.9324, Valid Loss: 0.1393, Valid Acc: 0.9581\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 2/37 - Train Loss: 0.3041, Train Acc: 0.9038, Valid Loss: 0.2853, Valid Acc: 0.9138, Test Loss: 0.0066, Test Acc: 0.9426\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/37 - Train Loss: 0.3053, Train Acc: 0.9091, Valid Loss: 0.2853, Valid Acc: 0.9138, Test Loss: 0.0066, Test Acc: 0.9426\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 4/37 - Train Loss: 0.2998, Train Acc: 0.9053, Valid Loss: 0.2853, Valid Acc: 0.9138, Test Loss: 0.0066, Test Acc: 0.9426\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 5/37 - Train Loss: 0.3021, Train Acc: 0.9040, Valid Loss: 0.2853, Valid Acc: 0.9138, Test Loss: 0.0066, Test Acc: 0.9426\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 6/37 - Train Loss: 0.3156, Train Acc: 0.9091, Valid Loss: 0.2853, Valid Acc: 0.9138, Test Loss: 0.0066, Test Acc: 0.9426\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 7/37 - Train Loss: 0.3010, Train Acc: 0.9085, Valid Loss: 0.2853, Valid Acc: 0.9138, Test Loss: 0.0066, Test Acc: 0.9426\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 8/37 - Train Loss: 0.3046, Train Acc: 0.9067, Valid Loss: 0.2853, Valid Acc: 0.9138, Test Loss: 0.0066, Test Acc: 0.9426\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 9/37 - Train Loss: 0.3141, Train Acc: 0.9071, Valid Loss: 0.2853, Valid Acc: 0.9138, Test Loss: 0.0066, Test Acc: 0.9426\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 10/37 - Train Loss: 0.3072, Train Acc: 0.9067, Valid Loss: 0.2853, Valid Acc: 0.9138, Test Loss: 0.0066, Test Acc: 0.9426\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 11/37 - Train Loss: 0.3031, Train Acc: 0.9084, Valid Loss: 0.2853, Valid Acc: 0.9138, Test Loss: 0.0066, Test Acc: 0.9426\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv1.weight'. Expected torch.Size([16, 1, 3, 3]), got torch.Size([16]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv1.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv1.bias'. Expected torch.Size([16]), got torch.Size([16, 1, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv1.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.weight'. Expected torch.Size([32, 16, 3, 3]), got torch.Size([32]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 11/37 - Train Loss: 0.5987, Train Acc: 0.7964, Valid Loss: 0.5726, Valid Acc: 0.8174, Test Loss: 0.0093, Test Acc: 0.9108\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 1/3 - Train Loss: 0.9489, Train Acc: 0.7107, Valid Loss: 0.3351, Valid Acc: 0.8943\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 2/3 - Train Loss: 0.2888, Train Acc: 0.9126, Valid Loss: 0.1793, Valid Acc: 0.9396\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/3 - Train Loss: 0.1812, Train Acc: 0.9423, Valid Loss: 0.1240, Valid Acc: 0.9698\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: quantized::linear_dynamic: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 1/37 - Train Loss: 0.5906, Train Acc: 0.8099, Valid Loss: 0.4683, Valid Acc: 0.8429, Test Loss: 0.0084, Test Acc: 0.9248\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/3 - Train Loss: 0.2045, Train Acc: 0.9382, Valid Loss: 0.1362, Valid Acc: 0.9497\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 2/37 - Train Loss: 0.5872, Train Acc: 0.8039, Valid Loss: 0.4683, Valid Acc: 0.8429, Test Loss: 0.0084, Test Acc: 0.9248\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/37 - Train Loss: 0.5924, Train Acc: 0.8093, Valid Loss: 0.4683, Valid Acc: 0.8429, Test Loss: 0.0084, Test Acc: 0.9248\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 4/37 - Train Loss: 0.5964, Train Acc: 0.8033, Valid Loss: 0.4683, Valid Acc: 0.8429, Test Loss: 0.0084, Test Acc: 0.9248\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 5/37 - Train Loss: 0.5889, Train Acc: 0.8116, Valid Loss: 0.4683, Valid Acc: 0.8429, Test Loss: 0.0084, Test Acc: 0.9248\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 6/37 - Train Loss: 0.5882, Train Acc: 0.8097, Valid Loss: 0.4683, Valid Acc: 0.8429, Test Loss: 0.0084, Test Acc: 0.9248\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 7/37 - Train Loss: 0.5927, Train Acc: 0.8117, Valid Loss: 0.4683, Valid Acc: 0.8429, Test Loss: 0.0084, Test Acc: 0.9248\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 8/37 - Train Loss: 0.5922, Train Acc: 0.8087, Valid Loss: 0.4683, Valid Acc: 0.8429, Test Loss: 0.0084, Test Acc: 0.9248\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 8/37 - Train Loss: 0.4954, Train Acc: 0.8532, Valid Loss: 0.4436, Valid Acc: 0.8796, Test Loss: 0.0100, Test Acc: 0.9124\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 9/37 - Train Loss: 0.5911, Train Acc: 0.8069, Valid Loss: 0.4683, Valid Acc: 0.8429, Test Loss: 0.0084, Test Acc: 0.9248\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 9/37 - Train Loss: 0.4955, Train Acc: 0.8495, Valid Loss: 0.4436, Valid Acc: 0.8796, Test Loss: 0.0100, Test Acc: 0.9124\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 10/37 - Train Loss: 0.5896, Train Acc: 0.8054, Valid Loss: 0.4683, Valid Acc: 0.8429, Test Loss: 0.0084, Test Acc: 0.9248\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 10/37 - Train Loss: 0.5004, Train Acc: 0.8505, Valid Loss: 0.4436, Valid Acc: 0.8796, Test Loss: 0.0100, Test Acc: 0.9124\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 11/37 - Train Loss: 0.5917, Train Acc: 0.8099, Valid Loss: 0.4683, Valid Acc: 0.8429, Test Loss: 0.0084, Test Acc: 0.9248\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 10 results and 0 failures\n",
            "INFO:flwr:aggregate_fit: received 10 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 10)\n",
            "INFO:flwr:configure_evaluate: strategy sampled 5 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv1.weight'. Expected torch.Size([16, 1, 3, 3]), got torch.Size([16]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv1.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv1.bias'. Expected torch.Size([16]), got torch.Size([16, 1, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv1.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.weight'. Expected torch.Size([32, 16, 3, 3]), got torch.Size([32]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 11/37 - Train Loss: 0.4899, Train Acc: 0.8557, Valid Loss: 0.4436, Valid Acc: 0.8796, Test Loss: 0.0100, Test Acc: 0.9124\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 1/3 - Train Loss: 0.8638, Train Acc: 0.7409, Valid Loss: 0.3287, Valid Acc: 0.8876\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 2/3 - Train Loss: 0.2607, Train Acc: 0.9186, Valid Loss: 0.1627, Valid Acc: 0.9614\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/3 - Train Loss: 0.1484, Train Acc: 0.9572, Valid Loss: 0.1321, Valid Acc: 0.9530\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv1.weight'. Expected torch.Size([16, 1, 3, 3]), got torch.Size([16]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv1.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv1.bias'. Expected torch.Size([16]), got torch.Size([16, 1, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv1.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.weight'. Expected torch.Size([32, 16, 3, 3]), got torch.Size([32]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/3 - Train Loss: 0.1489, Train Acc: 0.9544, Valid Loss: 0.1001, Valid Acc: 0.9664\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 1/3 - Train Loss: 0.8821, Train Acc: 0.7379, Valid Loss: 0.3760, Valid Acc: 0.8876\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 2/3 - Train Loss: 0.3210, Train Acc: 0.8951, Valid Loss: 0.2127, Valid Acc: 0.9346\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/3 - Train Loss: 0.2075, Train Acc: 0.9376, Valid Loss: 0.1497, Valid Acc: 0.9530\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv1.weight'. Expected torch.Size([16, 1, 3, 3]), got torch.Size([16]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv1.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv1.bias'. Expected torch.Size([16]), got torch.Size([16, 1, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv1.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.weight'. Expected torch.Size([32, 16, 3, 3]), got torch.Size([32]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/3 - Train Loss: 0.1499, Train Acc: 0.9521, Valid Loss: 0.1282, Valid Acc: 0.9597\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 1/3 - Train Loss: 0.8009, Train Acc: 0.7551, Valid Loss: 0.2815, Valid Acc: 0.9161\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 2/3 - Train Loss: 0.2512, Train Acc: 0.9233, Valid Loss: 0.1586, Valid Acc: 0.9513\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/3 - Train Loss: 0.1568, Train Acc: 0.9529, Valid Loss: 0.0980, Valid Acc: 0.9614\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
            "INFO:flwr:aggregate_evaluate: received 5 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "INFO:flwr:\n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
            "INFO:flwr:[ROUND 3]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n",
            "INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv1.weight'. Expected torch.Size([16, 1, 3, 3]), got torch.Size([16]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv1.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv1.bias'. Expected torch.Size([16]), got torch.Size([16, 1, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv1.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.weight'. Expected torch.Size([32, 16, 3, 3]), got torch.Size([32]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 1/3 - Train Loss: 0.8624, Train Acc: 0.7458, Valid Loss: 0.3506, Valid Acc: 0.8893\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 2/3 - Train Loss: 0.2655, Train Acc: 0.9167, Valid Loss: 0.1686, Valid Acc: 0.9547\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/3 - Train Loss: 0.1673, Train Acc: 0.9514, Valid Loss: 0.1111, Valid Acc: 0.9648\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: quantized::linear_dynamic: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 1/37 - Train Loss: 0.2383, Train Acc: 0.9317, Valid Loss: 0.1569, Valid Acc: 0.9555, Test Loss: 0.0069, Test Acc: 0.9358\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/3 - Train Loss: 0.2100, Train Acc: 0.9361, Valid Loss: 0.1521, Valid Acc: 0.9597\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 2/37 - Train Loss: 0.2350, Train Acc: 0.9307, Valid Loss: 0.1569, Valid Acc: 0.9555, Test Loss: 0.0069, Test Acc: 0.9358\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/37 - Train Loss: 0.1877, Train Acc: 0.9445, Valid Loss: 0.1431, Valid Acc: 0.9597, Test Loss: 0.0103, Test Acc: 0.9076\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 4/37 - Train Loss: 0.1895, Train Acc: 0.9438, Valid Loss: 0.1431, Valid Acc: 0.9597, Test Loss: 0.0103, Test Acc: 0.9076\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 5/37 - Train Loss: 0.1804, Train Acc: 0.9458, Valid Loss: 0.1431, Valid Acc: 0.9597, Test Loss: 0.0103, Test Acc: 0.9076\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 6/37 - Train Loss: 0.1836, Train Acc: 0.9462, Valid Loss: 0.1431, Valid Acc: 0.9597, Test Loss: 0.0103, Test Acc: 0.9076\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 7/37 - Train Loss: 0.1898, Train Acc: 0.9425, Valid Loss: 0.1431, Valid Acc: 0.9597, Test Loss: 0.0103, Test Acc: 0.9076\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 8/37 - Train Loss: 0.1864, Train Acc: 0.9440, Valid Loss: 0.1431, Valid Acc: 0.9597, Test Loss: 0.0103, Test Acc: 0.9076\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 9/37 - Train Loss: 0.1867, Train Acc: 0.9440, Valid Loss: 0.1431, Valid Acc: 0.9597, Test Loss: 0.0103, Test Acc: 0.9076\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 10/37 - Train Loss: 0.1880, Train Acc: 0.9427, Valid Loss: 0.1431, Valid Acc: 0.9597, Test Loss: 0.0103, Test Acc: 0.9076\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 11/37 - Train Loss: 0.1889, Train Acc: 0.9417, Valid Loss: 0.1431, Valid Acc: 0.9597, Test Loss: 0.0103, Test Acc: 0.9076\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.weight: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.bias: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.weight: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv1.weight'. Expected torch.Size([16, 1, 3, 3]), got torch.Size([16]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv1.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv1.bias'. Expected torch.Size([16]), got torch.Size([16, 1, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv1.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv2.weight'. Expected torch.Size([32, 16, 3, 3]), got torch.Size([32]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv2.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 11/37 - Train Loss: 0.2307, Train Acc: 0.9307, Valid Loss: 0.1569, Valid Acc: 0.9555, Test Loss: 0.0069, Test Acc: 0.9358\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/3 - Train Loss: 0.8904, Train Acc: 0.7277, Valid Loss: 0.3191, Valid Acc: 0.8893\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/3 - Train Loss: 0.3019, Train Acc: 0.9015, Valid Loss: 0.1963, Valid Acc: 0.9346\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/3 - Train Loss: 0.1973, Train Acc: 0.9389, Valid Loss: 0.1430, Valid Acc: 0.9530\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: quantized::linear_dynamic: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/37 - Train Loss: 0.5155, Train Acc: 0.8271, Valid Loss: 0.4428, Valid Acc: 0.8615, Test Loss: 0.0090, Test Acc: 0.9125\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/3 - Train Loss: 0.2104, Train Acc: 0.9367, Valid Loss: 0.1471, Valid Acc: 0.9648\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/37 - Train Loss: 0.5048, Train Acc: 0.8320, Valid Loss: 0.4428, Valid Acc: 0.8615, Test Loss: 0.0090, Test Acc: 0.9125\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/37 - Train Loss: 0.5187, Train Acc: 0.8391, Valid Loss: 0.4428, Valid Acc: 0.8615, Test Loss: 0.0090, Test Acc: 0.9125\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 4/37 - Train Loss: 0.5098, Train Acc: 0.8359, Valid Loss: 0.4428, Valid Acc: 0.8615, Test Loss: 0.0090, Test Acc: 0.9125\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 5/37 - Train Loss: 0.5173, Train Acc: 0.8308, Valid Loss: 0.4428, Valid Acc: 0.8615, Test Loss: 0.0090, Test Acc: 0.9125\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 6/37 - Train Loss: 0.5069, Train Acc: 0.8338, Valid Loss: 0.4428, Valid Acc: 0.8615, Test Loss: 0.0090, Test Acc: 0.9125\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 7/37 - Train Loss: 0.5070, Train Acc: 0.8288, Valid Loss: 0.4428, Valid Acc: 0.8615, Test Loss: 0.0090, Test Acc: 0.9125\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 8/37 - Train Loss: 0.5098, Train Acc: 0.8303, Valid Loss: 0.4428, Valid Acc: 0.8615, Test Loss: 0.0090, Test Acc: 0.9125\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 9/37 - Train Loss: 0.5108, Train Acc: 0.8314, Valid Loss: 0.4428, Valid Acc: 0.8615, Test Loss: 0.0090, Test Acc: 0.9125\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 10/37 - Train Loss: 0.5094, Train Acc: 0.8321, Valid Loss: 0.4428, Valid Acc: 0.8615, Test Loss: 0.0090, Test Acc: 0.9125\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 11/37 - Train Loss: 0.5100, Train Acc: 0.8273, Valid Loss: 0.4428, Valid Acc: 0.8615, Test Loss: 0.0090, Test Acc: 0.9125\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.weight: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv1.bias: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.weight: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv1.weight'. Expected torch.Size([16, 1, 3, 3]), got torch.Size([16]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv1.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv1.bias'. Expected torch.Size([16]), got torch.Size([16, 1, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv1.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv2.weight'. Expected torch.Size([32, 16, 3, 3]), got torch.Size([32]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv2.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 11/37 - Train Loss: 0.3768, Train Acc: 0.8901, Valid Loss: 0.3442, Valid Acc: 0.9003, Test Loss: 0.0089, Test Acc: 0.9222\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/3 - Train Loss: 0.7517, Train Acc: 0.7756, Valid Loss: 0.2748, Valid Acc: 0.9178\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/3 - Train Loss: 0.2227, Train Acc: 0.9365, Valid Loss: 0.1524, Valid Acc: 0.9547\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/3 - Train Loss: 0.1367, Train Acc: 0.9587, Valid Loss: 0.0882, Valid Acc: 0.9715\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: quantized::linear_dynamic: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/37 - Train Loss: 0.2998, Train Acc: 0.9141, Valid Loss: 0.2720, Valid Acc: 0.9263, Test Loss: 0.0065, Test Acc: 0.9411\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/3 - Train Loss: 0.1947, Train Acc: 0.9402, Valid Loss: 0.1303, Valid Acc: 0.9698\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/37 - Train Loss: 0.2946, Train Acc: 0.9158, Valid Loss: 0.2720, Valid Acc: 0.9263, Test Loss: 0.0065, Test Acc: 0.9411\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/37 - Train Loss: 0.2897, Train Acc: 0.9149, Valid Loss: 0.2720, Valid Acc: 0.9263, Test Loss: 0.0065, Test Acc: 0.9411\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 4/37 - Train Loss: 0.2950, Train Acc: 0.9164, Valid Loss: 0.2720, Valid Acc: 0.9263, Test Loss: 0.0065, Test Acc: 0.9411\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 5/37 - Train Loss: 0.2896, Train Acc: 0.9156, Valid Loss: 0.2720, Valid Acc: 0.9263, Test Loss: 0.0065, Test Acc: 0.9411\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 6/37 - Train Loss: 0.2949, Train Acc: 0.9149, Valid Loss: 0.2720, Valid Acc: 0.9263, Test Loss: 0.0065, Test Acc: 0.9411\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 7/37 - Train Loss: 0.3029, Train Acc: 0.9151, Valid Loss: 0.2720, Valid Acc: 0.9263, Test Loss: 0.0065, Test Acc: 0.9411\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 8/37 - Train Loss: 0.2993, Train Acc: 0.9143, Valid Loss: 0.2720, Valid Acc: 0.9263, Test Loss: 0.0065, Test Acc: 0.9411\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 9/37 - Train Loss: 0.2884, Train Acc: 0.9210, Valid Loss: 0.2720, Valid Acc: 0.9263, Test Loss: 0.0065, Test Acc: 0.9411\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 10/37 - Train Loss: 0.3089, Train Acc: 0.9166, Valid Loss: 0.2720, Valid Acc: 0.9263, Test Loss: 0.0065, Test Acc: 0.9411\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 11/37 - Train Loss: 0.2998, Train Acc: 0.9162, Valid Loss: 0.2720, Valid Acc: 0.9263, Test Loss: 0.0065, Test Acc: 0.9411\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m fc2.bias: torch.Size([10])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.weight: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv1.bias: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.weight: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv1.weight'. Expected torch.Size([16, 1, 3, 3]), got torch.Size([16]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv1.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv1.bias'. Expected torch.Size([16]), got torch.Size([16, 1, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv1.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.weight'. Expected torch.Size([32, 16, 3, 3]), got torch.Size([32]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.weight' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 11/37 - Train Loss: 0.3060, Train Acc: 0.9077, Valid Loss: 0.2706, Valid Acc: 0.9281, Test Loss: 0.0077, Test Acc: 0.9298\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 1/3 - Train Loss: 0.9100, Train Acc: 0.7247, Valid Loss: 0.3439, Valid Acc: 0.8960\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Expected parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m conv2.bias: torch.Size([32, 16, 3, 3])\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Received parameter shapes:\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch occurred. Attempting to handle gracefully...\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch for parameter 'conv2.bias'. Expected torch.Size([32]), got torch.Size([32, 16, 3, 3]).\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Cannot reshape parameter 'conv2.bias' due to incompatible number of elements.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Shape mismatch handled.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/3 - Train Loss: 0.2605, Train Acc: 0.9171, Valid Loss: 0.1665, Valid Acc: 0.9513\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16053)\u001b[0m Epoch 3/3 - Train Loss: 0.1953, Train Acc: 0.9408, Valid Loss: 0.1291, Valid Acc: 0.9513\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: quantized::linear_dynamic: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 1/37 - Train Loss: 0.2792, Train Acc: 0.9197, Valid Loss: 0.2004, Valid Acc: 0.9396, Test Loss: 0.0081, Test Acc: 0.9231\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/3 - Train Loss: 0.1702, Train Acc: 0.9490, Valid Loss: 0.1200, Valid Acc: 0.9614\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 2/37 - Train Loss: 0.2751, Train Acc: 0.9189, Valid Loss: 0.2004, Valid Acc: 0.9396, Test Loss: 0.0081, Test Acc: 0.9231\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 3/37 - Train Loss: 0.2711, Train Acc: 0.9204, Valid Loss: 0.2004, Valid Acc: 0.9396, Test Loss: 0.0081, Test Acc: 0.9231\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 4/37 - Train Loss: 0.2678, Train Acc: 0.9200, Valid Loss: 0.2004, Valid Acc: 0.9396, Test Loss: 0.0081, Test Acc: 0.9231\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 5/37 - Train Loss: 0.2741, Train Acc: 0.9165, Valid Loss: 0.2004, Valid Acc: 0.9396, Test Loss: 0.0081, Test Acc: 0.9231\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 6/37 - Train Loss: 0.2683, Train Acc: 0.9223, Valid Loss: 0.2004, Valid Acc: 0.9396, Test Loss: 0.0081, Test Acc: 0.9231\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 7/37 - Train Loss: 0.2707, Train Acc: 0.9185, Valid Loss: 0.2004, Valid Acc: 0.9396, Test Loss: 0.0081, Test Acc: 0.9231\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=16051)\u001b[0m Epoch 8/37 - Train Loss: 0.2736, Train Acc: 0.9208, Valid Loss: 0.2004, Valid Acc: 0.9396, Test Loss: 0.0081, Test Acc: 0.9231\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Load the MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "mnist_dataset = MNIST(\"./dataset\", train=True, download=True, transform=transform)\n",
        "\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%)\n",
        "train_size = int(0.8 * len(mnist_dataset))\n",
        "test_size = len(mnist_dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(mnist_dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "\n",
        "def client_fn(cid, pca, train_removed_percentage, test_removed_percentage, testloader, n_components):\n",
        "    model = Net().to(DEVICE)\n",
        "    trainloaders, valloaders, testloader, datasets, testset, train_removed_percentage, test_removed_percentage = load_datasets(pca, n_components)\n",
        "    pruning_amount, quantization_type = determine_compression_parameters(client_data_variabilities)\n",
        "    client = FlowerClient(model, trainloaders[int(cid)], valloaders[int(cid)], testloader, pca, train_removed_percentage, test_removed_percentage, best_early_stopping_epoch, n_components, available_bandwidth=1000, pruning_amount=pruning_amount, quantization_type=quantization_type)\n",
        "    return client.to_client()\n",
        "\n",
        "# Characterize client data variability\n",
        "client_data_variabilities = []\n",
        "for i in range(NUM_CLIENTS):\n",
        "    data_variability = characterize_client_data(datasets[i].dataset.data.numpy())\n",
        "    client_data_variabilities.append(data_variability)\n",
        "\n",
        "# Determine PCA components dynamically based on client data variability\n",
        "pca, n_components, avg_variability = determine_pca_components(mnist_dataset.data.numpy(), client_data_variabilities)\n",
        "\n",
        "# Load the datasets using the PCA object\n",
        "trainloaders, valloaders, testloader, datasets, testset, _, _ = load_datasets(pca, n_components)\n",
        "\n",
        "# Determine compression parameters based on client data variability\n",
        "pruning_amount, quantization_type = determine_compression_parameters(client_data_variabilities)\n",
        "\n",
        "\n",
        "pca_components_range = [8, 16, 32, 64, 128]\n",
        "best_accuracy = 0\n",
        "best_pca_components = None\n",
        "best_test_loss = float('inf')\n",
        "best_early_stopping_epoch = 0\n",
        "\n",
        "for n_components in pca_components_range:\n",
        "    print(f\"Training with {n_components} PCA components...\")\n",
        "    pca, n_components, avg_variability = determine_pca_components(mnist_dataset.data.numpy(), client_data_variabilities, n_components_range=(n_components, n_components))\n",
        "    print(f\"Average variability: {avg_variability:.4f}\")\n",
        "    print(f\"Determined number of PCA components: {n_components}\")\n",
        "\n",
        "    trainloaders, valloaders, testloader, datasets, testset, _, _ = load_datasets(pca, n_components)\n",
        "\n",
        "    # Determine compression parameters based on client data variability\n",
        "    pruning_amount, quantization_type = determine_compression_parameters(client_data_variabilities)\n",
        "\n",
        "\n",
        "    train_removed_percentages = []\n",
        "    test_removed_percentages = []\n",
        "    for i in range(NUM_CLIENTS):\n",
        "        _, train_removed_percentage = apply_pca_locally(datasets[i].dataset.data.numpy(), pca, n_components)\n",
        "        _, test_removed_percentage = apply_pca_locally(testset.data.numpy(), pca, n_components)\n",
        "        train_removed_percentages.append(train_removed_percentage)\n",
        "        test_removed_percentages.append(test_removed_percentage)\n",
        "\n",
        "    trainloader = trainloaders[0]\n",
        "    valloader = valloaders[0]\n",
        "\n",
        "    test_accuracy, test_loss, early_stopping_epoch = train_and_evaluate(pca, n_components, trainloader, valloader, testloader)\n",
        "\n",
        "    if test_accuracy > best_accuracy:\n",
        "        best_accuracy = test_accuracy\n",
        "        best_pca_components = n_components\n",
        "        best_test_loss = test_loss\n",
        "        best_early_stopping_epoch = early_stopping_epoch\n",
        "\n",
        "\n",
        "print(f\"Best PCA Components: {best_pca_components}\")\n",
        "print(f\"Best Test Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"Best Test Loss: {best_test_loss:.4f}\")\n",
        "print(f\"Best Early Stopping Epoch: {best_early_stopping_epoch}\")\n",
        "print(f\"Quantization Type: {quantization_type}\")\n",
        "print(f\"Pruning Amount: {pruning_amount}\")\n",
        "\n",
        "# Pass the PCA object, data removed percentages, testloader, number of components, and compression parameters to the client_fn\n",
        "def client_fn_with_pca(cid):\n",
        "    return client_fn(cid, pca, train_removed_percentages[int(cid)], test_removed_percentages[int(cid)], testloader, best_pca_components)\n",
        "\n",
        "strategy = fl.server.strategy.FedAvg(\n",
        "    fraction_fit=1.0,\n",
        "    fraction_evaluate=0.5,\n",
        "    min_fit_clients=10,\n",
        "    min_evaluate_clients=5,\n",
        "    min_available_clients=10,\n",
        "    evaluate_metrics_aggregation_fn=weighted_average,\n",
        ")\n",
        "\n",
        "\n",
        "# ... (previous code remains the same)\n",
        "\n",
        "metrics_history = {}  # Dictionary to store metrics for each epoch\n",
        "\n",
        "try:\n",
        "    history = fl.simulation.start_simulation(\n",
        "        client_fn=client_fn_with_pca,\n",
        "        num_clients=NUM_CLIENTS,\n",
        "        config=fl.server.ServerConfig(num_rounds=best_early_stopping_epoch),\n",
        "        strategy=strategy,\n",
        "    )\n",
        "\n",
        "    if history is not None and history.metrics_distributed:\n",
        "        for epoch, epoch_metrics in enumerate(history.metrics_distributed, start=1):\n",
        "            if epoch_metrics:\n",
        "                epoch_accuracies = [client[1].get(\"accuracy\", 0.0) for client in epoch_metrics]\n",
        "                epoch_losses = [client[1].get(\"loss\", 0.0) for client in epoch_metrics]\n",
        "                epoch_communication_costs = [client[1].get(\"communication_cost\", 0.0) for client in epoch_metrics]\n",
        "                epoch_latencies = [client[1].get(\"latency\", 0.0) for client in epoch_metrics]\n",
        "                epoch_bandwidth_utilizations = [client[1].get(\"bandwidth_utilization\", 0.0) for client in epoch_metrics]\n",
        "                epoch_compression_reduction_percentages = [client[1].get(\"compression_reduction_percentage\", 0.0) for client in epoch_metrics]\n",
        "                epoch_train_accuracies = [client[1].get(\"train_accuracy\", 0.0) for client in epoch_metrics]\n",
        "                epoch_train_losses = [client[1].get(\"train_loss\", 0.0) for client in epoch_metrics]\n",
        "                epoch_test_accuracies = [client[1].get(\"test_accuracy\", 0.0) for client in epoch_metrics]\n",
        "                epoch_test_losses = [client[1].get(\"test_loss\", 0.0) for client in epoch_metrics]\n",
        "\n",
        "                metrics_history[epoch] = {\n",
        "                    \"accuracy\": sum(epoch_accuracies) / len(epoch_accuracies),\n",
        "                    \"loss\": sum(epoch_losses) / len(epoch_losses),\n",
        "                    \"communication_cost\": sum(epoch_communication_costs),\n",
        "                    \"latency\": sum(epoch_latencies) / len(epoch_latencies),\n",
        "                    \"bandwidth_utilization\": sum(epoch_bandwidth_utilizations) / len(epoch_bandwidth_utilizations),\n",
        "                    \"compression_reduction_percentage\": sum(epoch_compression_reduction_percentages) / len(epoch_compression_reduction_percentages),\n",
        "                    \"train_accuracy\": sum(epoch_train_accuracies) / len(epoch_train_accuracies),\n",
        "                    \"train_loss\": sum(epoch_train_losses) / len(epoch_train_losses),\n",
        "                    \"test_accuracy\": sum(epoch_test_accuracies) / len(epoch_test_accuracies),\n",
        "                    \"test_loss\": sum(epoch_test_losses) / len(epoch_test_losses),\n",
        "                }\n",
        "    else:\n",
        "        print(\"No metrics available.\")\n",
        "except Exception as e:\n",
        "    print(f\"Simulation failed with error: {str(e)}\")\n",
        "    # Handle the exception appropriately\n",
        "\n",
        "# ... (previous code remains the same)\n",
        "\n",
        "print(f\"Average metrics for {best_pca_components} PCA components:\")\n",
        "\n",
        "for epoch in range(1, best_early_stopping_epoch + 1):\n",
        "    if epoch in metrics_history:\n",
        "        epoch_metrics = metrics_history[epoch]\n",
        "\n",
        "        avg_accuracy = epoch_metrics[\"accuracy\"]\n",
        "        avg_loss = epoch_metrics[\"loss\"]\n",
        "        avg_communication_cost = epoch_metrics[\"communication_cost\"]\n",
        "        avg_latency = epoch_metrics[\"latency\"]\n",
        "        avg_bandwidth_utilization = epoch_metrics[\"bandwidth_utilization\"]\n",
        "        avg_compression_reduction_percentage = epoch_metrics[\"compression_reduction_percentage\"]\n",
        "        avg_train_accuracy = epoch_metrics[\"train_accuracy\"]\n",
        "        avg_train_loss = epoch_metrics[\"train_loss\"]\n",
        "        avg_test_accuracy = epoch_metrics[\"test_accuracy\"]\n",
        "        avg_test_loss = epoch_metrics[\"test_loss\"]\n",
        "\n",
        "        print(f\"Epoch {epoch}:\")\n",
        "        print(f\"  Average Accuracy: {avg_accuracy:.4f}\")\n",
        "        print(f\"  Average Loss: {avg_loss:.4f}\")\n",
        "        print(f\"  Average Communication Cost: {avg_communication_cost:.2f} bytes\")\n",
        "        print(f\"  Average Latency: {avg_latency:.2f} seconds\")\n",
        "        print(f\"  Average Bandwidth Utilization: {avg_bandwidth_utilization:.2f}%\")\n",
        "        print(f\"  Average Compression Reduction Percentage: {avg_compression_reduction_percentage:.2f}%\")\n",
        "        print(f\"  Average Train Accuracy: {avg_train_accuracy:.4f}\")\n",
        "        print(f\"  Average Train Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"  Average Test Accuracy: {avg_test_accuracy:.4f}\")\n",
        "        print(f\"  Average Test Loss: {avg_test_loss:.4f}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"No metrics available for Epoch {epoch}\")\n",
        "        print()\n",
        "\n",
        "# Print the metrics for each epoch\n",
        "for epoch in range(1, best_early_stopping_epoch + 1):\n",
        "    if epoch in metrics_history:\n",
        "        metrics = metrics_history[epoch]\n",
        "        print(f\"Epoch {epoch}:\")\n",
        "        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
        "        print(f\"  Loss: {metrics['loss']:.4f}\")\n",
        "        print(f\"  Communication Cost: {metrics['communication_cost']:.2f} bytes\")\n",
        "        print(f\"  Latency: {metrics['latency']:.2f} seconds\")\n",
        "        print(f\"  Bandwidth Utilization: {metrics['bandwidth_utilization']:.2f}%\")\n",
        "        print(f\"  Compression Reduction Percentage: {metrics['compression_reduction_percentage']:.2f}%\")\n",
        "        print(f\"  Train Accuracy: {metrics['train_accuracy']:.4f}\")\n",
        "        print(f\"  Train Loss: {metrics['train_loss']:.4f}\")\n",
        "        print(f\"  Test Accuracy: {metrics['test_accuracy']:.4f}\")\n",
        "        print(f\"  Test Loss: {metrics['test_loss']:.4f}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"No metrics available for Epoch {epoch}\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH5NFAzG-3I6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}