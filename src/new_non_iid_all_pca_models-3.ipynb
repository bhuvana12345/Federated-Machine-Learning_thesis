{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjf-WB35QiDr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d9d847c-8bc8-4593-f859-05447db36d10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q flwr[simulation] torch torchvision matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import zipfile\n",
        "\n",
        "from collections import OrderedDict\n",
        "from typing import List, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "\n",
        "import flwr as fl\n",
        "from flwr.common import Metrics\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n"
      ],
      "metadata": {
        "id": "97UtJ4_eRQvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLASSES = (\n",
        "    \"0\",\n",
        "    \"1\",\n",
        "    \"2\",\n",
        "    \"3\",\n",
        "    \"4\",\n",
        "    \"5\",\n",
        "    \"6\",\n",
        "    \"7\",\n",
        "    \"8\",\n",
        "    \"9\",\n",
        ")\n",
        "\n",
        "NUM_CLIENTS = 10\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "G6BPtnTEque1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d97b3e-ca1a-4eee-d47a-9af36077976c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def determine_pca_components(public_dataset, n_components):\n",
        "    # Reshape the public dataset to 2D (num_samples, num_features)\n",
        "    public_data = public_dataset.reshape(public_dataset.shape[0], -1)\n",
        "\n",
        "    # Fit PCA on the public dataset\n",
        "    pca = PCA(n_components=n_components)\n",
        "    pca.fit(public_data)\n",
        "\n",
        "    return pca"
      ],
      "metadata": {
        "id": "XHafWP3zqzPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_pca_locally(data, pca, n_components):\n",
        "    assert data is not None, \"Data cannot be None\"\n",
        "    assert n_components is not None and isinstance(n_components, int), \"n_components must be a valid integer\"\n",
        "\n",
        "    # Reshape the data to 2D (num_samples, num_features)\n",
        "    data_2d = data.reshape(data.shape[0], -1)\n",
        "\n",
        "    # Apply PCA transformation\n",
        "    data_transformed = pca.transform(data_2d)[:, :n_components]\n",
        "\n",
        "    # Calculate the percentage of data reduced or removed\n",
        "    original_size = data_2d.size\n",
        "    transformed_size = data_transformed.size\n",
        "    data_removed_percentage = (1 - transformed_size / original_size) * 100\n",
        "\n",
        "    # Reshape the transformed data to (num_samples, 1, n_components)\n",
        "    data_transformed = data_transformed.reshape(-1, 1, n_components)\n",
        "\n",
        "    return data_transformed, data_removed_percentage"
      ],
      "metadata": {
        "id": "EiLQs1iIrSU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import MNIST\n",
        "\n",
        "# Define the transform to preprocess the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "mnist_dataset = MNIST(\"./dataset\", train=True, download=True, transform=transform)\n",
        "\n",
        "# Load the MNIST dataset\n",
        "trainset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = MNIST(root='./data', train=False, download=True, transform=transform)\n"
      ],
      "metadata": {
        "id": "zB-4SHD5rU77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d48228ca-36d7-4788-8ec2-f173a934c9ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./dataset/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 137598663.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/MNIST/raw/train-images-idx3-ubyte.gz to ./dataset/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 19304493.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/MNIST/raw/train-labels-idx1-ubyte.gz to ./dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 169067897.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to ./dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3441829.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 134581021.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 83026520.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 100653345.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 12451325.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import random\n",
        "\n",
        "def custom_distributed_split(indices, num_clients, class_idx, distribution_ratio):\n",
        "    data_len = len(indices)\n",
        "    class_data_range = int(data_len * distribution_ratio)\n",
        "    non_class_data_range = int((data_len * (1 - distribution_ratio)) // (num_clients-1))\n",
        "\n",
        "    out = []\n",
        "    non_class_data_start = class_data_range + 1\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        if i == class_idx:\n",
        "            out.append(indices[:class_data_range + 1])\n",
        "        else:\n",
        "            non_class_data_end = non_class_data_start + non_class_data_range + 1\n",
        "            out.append(indices[non_class_data_start : non_class_data_end])\n",
        "            non_class_data_start += non_class_data_range\n",
        "\n",
        "    if non_class_data_end != data_len:\n",
        "        clients = [i for i in range(num_clients)]\n",
        "        clients.pop(class_idx)\n",
        "        for i in range(non_class_data_end, data_len):\n",
        "            rand_class_idx = random.choice(clients)\n",
        "            out[rand_class_idx].append(i)\n",
        "\n",
        "    return out\n",
        "\n",
        "def distribute_data_non_iid(dataset, num_clients):\n",
        "    class_indices = [[] for _ in range(10)]  # 10 classes in MNIST\n",
        "    for idx, (_, label) in enumerate(dataset):\n",
        "        class_indices[label].append(idx)\n",
        "\n",
        "    client_data_indices = [[] for _ in range(num_clients)]\n",
        "    for class_idx, indices in enumerate(class_indices):\n",
        "        np.random.shuffle(indices)\n",
        "        split = custom_distributed_split(indices, num_clients, class_idx, 0.6)\n",
        "        for client_idx in range(num_clients):\n",
        "            client_data_indices[client_idx].extend(split[client_idx])\n",
        "\n",
        "    client_datasets = [Subset(dataset, indices) for indices in client_data_indices]\n",
        "    return client_datasets\n"
      ],
      "metadata": {
        "id": "PiaLL0RNrzes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_compression_parameters(client_data_variabilities):\n",
        "    avg_variability = np.mean(client_data_variabilities)\n",
        "    pruning_amount = np.interp(avg_variability, [0, 1], [0.2, 0.8])\n",
        "    quantization_type = 'dynamic' if avg_variability > 0.5 else 'static'\n",
        "    return pruning_amount, quantization_type"
      ],
      "metadata": {
        "id": "wKI0WuRWMqIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "\n",
        "def determine_pca_components(public_dataset, client_data_variabilities, n_components_range=(8, 128)):\n",
        "    # Reshape the public dataset to 2D (num_samples, num_features)\n",
        "    public_data = public_dataset.reshape(public_dataset.shape[0], -1)\n",
        "\n",
        "    # Determine the number of PCA components based on client data variability\n",
        "    avg_variability = np.mean(client_data_variabilities)\n",
        "    n_components = int(np.interp(avg_variability, [0, 1], n_components_range))\n",
        "\n",
        "    # Fit PCA on the public dataset\n",
        "    pca = PCA(n_components=n_components)\n",
        "    pca.fit(public_data)\n",
        "\n",
        "    return pca, n_components, avg_variability\n",
        "\n",
        "def characterize_client_data(data):\n",
        "    # Reshape the data to 2D (num_samples, num_features)\n",
        "    data_2d = data.reshape(data.shape[0], -1)\n",
        "\n",
        "    # Calculate feature variances\n",
        "    feature_variances = np.var(data_2d, axis=0)\n",
        "\n",
        "    # Calculate class distribution\n",
        "    unique_classes, class_counts = np.unique(data_2d, return_counts=True)\n",
        "    class_distribution = class_counts / np.sum(class_counts)\n",
        "\n",
        "    # Calculate overall data variability\n",
        "    data_variability = np.mean(feature_variances) * np.sum(class_distribution ** 2)\n",
        "\n",
        "    return data_variability\n"
      ],
      "metadata": {
        "id": "GjsZM0YSuDuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_components_range = [128]\n",
        "#pca_components_range = [8, 16, 32, 64]"
      ],
      "metadata": {
        "id": "yD7uEx_3uiC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "def load_datasets(pca, n_components):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    trainset = MNIST(\"./dataset\", train=True, download=True, transform=transform)\n",
        "    testset = MNIST(\"./dataset\", train=False, download=True, transform=transform)\n",
        "\n",
        "    trainset_transformed, train_removed_percentage = apply_pca_locally(trainset.data.numpy(), pca, n_components)\n",
        "    testset_transformed, test_removed_percentage = apply_pca_locally(testset.data.numpy(), pca, n_components)\n",
        "\n",
        "    # Determine compression parameters based on client data variability\n",
        "    avg_variability = np.mean(client_data_variabilities)\n",
        "    pruning_amount = np.interp(avg_variability, [0, 1], [0.2, 0.8])\n",
        "    quantization_type = 'dynamic' if avg_variability > 0.5 else 'static'\n",
        "\n",
        "    # Split the dataset into non-IID client datasets\n",
        "    client_datasets = distribute_data_non_iid(trainset, NUM_CLIENTS)\n",
        "\n",
        "    trainloaders = []\n",
        "    valloaders = []\n",
        "    for ds in client_datasets:\n",
        "        len_val = len(ds) // 10\n",
        "        len_train = len(ds) - len_val\n",
        "        ds_train, ds_val = random_split(ds, [len_train, len_val], generator=torch.Generator().manual_seed(42))\n",
        "        trainloaders.append(DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True))\n",
        "        valloaders.append(DataLoader(ds_val, batch_size=BATCH_SIZE))\n",
        "\n",
        "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
        "    return trainloaders, valloaders, testloader, client_datasets, testset, train_removed_percentage, test_removed_percentage\n",
        "    #return trainloaders, valloaders, testloader, client_datasets, testset, train_removed_percentage, test_removed_percentage, pruning_amount, quantization_type\n",
        "\n",
        "def train_and_evaluate(pca, n_components, trainloader, valloader, testloader):\n",
        "    net = Net().to(DEVICE)\n",
        "    net, early_stopping_epoch, test_loss, test_accuracy = train(net, trainloader, valloader, testloader, epochs=100, device=DEVICE)\n",
        "    print(f\"Final test set performance for {n_components} PCA components: Loss {test_loss:.4f}, Accuracy {test_accuracy:.4f}\")\n",
        "    print(f\"Early stopping at epoch: {early_stopping_epoch}\")\n",
        "    print()\n",
        "    return test_accuracy, test_loss, early_stopping_epoch"
      ],
      "metadata": {
        "id": "hhs-gLucr1R2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 7 * 7)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "dT1q-EkCrYSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.optim import Adam\n",
        "from torch.nn.functional import relu, softmax\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def train(net, trainloader, valloader, testloader, epochs, device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "\n",
        "    best_valid_loss = float('inf')\n",
        "    best_model = None\n",
        "    best_test_loss = float('inf')\n",
        "    best_test_accuracy = 0.0\n",
        "    patience_counter = 0\n",
        "    early_stopping_epoch = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in trainloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(trainloader)\n",
        "        train_acc = correct / total\n",
        "\n",
        "        net.eval()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in valloader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = net(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        valid_loss = running_loss / len(valloader)\n",
        "        valid_acc = correct / total\n",
        "\n",
        "        # Calculate test metrics for every epoch\n",
        "        with torch.no_grad():\n",
        "            test_loss, test_accuracy = test(net, testloader)\n",
        "\n",
        "        # Print metrics for every epoch\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.4f}\")\n",
        "\n",
        "        scheduler.step(valid_loss)\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            best_model = net.state_dict()\n",
        "            best_test_loss = test_loss\n",
        "            best_test_accuracy = test_accuracy\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= 10:\n",
        "            early_stopping_epoch = epoch + 1\n",
        "            break\n",
        "\n",
        "    net.load_state_dict(best_model)\n",
        "    return net, early_stopping_epoch, best_test_loss, best_test_accuracy\n",
        "\n",
        "\n",
        "def test(net, testloader):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    correct, total, loss = 0, 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    loss /= len(testloader.dataset)\n",
        "    accuracy = correct / total\n",
        "    return loss, accuracy\n"
      ],
      "metadata": {
        "id": "GhxdKTXNrbkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pca_components_range = [128]  # Define the range of PCA components to try\n",
        "\n",
        "best_accuracy = 0\n",
        "best_pca_components = None\n",
        "best_test_loss = float('inf')\n",
        "best_compression = 0\n",
        "\n",
        "# ... (previous code remains the same)\n",
        "\n",
        "# Split the dataset into NUM_CLIENTS partitions\n",
        "partition_size = len(trainset) // NUM_CLIENTS\n",
        "lengths = [partition_size] * NUM_CLIENTS\n",
        "datasets = random_split(trainset, lengths, torch.Generator().manual_seed(42))\n",
        "\n",
        "\n",
        "client_data_variabilities = []\n",
        "for i in range(NUM_CLIENTS):\n",
        "    data_variability = characterize_client_data(datasets[i].dataset.data.numpy())\n",
        "    client_data_variabilities.append(data_variability)\n",
        "\n",
        "# Determine PCA components dynamically based on client data variability\n",
        "pca, n_components, avg_variability = determine_pca_components(mnist_dataset.data.numpy(), client_data_variabilities)\n",
        "\n",
        "# Load the datasets using the PCA object\n",
        "trainloaders, valloaders, testloader, datasets, testset, _, _ = load_datasets(pca, n_components)\n",
        "\n",
        "pca_components_range = [128]\n",
        "best_accuracy = 0\n",
        "best_pca_components = None\n",
        "best_test_loss = float('inf')\n",
        "best_early_stopping_epoch = 0\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "\n",
        "for n_components in pca_components_range:\n",
        "    print(f\"Training with {n_components} PCA components...\")\n",
        "    pca, n_components, avg_variability = determine_pca_components(mnist_dataset.data.numpy(), client_data_variabilities, n_components_range=(n_components, n_components))\n",
        "    print(f\"Average variability: {avg_variability:.4f}\")\n",
        "    print(f\"Determined number of PCA components: {n_components}\")\n",
        "\n",
        "    trainloaders, valloaders, testloader, datasets, testset, _, _ = load_datasets(pca, n_components)\n",
        "\n",
        "    train_removed_percentages = []\n",
        "    test_removed_percentages = []\n",
        "    for i in range(NUM_CLIENTS):\n",
        "        _, train_removed_percentage = apply_pca_locally(datasets[i].dataset.data.numpy(), pca, n_components)\n",
        "        _, test_removed_percentage = apply_pca_locally(testset.data.numpy(), pca, n_components)\n",
        "        train_removed_percentages.append(train_removed_percentage)\n",
        "        test_removed_percentages.append(test_removed_percentage)\n",
        "\n",
        "    # Determine compression parameters based on client data variability\n",
        "    pruning_amount, quantization_type = determine_compression_parameters(client_data_variabilities)\n",
        "\n",
        "    trainloader = trainloaders[0]\n",
        "    valloader = valloaders[0]\n",
        "\n",
        "    test_accuracy, test_loss, early_stopping_epoch = train_and_evaluate(pca, n_components, trainloader, valloader, testloader)\n",
        "\n",
        "    compression_ratio = (1 - (n_components * 784) / (28 * 28 * 784)) * 100\n",
        "\n",
        "    results.append({\n",
        "        'n_components': n_components,\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'compression_ratio': compression_ratio\n",
        "    })\n",
        "\n",
        "    if test_accuracy > best_accuracy:\n",
        "        best_accuracy = test_accuracy\n",
        "        best_pca_components = n_components\n",
        "        best_test_loss = test_loss\n",
        "        best_compression = compression_ratio\n",
        "        best_early_stopping_epoch = early_stopping_epoch\n",
        "\n",
        "\n",
        "print(f\"Best PCA Components: {best_pca_components}\")\n",
        "print(f\"Best Test Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"Best Test Loss: {best_test_loss:.4f}\")\n",
        "print(f\"Best Early Stopping Epoch: {best_early_stopping_epoch}\")\n",
        "print(f\"Quantization Type: {quantization_type}\")\n",
        "print(f\"Pruning Amount: {pruning_amount}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"Results:\")\n",
        "for result in results:\n",
        "    print(f\"PCA Components: {result['n_components']}, Test Loss: {result['test_loss']:.4f}, Test Accuracy: {result['test_accuracy']:.4f}, Compression Ratio: {result['compression_ratio']:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTVJxxkvunoX",
        "outputId": "4e6563f9-cbbb-4e25-c7e0-72c443c1fc90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with 128 PCA components...\n",
            "Average variability: 2867.7677\n",
            "Determined number of PCA components: 128\n",
            "Epoch 1/100 - Train Loss: 0.6624, Train Acc: 0.7971, Valid Loss: 0.2364, Valid Acc: 0.9228, Test Loss: 0.0134, Test Acc: 0.8820\n",
            "Epoch 2/100 - Train Loss: 0.2010, Train Acc: 0.9383, Valid Loss: 0.1195, Valid Acc: 0.9664, Test Loss: 0.0077, Test Acc: 0.9313\n",
            "Epoch 3/100 - Train Loss: 0.1279, Train Acc: 0.9614, Valid Loss: 0.0832, Valid Acc: 0.9782, Test Loss: 0.0055, Test Acc: 0.9482\n",
            "Epoch 4/100 - Train Loss: 0.0881, Train Acc: 0.9763, Valid Loss: 0.0826, Valid Acc: 0.9765, Test Loss: 0.0045, Test Acc: 0.9583\n",
            "Epoch 5/100 - Train Loss: 0.0634, Train Acc: 0.9808, Valid Loss: 0.0938, Valid Acc: 0.9732, Test Loss: 0.0046, Test Acc: 0.9569\n",
            "Epoch 6/100 - Train Loss: 0.0541, Train Acc: 0.9829, Valid Loss: 0.0754, Valid Acc: 0.9782, Test Loss: 0.0037, Test Acc: 0.9643\n",
            "Epoch 7/100 - Train Loss: 0.0415, Train Acc: 0.9877, Valid Loss: 0.0831, Valid Acc: 0.9765, Test Loss: 0.0039, Test Acc: 0.9614\n",
            "Epoch 8/100 - Train Loss: 0.0321, Train Acc: 0.9911, Valid Loss: 0.0699, Valid Acc: 0.9815, Test Loss: 0.0035, Test Acc: 0.9678\n",
            "Epoch 9/100 - Train Loss: 0.0239, Train Acc: 0.9916, Valid Loss: 0.0776, Valid Acc: 0.9765, Test Loss: 0.0047, Test Acc: 0.9571\n",
            "Epoch 10/100 - Train Loss: 0.0346, Train Acc: 0.9899, Valid Loss: 0.0748, Valid Acc: 0.9799, Test Loss: 0.0039, Test Acc: 0.9641\n",
            "Epoch 11/100 - Train Loss: 0.0203, Train Acc: 0.9944, Valid Loss: 0.0945, Valid Acc: 0.9799, Test Loss: 0.0051, Test Acc: 0.9570\n",
            "Epoch 12/100 - Train Loss: 0.0224, Train Acc: 0.9929, Valid Loss: 0.0592, Valid Acc: 0.9849, Test Loss: 0.0045, Test Acc: 0.9625\n",
            "Epoch 13/100 - Train Loss: 0.0163, Train Acc: 0.9939, Valid Loss: 0.0922, Valid Acc: 0.9765, Test Loss: 0.0044, Test Acc: 0.9660\n",
            "Epoch 14/100 - Train Loss: 0.0187, Train Acc: 0.9939, Valid Loss: 0.0992, Valid Acc: 0.9698, Test Loss: 0.0040, Test Acc: 0.9675\n",
            "Epoch 15/100 - Train Loss: 0.0152, Train Acc: 0.9953, Valid Loss: 0.0867, Valid Acc: 0.9765, Test Loss: 0.0039, Test Acc: 0.9687\n",
            "Epoch 16/100 - Train Loss: 0.0107, Train Acc: 0.9961, Valid Loss: 0.0840, Valid Acc: 0.9832, Test Loss: 0.0039, Test Acc: 0.9693\n",
            "Epoch 17/100 - Train Loss: 0.0109, Train Acc: 0.9965, Valid Loss: 0.0833, Valid Acc: 0.9799, Test Loss: 0.0041, Test Acc: 0.9688\n",
            "Epoch 18/100 - Train Loss: 0.0087, Train Acc: 0.9972, Valid Loss: 0.1143, Valid Acc: 0.9748, Test Loss: 0.0048, Test Acc: 0.9670\n",
            "Epoch 19/100 - Train Loss: 0.0044, Train Acc: 0.9993, Valid Loss: 0.0756, Valid Acc: 0.9832, Test Loss: 0.0039, Test Acc: 0.9732\n",
            "Epoch 20/100 - Train Loss: 0.0048, Train Acc: 0.9989, Valid Loss: 0.0736, Valid Acc: 0.9849, Test Loss: 0.0038, Test Acc: 0.9740\n",
            "Epoch 21/100 - Train Loss: 0.0038, Train Acc: 0.9991, Valid Loss: 0.0766, Valid Acc: 0.9832, Test Loss: 0.0039, Test Acc: 0.9736\n",
            "Epoch 22/100 - Train Loss: 0.0034, Train Acc: 0.9991, Valid Loss: 0.0741, Valid Acc: 0.9832, Test Loss: 0.0039, Test Acc: 0.9736\n",
            "Final test set performance for 128 PCA components: Loss 0.0045, Accuracy 0.9625\n",
            "Early stopping at epoch: 22\n",
            "\n",
            "Best PCA Components: 128\n",
            "Best Test Accuracy: 0.9625\n",
            "Best Test Loss: 0.0045\n",
            "Best Early Stopping Epoch: 22\n",
            "Quantization Type: dynamic\n",
            "Pruning Amount: 0.8\n",
            "Results:\n",
            "PCA Components: 128, Test Loss: 0.0045, Test Accuracy: 0.9625, Compression Ratio: 83.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "\n",
        "# Model size calculation function\n",
        "def get_model_size(model):\n",
        "    buffer = io.BytesIO()\n",
        "    torch.save(model.state_dict(), buffer)\n",
        "    size = buffer.tell()  # Size in bytes\n",
        "    return size"
      ],
      "metadata": {
        "id": "PaQrgaPzv8Vk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79ae08dd-6f41-48bc-b5e9-4a4f60f70c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "def get_parameters(net) -> List[np.ndarray]:\n",
        "    parameters = []\n",
        "    for name, param in net.named_parameters():\n",
        "        print(f\"Get parameters: {name}: {param.shape}\")\n",
        "        parameters.append(param.detach().cpu().numpy())\n",
        "    return parameters\n",
        "\n",
        "def set_parameters(net, parameters: List[np.ndarray]):\n",
        "    params_dict = zip(net.state_dict().keys(), parameters)\n",
        "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "\n",
        "    # Print the expected parameter shapes and the received parameter shapes\n",
        "    print(\"Expected parameter shapes:\")\n",
        "    for name, param in net.named_parameters():\n",
        "        print(f\"{name}: {param.shape}\")\n",
        "\n",
        "    print(\"Received parameter shapes:\")\n",
        "    for name, param in state_dict.items():\n",
        "        print(f\"{name}: {param.shape}\")\n",
        "\n",
        "    # Try to load the state dictionary and handle shape mismatch\n",
        "    try:\n",
        "        net.load_state_dict(state_dict, strict=True)\n",
        "    except RuntimeError as e:\n",
        "        print(\"Shape mismatch occurred. Attempting to handle gracefully...\")\n",
        "\n",
        "        # Create a new state dictionary with matching shapes\n",
        "        new_state_dict = OrderedDict()\n",
        "        for (name, param), (state_name, state_param) in zip(net.named_parameters(), state_dict.items()):\n",
        "            if param.shape != state_param.shape:\n",
        "                print(f\"Shape mismatch for parameter '{name}'. Expected {param.shape}, got {state_param.shape}.\")\n",
        "                if param.numel() == state_param.numel():\n",
        "                    # Reshape the parameter if the total number of elements matches\n",
        "                    state_param = state_param.view(param.shape)\n",
        "                else:\n",
        "                    print(f\"Cannot reshape parameter '{name}' due to incompatible number of elements.\")\n",
        "                    continue\n",
        "            new_state_dict[name] = state_param\n",
        "\n",
        "        # Load the new state dictionary\n",
        "        net.load_state_dict(new_state_dict, strict=False)\n",
        "        print(\"Shape mismatch handled.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2MYLWWM0-m30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.quantization\n",
        "import torch.nn.utils.prune as prune\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "\n",
        "def apply_pruning(model, pruning_technique, amount):\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "            if pruning_technique == 'l1_unstructured':\n",
        "                prune.l1_unstructured(module, name='weight', amount=amount)\n",
        "            elif pruning_technique == 'l1_structured':\n",
        "                prune.ln_structured(module, name='weight', amount=amount, n=1, dim=0)\n",
        "            elif pruning_technique == 'global_unstructured':\n",
        "                prune.global_unstructured(module, name='weight', amount=amount)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported pruning technique: {pruning_technique}\")\n",
        "            prune.remove(module, 'weight')\n",
        "\n",
        "def apply_quantization(model, quantization_type):\n",
        "    if quantization_type == 'dynamic':\n",
        "        quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "    elif quantization_type == 'static':\n",
        "        model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "        torch.quantization.prepare(model, inplace=True)\n",
        "        torch.quantization.convert(model, inplace=True)\n",
        "        quantized_model = model\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported quantization type: {quantization_type}\")\n",
        "    return quantized_model\n"
      ],
      "metadata": {
        "id": "2KCIzXjz-o9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "def fine_tune(net, trainloader, valloader, device):\n",
        "    # Define the loss function and the optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "    # Initialize the learning rate scheduler\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "\n",
        "    for epoch in range(3):  # Run for 3 epochs\n",
        "        net.train()  # Set the network to training mode\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in trainloader:  # Iterate over the training data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()  # Zero the parameter gradients\n",
        "            outputs = net(images)  # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Compute the loss\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Optimize\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(trainloader)\n",
        "        train_acc = correct / total\n",
        "\n",
        "        net.eval()  # Set the network to evaluation mode\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient computation\n",
        "            for images, labels in valloader:  # Iterate over the validation data\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = net(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        valid_loss = running_loss / len(valloader)\n",
        "        valid_acc = correct / total\n",
        "\n",
        "        # Print the training and validation statistics\n",
        "        print(f\"Epoch {epoch+1}/3 - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}\")\n",
        "\n",
        "        scheduler.step(valid_loss)  # Adjust the learning rate based on the validation loss\n",
        "\n",
        "    return net  # Return the trained network"
      ],
      "metadata": {
        "id": "057ETU_2-rCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "def compress_model(net, pruning_technique, pruning_amount, quantization_type):\n",
        "    # Apply pruning\n",
        "    apply_pruning(net, pruning_technique, pruning_amount)\n",
        "\n",
        "    # Fine-tune the pruned model for 3 epochs\n",
        "    fine_tune(net, trainloader, valloader, DEVICE)  # Pass 4 arguments as per the function definition\n",
        "\n",
        "    # Apply quantization\n",
        "    quantized_model = apply_quantization(net, quantization_type)\n",
        "\n",
        "    return quantized_model"
      ],
      "metadata": {
        "id": "0qUI0TTu-tA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_compression_parameters(client_data_variabilities):\n",
        "    avg_variability = np.mean(client_data_variabilities)\n",
        "    pruning_amount = np.interp(avg_variability, [0, 1], [0.2, 0.8])\n",
        "    quantization_type = 'dynamic' if avg_variability > 0.5 else 'static'\n",
        "    return pruning_amount, quantization_type"
      ],
      "metadata": {
        "id": "Zx_dtIlv-u0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import flwr as fl\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, net, trainloader, valloader, testloader, pca, train_removed_percentage, test_removed_percentage, early_stopping_epoch, n_components, available_bandwidth=1000, pruning_amount=0.4, quantization_type=None):\n",
        "        self.net = net\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = valloader\n",
        "        self.testloader = testloader\n",
        "        self.pca = pca\n",
        "        self.train_removed_percentage = train_removed_percentage\n",
        "        self.test_removed_percentage = test_removed_percentage\n",
        "        self.early_stopping_epoch = early_stopping_epoch\n",
        "        self.n_components = n_components\n",
        "        self.original_model_size = get_model_size(net)\n",
        "        self.communication_rounds = 0\n",
        "        self.communication_cost = 0\n",
        "        self.bandwidth_utilization = 0\n",
        "        self.latency = 0\n",
        "        self.available_bandwidth = float(available_bandwidth)\n",
        "        self.pruning_amount = pruning_amount\n",
        "        self.quantization_type = quantization_type\n",
        "\n",
        "\n",
        "    def get_parameters(self, **kwargs):\n",
        "        parameters = get_parameters(self.net)\n",
        "        return parameters\n",
        "\n",
        "    def set_parameters(self, parameters):\n",
        "        set_parameters(self.net, parameters)\n",
        "\n",
        "    def calculate_bandwidth_utilization(self, model_size_before_compression, model_size_after_compression):\n",
        "        # Calculate the size of the data sent and received in bytes\n",
        "        data_sent = model_size_after_compression\n",
        "        data_received = model_size_after_compression\n",
        "\n",
        "        # Calculate the total data exchanged\n",
        "        total_data = data_sent + data_received\n",
        "\n",
        "        # Convert the total data to megabits (Mb)\n",
        "        total_data_mb = total_data / (1024 * 1024) * 8\n",
        "\n",
        "        # Calculate the bandwidth utilization percentage\n",
        "        bandwidth_utilization = (total_data_mb / float(self.available_bandwidth)) * 100\n",
        "\n",
        "        return bandwidth_utilization\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        start_time = time.time()\n",
        "        self.set_parameters(parameters)\n",
        "        epochs = config.get(\"epochs\", self.early_stopping_epoch)  # Use early_stopping_epoch as default if 'epochs' key is not present\n",
        "\n",
        "        if self.pruning_amount is None or self.quantization_type is None:\n",
        "            # Determine compression parameters based on client data variability\n",
        "            self.pruning_amount, self.quantization_type = determine_compression_parameters(client_data_variabilities)\n",
        "\n",
        "        model_size_before_compression = get_model_size(self.net)\n",
        "        compressed_model = compress_model(self.net, pruning_technique='l1_unstructured', pruning_amount=self.pruning_amount, quantization_type=self.quantization_type)\n",
        "        model_size_after_compression = get_model_size(compressed_model)\n",
        "\n",
        "        compressed_model, _, _, _ = train(compressed_model, self.trainloader, self.valloader, self.testloader, epochs, DEVICE)\n",
        "\n",
        "        reduced_size = model_size_after_compression\n",
        "        compression_reduction_percentage = (1 - reduced_size / model_size_before_compression) * 100\n",
        "\n",
        "        self.communication_rounds += 1\n",
        "        self.communication_cost += reduced_size * (1 - self.train_removed_percentage / 100) * (1 - compression_reduction_percentage / 100)\n",
        "        self.bandwidth_utilization += self.calculate_bandwidth_utilization(model_size_before_compression, model_size_after_compression) * (1 - self.train_removed_percentage / 100) * (1 - compression_reduction_percentage / 100)\n",
        "        self.latency += (time.time() - start_time) * (1 - self.train_removed_percentage / 100) * (1 - compression_reduction_percentage / 100)\n",
        "\n",
        "        return get_parameters(compressed_model), len(self.trainloader.dataset), {\n",
        "            \"pca_reduction_percentage\": self.train_removed_percentage,\n",
        "            \"compression_reduction_percentage\": compression_reduction_percentage,\n",
        "            \"n_components\": self.n_components,\n",
        "            \"communication_rounds\": self.communication_rounds,\n",
        "            \"communication_cost\": self.communication_cost,\n",
        "            \"bandwidth_utilization\": self.bandwidth_utilization,\n",
        "            \"latency\": self.latency,\n",
        "        }\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        start_time = time.time()\n",
        "        self.set_parameters(parameters)\n",
        "\n",
        "        if self.pruning_amount is None or self.quantization_type is None:\n",
        "            # Determine compression parameters based on client data variability\n",
        "            self.pruning_amount, self.quantization_type = determine_compression_parameters(client_data_variabilities)\n",
        "\n",
        "        model_size_before_compression = get_model_size(self.net)\n",
        "        compressed_model = compress_model(self.net, pruning_technique='l1_unstructured', pruning_amount=self.pruning_amount, quantization_type=self.quantization_type)\n",
        "        model_size_after_compression = get_model_size(compressed_model)\n",
        "\n",
        "        reduced_size = model_size_after_compression\n",
        "        compression_reduction_percentage = (1 - reduced_size / model_size_before_compression) * 100\n",
        "        loss, accuracy = test(compressed_model, self.testloader)\n",
        "\n",
        "        self.communication_rounds += 1\n",
        "        self.communication_cost += reduced_size * (1 - self.test_removed_percentage / 100) * (1 - compression_reduction_percentage / 100)\n",
        "        self.bandwidth_utilization += self.calculate_bandwidth_utilization(model_size_before_compression, model_size_after_compression) * (1 - self.test_removed_percentage / 100) * (1 - compression_reduction_percentage / 100)\n",
        "        self.latency += (time.time() - start_time) * (1 - self.test_removed_percentage / 100) * (1 - compression_reduction_percentage / 100)\n",
        "\n",
        "        return float(loss), len(self.testloader.dataset), {\n",
        "            \"accuracy\": float(accuracy),\n",
        "            \"pca_reduction_percentage\": float(self.test_removed_percentage),\n",
        "            \"compression_reduction_percentage\": compression_reduction_percentage,\n",
        "            \"n_components\": float(self.n_components),\n",
        "            \"communication_rounds\": float(self.communication_rounds),\n",
        "            \"communication_cost\": float(self.communication_cost),\n",
        "            \"bandwidth_utilization\": float(self.bandwidth_utilization),\n",
        "            \"latency\": float(self.latency),\n",
        "        }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hhHTvSYv-w3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
        "    # Multiply accuracy of each client by number of examples used\n",
        "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
        "    examples = [num_examples for num_examples, _ in metrics]\n",
        "\n",
        "    # Extract PCA reduction percentages from metrics\n",
        "    pca_reduction_percentages = [m[\"pca_reduction_percentage\"] for _, m in metrics]\n",
        "\n",
        "    # Extract compression reduction percentages from metrics\n",
        "    compression_reduction_percentages = [m[\"compression_reduction_percentage\"] for _, m in metrics]\n",
        "\n",
        "    communication_rounds = [m[\"communication_rounds\"] for _, m in metrics]\n",
        "    communication_cost = [m[\"communication_cost\"] for _, m in metrics]\n",
        "    bandwidth_utilization = [m[\"bandwidth_utilization\"] for _, m in metrics]\n",
        "    latency = [m[\"latency\"] for _, m in metrics]\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": sum(accuracies) / sum(examples),\n",
        "        \"pca_reduction_percentage\": sum(pca_reduction_percentages) / len(pca_reduction_percentages),\n",
        "        \"compression_reduction_percentage\": sum(compression_reduction_percentages) / len(compression_reduction_percentages),\n",
        "        \"communication_rounds\": sum(communication_rounds) / len(communication_rounds),\n",
        "        \"communication_cost\": sum(communication_cost),\n",
        "        \"bandwidth_utilization\": sum(bandwidth_utilization) / len(bandwidth_utilization),\n",
        "        \"latency\": sum(latency) / len(latency),\n",
        "    }"
      ],
      "metadata": {
        "id": "yrVZ_Hjs-y0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "mnist_dataset = MNIST(\"./dataset\", train=True, download=True, transform=transform)\n",
        "\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%)\n",
        "train_size = int(0.8 * len(mnist_dataset))\n",
        "test_size = len(mnist_dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(mnist_dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "\n",
        "def client_fn(cid, pca, train_removed_percentage, test_removed_percentage, testloader, n_components):\n",
        "    model = Net().to(DEVICE)\n",
        "    trainloaders, valloaders, testloader, datasets, testset, train_removed_percentage, test_removed_percentage = load_datasets(pca, n_components)\n",
        "    pruning_amount, quantization_type = determine_compression_parameters(client_data_variabilities)\n",
        "    client = FlowerClient(model, trainloaders[int(cid)], valloaders[int(cid)], testloader, pca, train_removed_percentage, test_removed_percentage, best_early_stopping_epoch, n_components, available_bandwidth=1000, pruning_amount=pruning_amount, quantization_type=quantization_type)\n",
        "    return client.to_client()\n",
        "\n",
        "# Characterize client data variability\n",
        "client_data_variabilities = []\n",
        "for i in range(NUM_CLIENTS):\n",
        "    data_variability = characterize_client_data(datasets[i].dataset.data.numpy())\n",
        "    client_data_variabilities.append(data_variability)\n",
        "\n",
        "# Determine PCA components dynamically based on client data variability\n",
        "pca, n_components, avg_variability = determine_pca_components(mnist_dataset.data.numpy(), client_data_variabilities)\n",
        "\n",
        "# Load the datasets using the PCA object\n",
        "trainloaders, valloaders, testloader, datasets, testset, _, _ = load_datasets(pca, n_components)\n",
        "\n",
        "# Determine compression parameters based on client data variability\n",
        "pruning_amount, quantization_type = determine_compression_parameters(client_data_variabilities)\n",
        "\n",
        "\n",
        "pca_components_range = [128]\n",
        "best_accuracy = 0\n",
        "best_pca_components = None\n",
        "best_test_loss = float('inf')\n",
        "best_early_stopping_epoch = 0\n",
        "\n",
        "for n_components in pca_components_range:\n",
        "    print(f\"Training with {n_components} PCA components...\")\n",
        "    pca, n_components, avg_variability = determine_pca_components(mnist_dataset.data.numpy(), client_data_variabilities, n_components_range=(n_components, n_components))\n",
        "    print(f\"Average variability: {avg_variability:.4f}\")\n",
        "    print(f\"Determined number of PCA components: {n_components}\")\n",
        "\n",
        "    trainloaders, valloaders, testloader, datasets, testset, _, _ = load_datasets(pca, n_components)\n",
        "\n",
        "    # Determine compression parameters based on client data variability\n",
        "    pruning_amount, quantization_type = determine_compression_parameters(client_data_variabilities)\n",
        "\n",
        "\n",
        "    train_removed_percentages = []\n",
        "    test_removed_percentages = []\n",
        "    for i in range(NUM_CLIENTS):\n",
        "        _, train_removed_percentage = apply_pca_locally(datasets[i].dataset.data.numpy(), pca, n_components)\n",
        "        _, test_removed_percentage = apply_pca_locally(testset.data.numpy(), pca, n_components)\n",
        "        train_removed_percentages.append(train_removed_percentage)\n",
        "        test_removed_percentages.append(test_removed_percentage)\n",
        "\n",
        "    trainloader = trainloaders[0]\n",
        "    valloader = valloaders[0]\n",
        "\n",
        "    test_accuracy, test_loss, early_stopping_epoch = train_and_evaluate(pca, n_components, trainloader, valloader, testloader)\n",
        "\n",
        "    if test_accuracy > best_accuracy:\n",
        "        best_accuracy = test_accuracy\n",
        "        best_pca_components = n_components\n",
        "        best_test_loss = test_loss\n",
        "        best_early_stopping_epoch = early_stopping_epoch\n",
        "\n",
        "\n",
        "print(f\"Best PCA Components: {best_pca_components}\")\n",
        "print(f\"Best Test Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"Best Test Loss: {best_test_loss:.4f}\")\n",
        "print(f\"Best Early Stopping Epoch: {best_early_stopping_epoch}\")\n",
        "print(f\"Quantization Type: {quantization_type}\")\n",
        "print(f\"Pruning Amount: {pruning_amount}\")\n",
        "\n",
        "# Pass the PCA object, data removed percentages, testloader, number of components, and compression parameters to the client_fn\n",
        "def client_fn_with_pca(cid):\n",
        "    return client_fn(cid, pca, train_removed_percentages[int(cid)], test_removed_percentages[int(cid)], testloader, best_pca_components)\n",
        "\n",
        "strategy = fl.server.strategy.FedAvg(\n",
        "    fraction_fit=1.0,\n",
        "    fraction_evaluate=0.5,\n",
        "    min_fit_clients=10,\n",
        "    min_evaluate_clients=5,\n",
        "    min_available_clients=10,\n",
        "    evaluate_metrics_aggregation_fn=weighted_average,\n",
        ")\n",
        "\n",
        "\n",
        "# ... (previous code remains the same)\n",
        "\n",
        "metrics_history = {}  # Dictionary to store metrics for each epoch\n",
        "\n",
        "try:\n",
        "    history = fl.simulation.start_simulation(\n",
        "        client_fn=client_fn_with_pca,\n",
        "        num_clients=NUM_CLIENTS,\n",
        "        config=fl.server.ServerConfig(num_rounds=best_early_stopping_epoch),\n",
        "        strategy=strategy,\n",
        "    )\n",
        "\n",
        "    if history is not None and history.metrics_distributed:\n",
        "        for epoch, epoch_metrics in enumerate(history.metrics_distributed, start=1):\n",
        "            if epoch_metrics:\n",
        "                epoch_accuracies = [client[1].get(\"accuracy\", 0.0) for client in epoch_metrics]\n",
        "                epoch_losses = [client[1].get(\"loss\", 0.0) for client in epoch_metrics]\n",
        "                epoch_communication_costs = [client[1].get(\"communication_cost\", 0.0) for client in epoch_metrics]\n",
        "                epoch_latencies = [client[1].get(\"latency\", 0.0) for client in epoch_metrics]\n",
        "                epoch_bandwidth_utilizations = [client[1].get(\"bandwidth_utilization\", 0.0) for client in epoch_metrics]\n",
        "                epoch_compression_reduction_percentages = [client[1].get(\"compression_reduction_percentage\", 0.0) for client in epoch_metrics]\n",
        "                epoch_train_accuracies = [client[1].get(\"train_accuracy\", 0.0) for client in epoch_metrics]\n",
        "                epoch_train_losses = [client[1].get(\"train_loss\", 0.0) for client in epoch_metrics]\n",
        "                epoch_test_accuracies = [client[1].get(\"test_accuracy\", 0.0) for client in epoch_metrics]\n",
        "                epoch_test_losses = [client[1].get(\"test_loss\", 0.0) for client in epoch_metrics]\n",
        "\n",
        "                metrics_history[epoch] = {\n",
        "                    \"accuracy\": sum(epoch_accuracies) / len(epoch_accuracies),\n",
        "                    \"loss\": sum(epoch_losses) / len(epoch_losses),\n",
        "                    \"communication_cost\": sum(epoch_communication_costs),\n",
        "                    \"latency\": sum(epoch_latencies) / len(epoch_latencies),\n",
        "                    \"bandwidth_utilization\": sum(epoch_bandwidth_utilizations) / len(epoch_bandwidth_utilizations),\n",
        "                    \"compression_reduction_percentage\": sum(epoch_compression_reduction_percentages) / len(epoch_compression_reduction_percentages),\n",
        "                    \"train_accuracy\": sum(epoch_train_accuracies) / len(epoch_train_accuracies),\n",
        "                    \"train_loss\": sum(epoch_train_losses) / len(epoch_train_losses),\n",
        "                    \"test_accuracy\": sum(epoch_test_accuracies) / len(epoch_test_accuracies),\n",
        "                    \"test_loss\": sum(epoch_test_losses) / len(epoch_test_losses),\n",
        "                }\n",
        "    else:\n",
        "        print(\"No metrics available.\")\n",
        "except Exception as e:\n",
        "    print(f\"Simulation failed with error: {str(e)}\")\n",
        "    # Handle the exception appropriately\n",
        "\n",
        "# Print the metrics for each epoch\n",
        "for epoch in range(1, best_early_stopping_epoch + 1):\n",
        "    if epoch in metrics_history:\n",
        "        metrics = metrics_history[epoch]\n",
        "        print(f\"Epoch {epoch}:\")\n",
        "        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
        "        print(f\"  Loss: {metrics['loss']:.4f}\")\n",
        "        print(f\"  Communication Cost: {metrics['communication_cost']:.2f} bytes\")\n",
        "        print(f\"  Latency: {metrics['latency']:.2f} seconds\")\n",
        "        print(f\"  Bandwidth Utilization: {metrics['bandwidth_utilization']:.2f}%\")\n",
        "        print(f\"  Compression Reduction Percentage: {metrics['compression_reduction_percentage']:.2f}%\")\n",
        "        print(f\"  Train Accuracy: {metrics['train_accuracy']:.4f}\")\n",
        "        print(f\"  Train Loss: {metrics['train_loss']:.4f}\")\n",
        "        print(f\"  Test Accuracy: {metrics['test_accuracy']:.4f}\")\n",
        "        print(f\"  Test Loss: {metrics['test_loss']:.4f}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"No metrics available for Epoch {epoch}\")\n",
        "        print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZM8RyNN-0yT",
        "outputId": "cccb506d-d12e-4969-928e-9509961a9fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with 128 PCA components...\n",
            "Average variability: 2867.7677\n",
            "Determined number of PCA components: 128\n",
            "Epoch 1/100 - Train Loss: 0.7105, Train Acc: 0.7813, Valid Loss: 0.2667, Valid Acc: 0.9228, Test Loss: 0.0130, Test Acc: 0.8888\n",
            "Epoch 2/100 - Train Loss: 0.1849, Train Acc: 0.9454, Valid Loss: 0.1358, Valid Acc: 0.9648, Test Loss: 0.0067, Test Acc: 0.9384\n",
            "Epoch 3/100 - Train Loss: 0.1164, Train Acc: 0.9659, Valid Loss: 0.1202, Valid Acc: 0.9648, Test Loss: 0.0058, Test Acc: 0.9430\n",
            "Epoch 4/100 - Train Loss: 0.0858, Train Acc: 0.9737, Valid Loss: 0.0957, Valid Acc: 0.9732, Test Loss: 0.0050, Test Acc: 0.9528\n",
            "Epoch 5/100 - Train Loss: 0.0726, Train Acc: 0.9778, Valid Loss: 0.0961, Valid Acc: 0.9715, Test Loss: 0.0045, Test Acc: 0.9560\n",
            "Epoch 6/100 - Train Loss: 0.0483, Train Acc: 0.9843, Valid Loss: 0.0919, Valid Acc: 0.9782, Test Loss: 0.0042, Test Acc: 0.9594\n",
            "Epoch 7/100 - Train Loss: 0.0394, Train Acc: 0.9873, Valid Loss: 0.0790, Valid Acc: 0.9748, Test Loss: 0.0039, Test Acc: 0.9649\n",
            "Epoch 8/100 - Train Loss: 0.0366, Train Acc: 0.9888, Valid Loss: 0.0755, Valid Acc: 0.9765, Test Loss: 0.0035, Test Acc: 0.9671\n",
            "Epoch 9/100 - Train Loss: 0.0302, Train Acc: 0.9888, Valid Loss: 0.0961, Valid Acc: 0.9765, Test Loss: 0.0044, Test Acc: 0.9602\n",
            "Epoch 10/100 - Train Loss: 0.0231, Train Acc: 0.9914, Valid Loss: 0.0747, Valid Acc: 0.9765, Test Loss: 0.0035, Test Acc: 0.9684\n",
            "Epoch 11/100 - Train Loss: 0.0186, Train Acc: 0.9950, Valid Loss: 0.0601, Valid Acc: 0.9799, Test Loss: 0.0032, Test Acc: 0.9702\n",
            "Epoch 12/100 - Train Loss: 0.0197, Train Acc: 0.9933, Valid Loss: 0.0926, Valid Acc: 0.9748, Test Loss: 0.0041, Test Acc: 0.9662\n",
            "Epoch 13/100 - Train Loss: 0.0219, Train Acc: 0.9920, Valid Loss: 0.0766, Valid Acc: 0.9799, Test Loss: 0.0034, Test Acc: 0.9696\n",
            "Epoch 14/100 - Train Loss: 0.0135, Train Acc: 0.9959, Valid Loss: 0.0799, Valid Acc: 0.9815, Test Loss: 0.0036, Test Acc: 0.9675\n",
            "Epoch 15/100 - Train Loss: 0.0110, Train Acc: 0.9972, Valid Loss: 0.0791, Valid Acc: 0.9832, Test Loss: 0.0038, Test Acc: 0.9689\n",
            "Epoch 16/100 - Train Loss: 0.0128, Train Acc: 0.9953, Valid Loss: 0.0655, Valid Acc: 0.9849, Test Loss: 0.0033, Test Acc: 0.9727\n",
            "Epoch 17/100 - Train Loss: 0.0107, Train Acc: 0.9965, Valid Loss: 0.0770, Valid Acc: 0.9782, Test Loss: 0.0040, Test Acc: 0.9694\n",
            "Epoch 18/100 - Train Loss: 0.0084, Train Acc: 0.9976, Valid Loss: 0.0711, Valid Acc: 0.9782, Test Loss: 0.0034, Test Acc: 0.9730\n",
            "Epoch 19/100 - Train Loss: 0.0044, Train Acc: 0.9991, Valid Loss: 0.0694, Valid Acc: 0.9815, Test Loss: 0.0033, Test Acc: 0.9729\n",
            "Epoch 20/100 - Train Loss: 0.0035, Train Acc: 0.9996, Valid Loss: 0.0690, Valid Acc: 0.9799, Test Loss: 0.0033, Test Acc: 0.9735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      Starting Flower simulation, config: num_rounds=21, no round_timeout\n",
            "INFO:flwr:Starting Flower simulation, config: num_rounds=21, no round_timeout\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/100 - Train Loss: 0.0024, Train Acc: 0.9998, Valid Loss: 0.0707, Valid Acc: 0.9799, Test Loss: 0.0034, Test Acc: 0.9729\n",
            "Final test set performance for 128 PCA components: Loss 0.0032, Accuracy 0.9702\n",
            "Early stopping at epoch: 21\n",
            "\n",
            "Best PCA Components: 128\n",
            "Best Test Accuracy: 0.9702\n",
            "Best Test Loss: 0.0032\n",
            "Best Early Stopping Epoch: 21\n",
            "Quantization Type: dynamic\n",
            "Pruning Amount: 0.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-22 20:50:12,986\tINFO worker.py:1621 -- Started a local Ray instance.\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'node:__internal_head__': 1.0, 'node:172.28.0.12': 1.0, 'object_store_memory': 3939466444.0, 'memory': 7878932891.0}\n",
            "INFO:flwr:Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'node:__internal_head__': 1.0, 'node:172.28.0.12': 1.0, 'object_store_memory': 3939466444.0, 'memory': 7878932891.0}\n",
            "\u001b[92mINFO \u001b[0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
            "INFO:flwr:Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
            "\u001b[92mINFO \u001b[0m:      No `client_resources` specified. Using minimal resources for clients.\n",
            "INFO:flwr:No `client_resources` specified. Using minimal resources for clients.\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
            "INFO:flwr:Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Creating VirtualClientEngineActorPool with 2 actors\n",
            "INFO:flwr:Flower VCE: Creating VirtualClientEngineActorPool with 2 actors\n",
            "\u001b[92mINFO \u001b[0m:      [INIT]\n",
            "INFO:flwr:[INIT]\n",
            "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
            "INFO:flwr:Requesting initial parameters from one random client\n",
            "\u001b[2m\u001b[36m(pid=7716)\u001b[0m 2024-04-22 20:50:18.580460: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=7716)\u001b[0m 2024-04-22 20:50:18.580567: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=7716)\u001b[0m 2024-04-22 20:50:18.586939: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
            "INFO:flwr:Received initial parameters from one random client\n",
            "\u001b[92mINFO \u001b[0m:      Evaluating initial global parameters\n",
            "INFO:flwr:Evaluating initial global parameters\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "INFO:flwr:\n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
            "INFO:flwr:[ROUND 1]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n",
            "INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ClientAppActor pid=7716)\u001b[0m Get parameters: conv1.weight: torch.Size([16, 1, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=7716)\u001b[0m Get parameters: conv1.bias: torch.Size([16])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=7716)\u001b[0m Get parameters: conv2.weight: torch.Size([32, 16, 3, 3])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=7716)\u001b[0m Get parameters: conv2.bias: torch.Size([32])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=7716)\u001b[0m Get parameters: fc1.weight: torch.Size([128, 1568])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=7716)\u001b[0m Get parameters: fc1.bias: torch.Size([128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=7716)\u001b[0m Get parameters: fc2.weight: torch.Size([10, 128])\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=7716)\u001b[0m Get parameters: fc2.bias: torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QH5NFAzG-3I6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}